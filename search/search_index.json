{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IBM Sustainability Software Portfolio Connectors Connectors Overview IBM TRIRIGA Application Suite (TAS) and IBM Maximo Application Suite (MAS) customers have access to a collection of predefined integrations between TAS, MAS, and IBM Envizi ESG Suite. Those integrations can be used in a variety of combinations to enable many usecases and they can each be configured for additional flexibility. 1. The TRIRIGA - Maximo Connector is released in TAS under the name \"TAS Connector for Maximo\" and in MAS under the name \"MAS Connector for TRIRIGA\" . This connector supports the following capabilities: - Bi-directional loading, and continuous synchronization of portfolio data such as People, Locations and Assets. - Bi-directional routing of service requests between TRIRIGA and Maximo - Automatic shadowing of TRIRIGA Work Tasks with Maximo Work Orders to enable Facility teams to do planing, budgeting and project management in TRIRIGA while work tasks get reflected in Maximo as Work Orders for execution. 2. TAS Connector for Envizi automatically sync space and occupancy data from TRIRIGA with Envizi to enable energy usage calculations across entire facility portfolio with advanced analytics by location, by SQF, and by occupant. 3. MAS Connector for Envizi automatically synchronizes asset location and meter readings from MAS to Envizi to automate tracking of energy usage and calculating its related scope 1 and 2 emissions of Electricity, Natural Gas, and Water. App Connect IBM App Connect is a middlware that provides a flexible environment for integration solutions to transform, enrich, route, and process business messages and data. It is a pre-requisite for all the Portfolio Connectors. A restricted-use licence for App Connect Enterprise is made available as a supporting program through TAS 11.3+ and MAS 8.9+ but please note that it may not be hosted by IBM as a Managed Service. For all IBM hosted solution please acquire App Connect SaaS. App Connect Flows enable specific integration use cases by connecting to predefined APIs to route and transform data. They can be applied in different combinations to support a variety of use cases, they handle the required data mapping, data validation and they can be modified to support customizations. Installing App Connect App Connect SaaS is available in the AWS catalog . App Connect Enterprise on-prem can be provisioned within your RedHat OpenShift Cluster directly from the OpenShift Operator Note: If you are using a MAS or TAS Managed Service provided by IBM SRE you will need to obtain App Connect SaaS","title":"Sustainability Software Portfolio Connectors"},{"location":"#ibm-sustainability-software-portfolio-connectors","text":"","title":"IBM Sustainability Software Portfolio Connectors"},{"location":"#connectors-overview","text":"IBM TRIRIGA Application Suite (TAS) and IBM Maximo Application Suite (MAS) customers have access to a collection of predefined integrations between TAS, MAS, and IBM Envizi ESG Suite. Those integrations can be used in a variety of combinations to enable many usecases and they can each be configured for additional flexibility. 1. The TRIRIGA - Maximo Connector is released in TAS under the name \"TAS Connector for Maximo\" and in MAS under the name \"MAS Connector for TRIRIGA\" . This connector supports the following capabilities: - Bi-directional loading, and continuous synchronization of portfolio data such as People, Locations and Assets. - Bi-directional routing of service requests between TRIRIGA and Maximo - Automatic shadowing of TRIRIGA Work Tasks with Maximo Work Orders to enable Facility teams to do planing, budgeting and project management in TRIRIGA while work tasks get reflected in Maximo as Work Orders for execution. 2. TAS Connector for Envizi automatically sync space and occupancy data from TRIRIGA with Envizi to enable energy usage calculations across entire facility portfolio with advanced analytics by location, by SQF, and by occupant. 3. MAS Connector for Envizi automatically synchronizes asset location and meter readings from MAS to Envizi to automate tracking of energy usage and calculating its related scope 1 and 2 emissions of Electricity, Natural Gas, and Water.","title":"Connectors Overview"},{"location":"#app-connect","text":"IBM App Connect is a middlware that provides a flexible environment for integration solutions to transform, enrich, route, and process business messages and data. It is a pre-requisite for all the Portfolio Connectors. A restricted-use licence for App Connect Enterprise is made available as a supporting program through TAS 11.3+ and MAS 8.9+ but please note that it may not be hosted by IBM as a Managed Service. For all IBM hosted solution please acquire App Connect SaaS. App Connect Flows enable specific integration use cases by connecting to predefined APIs to route and transform data. They can be applied in different combinations to support a variety of use cases, they handle the required data mapping, data validation and they can be modified to support customizations.","title":"App Connect"},{"location":"#installing-app-connect","text":"App Connect SaaS is available in the AWS catalog . App Connect Enterprise on-prem can be provisioned within your RedHat OpenShift Cluster directly from the OpenShift Operator Note: If you are using a MAS or TAS Managed Service provided by IBM SRE you will need to obtain App Connect SaaS","title":"Installing App Connect"},{"location":"mas-tas-home/","text":"TRIRIGA - Maximo Connector The TRIRIGA - Maximo Connector is released in TAS under the name \"TAS Connector for Maximo\" and in MAS under the name \"MAS Connector for TRIRIGA\" . This connector supports the following capabilities through a collection of App Connect flows: Bi-directional loading, and continuous synchronization of portfolio data such as People, Locations and Assets. Bi-directional routing of service requests between TAS and Maximo Automatic shadowing of TAS Work Tasks with Maximo Work Orders to enable Facility teams to do planing, budgeting and project management in TAS while work tasks get reflected in Maximo as Work Orders for execution. Use case examples TAS is the system of record for People and Space data while Maximo is the system of record for Assets. The Maintenance team wants consistent reports across the entire organization and more streamlined operations. The Spaces flows provided can be used to bulk-load all Spaces data from TAS into Maximo Locations, and then continuously keep the data in sync as it gets updated in TAS by a Facility Management team. This can apply to Assets, or People data in both directions. Employees use Request Central to submit a Service Request for a broken elevator. Since elevators are maintained in Maximo, the flow will create a corresponding Service Request in Maximo that will be assigned to technicians and resolved. As the Maximo Service Request gets updated, the changes are automatically reflected in the TAS Service request so the originator of the request remains informed of its status. This flow can operate in reverse as well if a Service Request originates from Maximo against a maintenance operation that should be tracked in TAS. Facility Management team have a capital project in TAS to upgrade all the lights to LEDs. The project has the budget, scope and tasks already defined. The flow can then create corresponding Work Orders in Maximo for each task to get executed by technicians using the Maximo Manage and Mobile applications. As the tasks get updated in Maximo, those updates flow back into TAS to reflect in the project plan. Connector architecture IBM App Connect provides a flexible environment for integration solutions to transform, enrich, route, and process business messages and data. App Connect Flows enable specific integration use cases by connecting to predefined APIs to route and map data. Mapping has been pre-defined, but it can be customized. Native API framework is used for Maximo and TAS and enabled thorugh provided packages that can be imported. Data Mapping The image below illustrates the record types that are available the APIs and App Connect Flows Provided. App Connect Flows Included with this connector are 20+ flows that map TAS records to Maximo, and vice versa, along with all the required fields they contain. The table below shows all the flows you can use in differnet combinations to implement your desired integration use cases. For a more detailed mapping of all the fields please review the data mapping spreadsheet . File Flow Destination Operation PLUSITRIPerson2MX_v1_0_0.yaml People TRI to Max Single Record at a time PLUSITRIOrg2MX_v1_0_0.yaml People TRI to Max Single Record at a time PLUSIMXPerson2TRI_v1_0_0.yaml People Max to TRI Batch & Individual Records PLUSITRISpace2MX_v1_0_0.yaml Spaces/Locations TRI to Max Single Record at a time PLUSITRISpaceClass2MX_v1_0_0.yaml Spaces/Locations TRI to Max Single Record at a time PLUSITRILocPath2MX_v1_0_0.yaml Spaces/Locations TRI to Max Single Record at a time PLUSIMXLocation2TRI_v1_0_0.yaml Spaces/Locations Max to TRI Batch & Individual Records PLUSITRIAsset2MX_v1_0_0.yaml Assets TRI to Max Single Record at a time PLUSITRIAssetSpec2MX_v1_0_0.yaml Assets TRI to Max Single Record at a time PLUSIMXAsset2TRI_v1_0_0.yaml Assets Max to TRI Batch & Individual Records PLUSITRIServiceReq2MX_v1_0_0.yaml Service Requests TRI to Max Single Record at a time PLUSITRIReqClass2MX_v1_0_0.yaml Service Requests TRI to Max Single Record at a time PLUSIMXServiceReq2TRI_v1_0_0.yaml Service Requests Max to TRI Single Record at a time PLUSITRIWorkOrder2MX_v1_0_0.yaml Tasks / Work Orders TRI to Max Single Record at a time PLUSIMXWorkOrder2TRI_v1_0_0.yaml Tasks / Work Orders Max to TRI Single Record at a time PLUSITRIPEOPLEBATCH_v1_0_0.yaml Batch People TRI to Max Batch PLUSITRIORGANIZATIONBATCH_v1_0_0.yaml Batch Org TRI to Max Batch PLUSITRISPACEBATCH_v1_0_0.yaml Batch Space TRI to Max Batch PLUSITRIPROPERTYBATCH_v1_0_0.yaml Batch Property TRI to Max Batch PLUSITRIBUILDINGBATCH_v1_0_0.yaml Batch Building TRI to Max Batch PLUSITRIFLOORBATCH_v1_0_0.yaml Batch Floors TRI to Max Batch PLUSITRIASSETBATCH_v1_0_0.yaml Batch Assets TRI to Max Batch Installation & Configuration Guide Before you begin you will need: An instance of App Connect Enterprise or App Connect Pro with the Designer component. Admin access to your Maximo instance with an API key generated for this integration. Storage will need to be properly configured to store the images as they are coming in. Refer to the Doclinks setup page TRIRIGA with dedicated user/pw for this integration Secure connection between TRIRIGA, App Connect, and Maximo. If required IBM does provides a product that accomplishes this: Secure Gateway. Learn more about getting started with Secure Gateway. Import AppConnect Cert to Maximo to enable encrypted communication Import AppConnect Cert to TRIRIGA to enable encrypted communication Downloadable Resources Download the zip file that has all of the flows and configuration files. Installation Steps Overview Install Connector Configure App Connect a. Authenticate b. Import Flows Configure TRIRIGA a. Import TRIRIGA OM Package b. Point integration object to App Connect Flow Configure Maximo a. Add necessary fields b. Publish channel to point to App Connect Flow Test a. MAS outbound connectivity b. TRIRIGA POSTMAN Part 1. Install Connector MAS 8.10+ The MAS Connector to TAS in Manage can be installed on the MAS administration dashboard through the tile. Search for TRIRIGA from the Catalog page and select the TRIRIGA Connector tile. On the tile page, click Configure and it will take you to the update configuration page. Select the latest version of the connector under the Components section and then click Apply Changes at the top of the page and Confirm on the next window. Step 1: Select Connector Tile Step 2: Apply Changes for Connector The process can take anywhere between 20 minutes to 1-2 hours depending on the cluster resources. Monitor the logs in the created build pods in the Manage namespace in the OpenShift cluster to see the progress. MAS 8.8 & 8.9 The MAS Connector to TAS in Manage can be installed by using the proper customization .zip file provided from Passport Advantage. This .zip file will have all of the configurations necessary to complete step 4 in the Installation process. Navigate to the Admin dashboard of the instance of Manage within MAS. Select the workspace the instance is deployed on and update the configuration. Scroll down to Customization and link to the location of the .zip file in the field ( Additional details on setting customizations) Click Apply Changes and Manage will update the instance with the customization. The process can take anywhere between 20 minutes to 1-2 hours depending on the cluster resources. Monitor the logs in the created build pods in the Manage namespace in the OpenShift cluster to see the progress. Maximo 7.6.1.2+ On the Maximo Admin workstation, overlay the Maximo SMP directory ( /opt/IBM/SMP/maximo ) with the contents from the solution zip file that is provided. This will lay down the Java Classes and .dbc files provided with the solution. Shutdown the MXServer Run UpdateDB command to install the solution components a. Navigate to /opt/IBM/SMP/maximo/tools/maximo b. Run ./updatedb.sh Build the Maximo EAR file a. Navigate to /opt/IBM/SMP/maximo/deployment b. Run ./buildmaximoear.sh Deploy the new Maximo EAR File on all Maximo servers a. In WebSphere console, navigate to Applications->Application types->Websphere enterprise applications b. Select MAXIMO and then hit Update button c. Select Browse and select the EAR file from Step 4 d. Hit Next and accept defaults from all pages e. After deployment, click on Save Start the applications and MXServer Part 2. Configure App Connect App Connect Authentication Note: IBM App Connect Professional or Enterprise is needed to run these flows. The flows have been tested on IBM Cloud App Connect, AWS App Connect, as well as the containerized version of App Connect. Two accounts need to be created from the Catalog tab in order to connect the applications. Once all of the connectors have loaded, type in http to find the HTTP Application. If this is the first account, select Connect to begin setting up the initial HTTP account. If this is not the first account, make sure to take note if there are any other generic account names present because the number of the one created will depend on what has already been created. App Connect creates an account with a generic name in sequential order (Example: if Account 1 and Account 2 are present, the new account will be Account 3). Use the below table to associate the proper credentials for the accounts. Once the account is connected, head back to the HTTP Application on the Catalog page and rename the new account according to the Account Name column in the table below. Flow Account Name Username Password API key API location API key name Max -> Tri mxtririga Your TAS Username Your TAS Password N/A N/A N/A Tri -> Max trimaximo N/A N/A Your Maximo apikey header apikey App Connect catalog Importing App Connect Flows Locate the .yaml files for the desired flows from the provided zip. Import the Selected Flows into App Connect Enterprise , or App Connect SaaS on IBM Cloud using the instructions below Import Steps - App Connect Enterprise (CP4I) & App Connect SaaS (AWS) From the App Connect Dashboard, click New and select Import Flow from the drop down menu. Either drag and drop or select the flow for import. In this example, the MX2TRI Person flow will be used. The flow should now be uploaded onto the App Connect instance. From this screen navigate using the Edit flow button to see the individual nodes of this flow. Be sure to select the HTTP account that was configured for Maximo to TRIRIGA for the connector. Click Done on the top right of the screen then click on the three dots in the top right corner and select Start API . Go to the Test tab once it shows that the flow is Running and select the POST option on the left side of the screen. Click on Try It and grab the URL and security credentials from this screen for the next step. Step 1: App Connect Dashboard Step 2: Import a Flow Step 3: Completed Flow Step 4: Start Flow Step 5: Try It Import Steps - App Connect SaaS (IBM Cloud) If your instance of AppConnect is through the cloud, your page will look a bit different. The interface is mostly the same, but instead of Try It you'll see a tab called Manage . This page contains a few important pieces of information that you'll need to complete the configuration. First, at the top of the page under API Info you'll see a field called Route . Piece this together with the correct path of the flow that you're implementing in order to create the proper flow URL. Collect the API key at the bottom of the page from a field called Sharing outside of Cloud Foundry organization . Click on Create API key and documentation link . Provide a name and it will generate an apikey for you to use with this flow along with a documentation link that looks like the Test page from the on-prem configuration. The end of the URL at the top of the page should have the path that will complete the route URL. Copy the end of the URL that begins with tri and add it to the piece gathered earlier. The 6 character string that precedes the path on the documentation should match the last 6 characters of the route piece from before. Step 1: Manage Tab Step 2: Cloud Route Step 3: API key creation Step 4: API Documentation Link Running in Production Once the configuration of the below applications is complete and the connectivity between systems is working as expected, you will need to deploy the flow as an integration server via the App Connect Dashboard. From the dashboard view of the Designer application, click the three dots on the flow's tile, select Export , and then Runtime flow asset (BAR) . In the App Connect Dashboard instance, select Create Server and then the type of integration you would like to run. The flows work with all types of integration sizes, so pick the one that is right for your deployment. Import the bar file you just downloaded into the section on the Integrations tab. On the Configuration screen, refer to securing a REST-based integration to properly secure the server. Give the intended details of the server such as the name, version, and correct license to use. Click Create when you're ready to deploy. It may take upwards of 5-10 minutes for the server to be up and running. Step 1: Export Bar file Step 2: Create Integration Server Step 3: Final details Part 3. Configure TAS Import Object Migration Package Import the OM Package labeled APIConnector into the TAS instance. Go to Tools -> Administration -> Object Migration and select New Import Package to begin the import process. Refer to the Object Migration Overview for more information on Object Migration TAS API User Access In order for App Connect to be able to use TAS APIs, it will need a user with certain permissions. These user's credentials will be configured in App Connect. Create an integration user by following the steps given in Chapter 2 . This user should be a non-admin user and not part of the Admin security group. Assign that user to a new or existing group for the integration. If you need to create a new security group, follow the steps given in Chapter 1 . Add the \"TAS Base License\" to the License Details section on the user Profile. Select the newly created group or desired existing group and switch to the Access tab and add the appropriate access for the integration. Outbound Traffic from TAS For outbound traffic from TAS, grant at least READ access on the Business Objects that will be used. The table below shows the various supported business Objects the API can pull from: Module Business Object Label Asset Building Equipment Classification Request Class Classification Space Class Current Classification Asset Spec Class People People Location Property Location Building Location Floor Location Space Organization Organization Request Service Request Task Work Task In the example below, the API user is able to pull data from the Building Business Object: Inbound traffic to TAS For inbound traffic, Data Access needs to be enabled as well as Application Access permissions to the triAPIConnect Module or the individual Objects. To enable an API user to create a building, grant access to the triAPICBuilding Business object as shown below: Date Time Format The Date Time Format field in the user profile must be in UTC. Navigate to Portfolio -> People -> My Profile and select the user profile that will be triggering the action. The Date Time Format should be in UTC as shown below. Date Time Format Field in My Profile Configure Integration Object Configure the Integration Object - From the main page of TAS, click on Tools -> System Setup -> Integration -> Integration Object . Under the Name column, type in apic , and select the integration object that pertains to the record that is getting sent. Click on the object and fill in the credentials in the pop-up box. TAS End Point Part 4. Maximo Configuration Note 1 If you have not already done so, please import AppConnect Cert to Maximo to enable encrypted communication. Note 2 : The following steps and prerequisites are done against a Maximo demo database. The naming conventions may slightly differ from this, but these are the necessary components. Within Maximo, configure your instance to be ready to receive records from TAS. If these pre-requisites are not completed, the action will not be recorded. 1. Create an Organization named TRIRIGA a. Navigate to the Organizations page and click the blue + button on the top row. b. Fill in the Organization name with TRIRIGA and the description as \"TAS Organization\". c. Fill in the remaining required fields as such: Field Name Value Base Currency 1 USD Item Set SET1 Company Set COMPSET1 Default Item Status PENDING Default Stock Category STK d. Click Save Organization on the left side of the screen under Common Actions. This will be set to Active later once there is a clearing account. 2. Create a Testing clearing account in Chart of Accounts Un-require the GL Account fields a. Navigate to Financial -> Chart of Accounts and click on the previously created TAS org in the Organizations table. Currently, there should be no GL Accounts for TAS present. b. Click GL Component Maintenance on the left side under More Actions and add a New Row with the following values: Field Name Value GL Component Value 1001 Description Testing Active? Yes c. Click OK . Click New Row under GL Accounts for TRIRIGA and click the magnifying glass to search for that GL Component. Select it and it should populate in the GL Account and Description fields. The Active Date field should auto populate to the current date. d. Now that this account is present, head back to Organizations and update the TRIRIGA organization to show the just created Clearing Account, tick the Active box, and click Save Organization . 3. Create a site TRIMAIN and set it to active a. On the Organization page, click on the Sites tab at the top of the page. b. Click New Row under Sites and enter TRIMAIN for Site and MAIN Site for Description. Set the site to Active. c. Click Save Organization . 4. API Key MAS Navigate to Integration -> API Keys . Click on the button that reads Add API key Select user MXINTADM and click the Add button to generate an API key for this user. Securely store this API key for later use. Maximo 7.6.1.2 Navigate to Administration -> Administration and a new tab/window should open with the Maximo-X application. You should be on a page titled 'Integration'. Click on the tab at the top of the page that says API Keys and click on the button with the blue plus sign that reads Add API key Select user MXINTADM and click the Add button to generate an API key for this user. Securely store this API key for later use. 5. Integration Controls a. Head to Enterprise Services and click on Create Integration Controls . These integration controls help translate specific external values into values Maximo understands. The following tables contain values to set up this demo, but can be customized to fit your TAS and Maximo naming conventions. There should be 5 X-Ref Integration Controls created with the following associations: Integration Control MAXIMO Value External Value Description Domain PLUSILOCSTATUS ACTIVE ACTIVE TAS Location Status mapping for inbound flows LOCASSETSTATUS PLUSILOCSTATUS INACTIVE REVIEW IN PROGRESS N/A N/A PLUSILOCSTATUS OPERATING OPERATING N/A N/A PLUSIORG TRIMAIN IBM Organization mapping for TAS N/A PLUSIORG TAS TAS N/A N/A PLUSIORGEN TAS EAGLENA TAS Organization mapping for Inbound flow N/A PLUSIORGEN TAS IBM N/A N/A PLUSIORGEN TAS MAXIMO ORG N/A N/A PLUSIORGEN TAS TAS N/A N/A PLUSIPRIORITY 1 High Priority mapping for TAS N/A PLUSIPRIORITY 2 Medium N/A N/A PLUSIPRIORITY 3 Low N/A N/A PLUSISITEEN TRIMAIN BEDFORD TAS Location mapping for inbound flows N/A PLUSISITEEN TRIMAIN SPACE 01 N/A N/A PLUSISITEEN TRIMAIN TRIMAIN N/A N/A b. Once these Integration Controls are created, associate them in both the created Enterprise Services and Publish Channels by using the following two tables Enterprise Service Control PLUSIASSET PLUSIORGEN PLUSIASSET PLUSIPRIORITY PLUSIASSET PLUSISITEEN PLUSILOCATION PLUSILOCSTATUS PLUSILOCATION PLUSISITEEN PLUSILOCATION PLUSISTATUS PLUSIPERSON PLUSIORGEN PLUSIPERSON PLUSISITEEN PLUSIWO PLUSIWOPRIORITY Publish Channel Control PLUSIASSET PLUSIPRIORITY PLUSILOCATION N/A PLUSIPERSON PLUSIORG PLUSISR N/A PLUSIWO PLUSIWOPRIORITY PLUSIWO PLUSIWOSTART c. Return to the PLUSITRIRIGA External System. On the left side of the External Systems page, select Setup Integration Controls under More Actions and make sure that all 5 Integration Controls are showing as present. 6. Enable Object Structure Security The user needs to be able to transact against the specific object structureS in Manage. Navigate to Object Structures and search for MXPERSON. On the left side of the MXPERSON screen select Configure Object Structure Storage and turn on the button underneath Use Object Structure for Authorization Name? Configure Object Security Screen The structure should save. Complete this process for the following objects: MXASSET MXOPERLOC MXSR MXWO MXDOMAIN Next, navigate to Security -> Security Groups and select the security group the current user is apart of. Click the Object Structures tab and filter search for MXPERSON. Once selected, click Grant Listed Options for This Object Structure and Save the Group. Security Group Page Note Repeat this process for the other changed structures. 7. Application Designer Changes Go to System Configuration -> Platform Configuration -> Application Designer Person Search for PERSON , switch to the Person Tab and select a section to add three new boxes. At the top, click the icon labeled Control Palette and add a Multipart Textbox at the bottom of the section. Add the values from the below table within the properties of the Multipart Textbox and click Save Definition Note Be sure that the Attributes are taken from the PERSON Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TAS Location Path PLUSIPRIMARYLOCPATH PLUSILOCATIONPATH.DESCRIPTION VALUELIST Readonly Multipart Textbox TAS Primary Organization PLUSIORGPATH PLUSIORGANIZATIONPATH.DESCRIPTION VALUELIST Readonly Textbox TAS Record ID EXTERNALREFID ( From the Person Object ) N/A N/A N/A Follow the same directions using the following information for the other objects that will be used in the integration Asset Search for ASSET in Application Designer Note Be sure that the Attributes are taken from the ASSET Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TAS Building Equipment Spec PLUSIASSETSPECNAME PLUSIASSETSPECCLASS.DESCRIPTION VALUELIST Readonly Multipart Textbox TAS Primary Organization PLUSIORGPATH PLUSIORGANIZATIONPATH.DESCRIPTION VALUELIST Readonly Multipart Textbox TAS Location Path PLUSILOCPATH PLUSILOCATIONPATH.DESCRIPTION VALUELIST Readonly Textbox TAS Record ID EXTERNALREFID ( From the Asset Object ) N/A N/A N/A Location Search for LOCATION in Application Designer Note Be sure that the Attributes are taken from the LOCATION Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TAS Space Classification PLUSISPACECLASSIFICATION PLUSISPCCLASSIFICATION.DESCRIPTION VALUELIST Readonly Multipart Textbox TAS Parent Location PLUSIPARENTLOCATION PLUSIPARENTPATH.DESCRIPTION VALUELIST Readonly Textbox TAS Record ID EXTERNALREFID ( From the Location Object ) N/A N/A N/A Service Request Search for SR in Application Designer Note Be sure that the PLUSIREQCLASSID Attribute is taken from the TICKET Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TAS Request Classification PLUSIREQCLASSID PLUSIREQCLASS.DESCRIPTION VALUELIST Readonly Multipart Textbox TAS Primary Organization PLUSIORGPATH PLUSIORGANIZATIONPATH.DESCRIPTION VALUELIST Readonly Multipart Textbox TAS Parent Location PLUSIPARENTLOCATION PLUSIPARENTPATH.DESCRIPTION VALUELIST Readonly Textbox TAS Record ID EXTERNALREFID ( From the Ticket Object ) N/A N/A N/A e. Click Save Definition after the changes are added. Work Order Search for WOTRACK in Application Designer Note Be sure that the PLUSIREQCLASSID Attribute is taken from the WORKORDER Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TAS Location Path PLUSILOCPATH PLUSILOCATIONPATH.DESCRIPTION VALUELIST Readonly Multipart Textbox TAS Primary Organization PLUSIORGPATH PLUSIORGANIZATIONPATH.DESCRIPTION VALUELIST Readonly Textbox External Ref ID EXTERNALREFID ( From the WorkOrder Object ) N/A N/A N/A Click Save Definition after the changes are added. 8. TAS to Maximo Service Request Comment & Attachment Support The PLUSITRIServiceRequest2MX_v_1_1_0 integration supports both Comments and Attachments from TAS. This includes pictures or images that are attached as either Comments or Attachments in TAS. Some configuration will need to be done in order to allow for this. DB Config Go to Database Configuration and pull up the WORKLOG object. Add 'EXTERNALREFID' as an Attribute with a type ALN and 50 length. Turn on Admin Mode and Run Database Configuration for this change as it will need to be referenced in the Index. Once EXTERNALREFID has been added, navigate to Indexes and add the index 'PLUSIEXTREF'. Under the Columns table make sure to reference the EXTERNALREFID that was just created as well as to leave 'Enforce Uniqueness' unchecked. Turn on Admin Mode and run Database Configuration again to have these changes take effect. Object Structure Navigate to Object Structure and find MXSR. Duplicate this Object Structure and rename it PLUSISRDOCS. Add two new Source Objects to the Structure underneath SR: \"DOCLINKS\" and \"WORKLOG\". Make sure the Relationships match with the following image. - Select the WORKLOG Source Object and in the 'Details' section add the 'PLUSIEXTREF' Index to the Alternate Key field. Then navigate to Exclude/Include Fields. Click on Non-Persistent Fields in the window and make sure 'Description_LongDescription' is selected as Included. Enterprise Services Now that the changes are made in database and object structure level, they need to be referenced by the Enterprise Service. To do this, duplicate the Default PLUSISR Enterprise Service and name this new one 'PLUSISRDOCS' that references the newly created Object Structure. Remove the default PLUSISR Enterprise Service from the PLUSITRIRIGA External System in order to Delete the Enterprise Service. Once deleted, duplicate PLUSISRDOCS and name it PLUSISR to replace the deleted Enterprise Service. Add the new PLUSISR back into the External System. Now when a Service Request comes in with a comment or attachment, Maximo will keep track of the comments in the Worklog and images/documents in the Attachments. Part 5: Testing To test that the configuration is complete, send a test payload in order to test connectivity. Maximo Go to the End Points application and click Test at the bottom of the integration you would like to test. Send a test payload that is a valid object. {\"Hello\":\"World\"} would work. If the response is anything other than Bad Request , see what might be causing the error TAS Use a tool like POSTMan to test the connectivity of the App Connect flow. You can use a Sample JSON Payload from this open source repository . Part 6: Data Pre-requisites for the Integrations Because of internal references within the TRIRIGA and Maximo data models some flows depend on other flows to function. The following tables describe these relationships. You will need to run the pre-requisite flows first before running the main flows. Maximo to TAS Flow Flows to run first MXPerson2TRI TRISpace2MX, TRILocPath2MX, TRIOrg2MX MXAsset2TRI TRISpace2MX, TRILocPath2MX, TRIOrg2MX, TRIAssetSpec2MX MXLocation2TRI TRILocPath2MX, TRISpaceClass2MX MXServiceReq2TRI TRILocPath2MX, TRIReqClass2MX, TRIPerson2MX MXWorkOrder2TRI TRILocPath2MX, TRIOrg2MX TAS to Maximo Flow Flows to run first TRIPerson2MX TRISpace2MX, TRILocPath2MX, TRIOrg2MX TRIAsset2MX TRISpace2MX, TRILocPath2MX, TRIOrg2MX, TRIAssetSpec2MX TRISpace2MX TRILocPath2MX, TRISpaceClass2MX TRIServiceReq2MX TRILocPath2MX, TRIReqClass2MX, TRIPerson2MX TRIWorkOrder2MX TRILocPath2MX, TRIOrg2MX PeopleBatch TRISpace2MX, TRILocPath2MX, TRIOrg2MX AssetBatch TRISpace2MX, TRILocPath2MX, TRIOrg2MX, TRIAssetSpec2MX SpaceBatch TRILocPath2MX, TRISpaceClass2MX Troubleshooting Depending on the direction of the flow, cross-referencing errors from two systems can help identify the root cause of an issue with the integration. For example: If there is a Not Found error in Maximo Message Reprocessing when running the flow, double check the logs in App Connect to see if there is a corresponding error. If there is, the root cause might be related to what is being sent out of Maximo. If there isn't, then the message never left Maximo and the flow should be checked to make sure it is running. Where to find logs in App Connect App Connect Enterprise The logs in App Connect Enterprise are found in the Integration Server pod. If running from App Connect Designer, this integration server is found in the App Connect namespace in OpenShift. Typically the Designer integration server has 4 pods and the logs can be found in the pod with a similar naming convention to des-01-quickstart-ma-designer-designer-flows . App Connect Enterprise App Connect SaaS & AWS The logs in App Connect SaaS and AWS can be found within the application itself by clicking on the clipboard icon on the left side task bar. App Connect SaaS or AWS Common errors that arise from Maximo Testing the End Point When testing that the end point is entered correctly on the End Points application, there are two common errors: Error Cause Resolution Response code received from the HTTP request from the endpoint is not successful Invalid URL in the Integration Object Double check the URL that all of the components are entered correctly. Make sure there are no accidental spaces at the beginning or the end in event of a copy/paste. 404: Not Found Flow is not Running Make sure the flow is Running before starting the cron tasks PKSync error Certificate error Confirm the certificate is configured correctly Message Reprocessing Errors in Maximo can be found in Message Reprocessing Error Cause Resolution Bad Request error Either the payload being sent out is incorrect or the account configurations are wrong Double check to make sure the Accounts from the App Connect pre-requisite section are correct and that the item in Maximo has all of the mapped fields Common errors that arise from TAS Error Cause Resolution 404 - Not Found: Cannot POST Invalid URL in the Integration Object Double check the URL in the specific Integration Object that all of the components are entered correctly 404 - API doesn't exist Flow is not running Double check that the flow is Active in App Connect 404 - The HTTP request returned with an error 404 \"Not Found\" Incorrect App Connect connector config Double check that the credentials being used in the HTTP post node in App Connect are correct 401 - Authorization error Too many user sessions open in TAS Open the Admin dashboard on the TAS environment and check the Users logged in. This issue can arise after a number of requests are made to TAS and then gives a 401 error even with the proper credentials. Clear the users logged in and the issue should clear. Clear OSLC Cache in TAS Admin Console in case the integrations do not work in intended manner. Reference for Pre-requisite Pre-requisite: add an App Connect Certificate in MAS 8.9+ Extract the App Connect certificate from an imported flow URL. Navigate to the flow's page and click on Test and then Try It to get the proper URL. Navigate to the Admin dashboard for MAS and go to the workspace where Manage is deployed. Update the configuration and scroll down to Imported Certificates . Untick the System managed button and fill in the extracted certificate in the fields. Click Apply Changes at the top of the page and MAS will update the truststore with the new certificate. Step 2: Manage Workspace Step 3: Imported Certificates Pre-requisite: add an App Connect Certificate in Maximo 7.6.1.2+: Configure WebSphere Certificates. This makes a test connection to a Secure Sockets Layer (SSL) port and retrieves the signer from the server during the handshake. Log into the WebSphere console that is hosting the Maximo server. Click on Security -> SSL certificate & key management . Under Related Items click on Key stores and certificates . Click on CellDefaultTrustStore and on the next page under Additional Properties click on Signer certificates . From this page, click on the button that says Retrieve from Port and fill in the required fields using the table below: Field Value Host The host from the imported flow URL Port 443 Alias appconnect Once all three have been entered in, click Retrieve signer information and the information from the URL will populate on screen. Click Save in the box at the top and then repeat the process for NodeDefaultTrustStore . Step 2: WebSphere Homepage Step 3: Websphere Keystores Step 4: Websphere Signer Certs Pre-Requisite: Update TAS Custom Resource with App Connect Certificates: In OpenShift, import the following yaml with the App Connect certificates added under the spec.alias.crt section: cat <<EOF | oc create -f - apiVersion: truststore-mgr.ibm.com/v1 kind: Truststore metadata: name: my-tas-truststore spec: license: accept: true includeDefaultCAs: true servers: - \"example.com:443\" certificates: - alias: alias_1 crt: | -----BEGIN CERTIFICATE----- ... Certificate 1 ... -----END CERTIFICATE----- ... - alias: alias_n crt: | -----BEGIN CERTIFICATE----- ... Certificate n ... -----END CERTIFICATE----- EOF If there is already a truststore for TAS, update the truststore with the App Connect certificate.","title":"IBM MAS Connector for TRIRIGA"},{"location":"mas-tas-home/#tririga-maximo-connector","text":"The TRIRIGA - Maximo Connector is released in TAS under the name \"TAS Connector for Maximo\" and in MAS under the name \"MAS Connector for TRIRIGA\" . This connector supports the following capabilities through a collection of App Connect flows: Bi-directional loading, and continuous synchronization of portfolio data such as People, Locations and Assets. Bi-directional routing of service requests between TAS and Maximo Automatic shadowing of TAS Work Tasks with Maximo Work Orders to enable Facility teams to do planing, budgeting and project management in TAS while work tasks get reflected in Maximo as Work Orders for execution.","title":"TRIRIGA - Maximo Connector"},{"location":"mas-tas-home/#use-case-examples","text":"TAS is the system of record for People and Space data while Maximo is the system of record for Assets. The Maintenance team wants consistent reports across the entire organization and more streamlined operations. The Spaces flows provided can be used to bulk-load all Spaces data from TAS into Maximo Locations, and then continuously keep the data in sync as it gets updated in TAS by a Facility Management team. This can apply to Assets, or People data in both directions. Employees use Request Central to submit a Service Request for a broken elevator. Since elevators are maintained in Maximo, the flow will create a corresponding Service Request in Maximo that will be assigned to technicians and resolved. As the Maximo Service Request gets updated, the changes are automatically reflected in the TAS Service request so the originator of the request remains informed of its status. This flow can operate in reverse as well if a Service Request originates from Maximo against a maintenance operation that should be tracked in TAS. Facility Management team have a capital project in TAS to upgrade all the lights to LEDs. The project has the budget, scope and tasks already defined. The flow can then create corresponding Work Orders in Maximo for each task to get executed by technicians using the Maximo Manage and Mobile applications. As the tasks get updated in Maximo, those updates flow back into TAS to reflect in the project plan.","title":"Use case examples"},{"location":"mas-tas-home/#connector-architecture","text":"IBM App Connect provides a flexible environment for integration solutions to transform, enrich, route, and process business messages and data. App Connect Flows enable specific integration use cases by connecting to predefined APIs to route and map data. Mapping has been pre-defined, but it can be customized. Native API framework is used for Maximo and TAS and enabled thorugh provided packages that can be imported.","title":"Connector architecture"},{"location":"mas-tas-home/#data-mapping","text":"The image below illustrates the record types that are available the APIs and App Connect Flows Provided.","title":"Data Mapping"},{"location":"mas-tas-home/#app-connect-flows","text":"Included with this connector are 20+ flows that map TAS records to Maximo, and vice versa, along with all the required fields they contain. The table below shows all the flows you can use in differnet combinations to implement your desired integration use cases. For a more detailed mapping of all the fields please review the data mapping spreadsheet . File Flow Destination Operation PLUSITRIPerson2MX_v1_0_0.yaml People TRI to Max Single Record at a time PLUSITRIOrg2MX_v1_0_0.yaml People TRI to Max Single Record at a time PLUSIMXPerson2TRI_v1_0_0.yaml People Max to TRI Batch & Individual Records PLUSITRISpace2MX_v1_0_0.yaml Spaces/Locations TRI to Max Single Record at a time PLUSITRISpaceClass2MX_v1_0_0.yaml Spaces/Locations TRI to Max Single Record at a time PLUSITRILocPath2MX_v1_0_0.yaml Spaces/Locations TRI to Max Single Record at a time PLUSIMXLocation2TRI_v1_0_0.yaml Spaces/Locations Max to TRI Batch & Individual Records PLUSITRIAsset2MX_v1_0_0.yaml Assets TRI to Max Single Record at a time PLUSITRIAssetSpec2MX_v1_0_0.yaml Assets TRI to Max Single Record at a time PLUSIMXAsset2TRI_v1_0_0.yaml Assets Max to TRI Batch & Individual Records PLUSITRIServiceReq2MX_v1_0_0.yaml Service Requests TRI to Max Single Record at a time PLUSITRIReqClass2MX_v1_0_0.yaml Service Requests TRI to Max Single Record at a time PLUSIMXServiceReq2TRI_v1_0_0.yaml Service Requests Max to TRI Single Record at a time PLUSITRIWorkOrder2MX_v1_0_0.yaml Tasks / Work Orders TRI to Max Single Record at a time PLUSIMXWorkOrder2TRI_v1_0_0.yaml Tasks / Work Orders Max to TRI Single Record at a time PLUSITRIPEOPLEBATCH_v1_0_0.yaml Batch People TRI to Max Batch PLUSITRIORGANIZATIONBATCH_v1_0_0.yaml Batch Org TRI to Max Batch PLUSITRISPACEBATCH_v1_0_0.yaml Batch Space TRI to Max Batch PLUSITRIPROPERTYBATCH_v1_0_0.yaml Batch Property TRI to Max Batch PLUSITRIBUILDINGBATCH_v1_0_0.yaml Batch Building TRI to Max Batch PLUSITRIFLOORBATCH_v1_0_0.yaml Batch Floors TRI to Max Batch PLUSITRIASSETBATCH_v1_0_0.yaml Batch Assets TRI to Max Batch","title":"App Connect Flows"},{"location":"mas-tas-home/#installation-configuration-guide","text":"","title":"Installation &amp; Configuration Guide"},{"location":"mas-tas-home/#before-you-begin-you-will-need","text":"An instance of App Connect Enterprise or App Connect Pro with the Designer component. Admin access to your Maximo instance with an API key generated for this integration. Storage will need to be properly configured to store the images as they are coming in. Refer to the Doclinks setup page TRIRIGA with dedicated user/pw for this integration Secure connection between TRIRIGA, App Connect, and Maximo. If required IBM does provides a product that accomplishes this: Secure Gateway. Learn more about getting started with Secure Gateway. Import AppConnect Cert to Maximo to enable encrypted communication Import AppConnect Cert to TRIRIGA to enable encrypted communication","title":"Before you begin you will need:"},{"location":"mas-tas-home/#downloadable-resources","text":"Download the zip file that has all of the flows and configuration files.","title":"Downloadable Resources"},{"location":"mas-tas-home/#installation-steps-overview","text":"Install Connector Configure App Connect a. Authenticate b. Import Flows Configure TRIRIGA a. Import TRIRIGA OM Package b. Point integration object to App Connect Flow Configure Maximo a. Add necessary fields b. Publish channel to point to App Connect Flow Test a. MAS outbound connectivity b. TRIRIGA POSTMAN","title":"Installation Steps Overview"},{"location":"mas-tas-home/#part-1-install-connector","text":"","title":"Part 1. Install Connector"},{"location":"mas-tas-home/#mas-810","text":"The MAS Connector to TAS in Manage can be installed on the MAS administration dashboard through the tile. Search for TRIRIGA from the Catalog page and select the TRIRIGA Connector tile. On the tile page, click Configure and it will take you to the update configuration page. Select the latest version of the connector under the Components section and then click Apply Changes at the top of the page and Confirm on the next window. Step 1: Select Connector Tile Step 2: Apply Changes for Connector The process can take anywhere between 20 minutes to 1-2 hours depending on the cluster resources. Monitor the logs in the created build pods in the Manage namespace in the OpenShift cluster to see the progress.","title":"MAS 8.10+"},{"location":"mas-tas-home/#mas-88-89","text":"The MAS Connector to TAS in Manage can be installed by using the proper customization .zip file provided from Passport Advantage. This .zip file will have all of the configurations necessary to complete step 4 in the Installation process. Navigate to the Admin dashboard of the instance of Manage within MAS. Select the workspace the instance is deployed on and update the configuration. Scroll down to Customization and link to the location of the .zip file in the field ( Additional details on setting customizations) Click Apply Changes and Manage will update the instance with the customization. The process can take anywhere between 20 minutes to 1-2 hours depending on the cluster resources. Monitor the logs in the created build pods in the Manage namespace in the OpenShift cluster to see the progress.","title":"MAS 8.8 &amp; 8.9"},{"location":"mas-tas-home/#maximo-7612","text":"On the Maximo Admin workstation, overlay the Maximo SMP directory ( /opt/IBM/SMP/maximo ) with the contents from the solution zip file that is provided. This will lay down the Java Classes and .dbc files provided with the solution. Shutdown the MXServer Run UpdateDB command to install the solution components a. Navigate to /opt/IBM/SMP/maximo/tools/maximo b. Run ./updatedb.sh Build the Maximo EAR file a. Navigate to /opt/IBM/SMP/maximo/deployment b. Run ./buildmaximoear.sh Deploy the new Maximo EAR File on all Maximo servers a. In WebSphere console, navigate to Applications->Application types->Websphere enterprise applications b. Select MAXIMO and then hit Update button c. Select Browse and select the EAR file from Step 4 d. Hit Next and accept defaults from all pages e. After deployment, click on Save Start the applications and MXServer","title":"Maximo 7.6.1.2+"},{"location":"mas-tas-home/#part-2-configure-app-connect","text":"","title":"Part 2. Configure App Connect"},{"location":"mas-tas-home/#app-connect-authentication","text":"Note: IBM App Connect Professional or Enterprise is needed to run these flows. The flows have been tested on IBM Cloud App Connect, AWS App Connect, as well as the containerized version of App Connect. Two accounts need to be created from the Catalog tab in order to connect the applications. Once all of the connectors have loaded, type in http to find the HTTP Application. If this is the first account, select Connect to begin setting up the initial HTTP account. If this is not the first account, make sure to take note if there are any other generic account names present because the number of the one created will depend on what has already been created. App Connect creates an account with a generic name in sequential order (Example: if Account 1 and Account 2 are present, the new account will be Account 3). Use the below table to associate the proper credentials for the accounts. Once the account is connected, head back to the HTTP Application on the Catalog page and rename the new account according to the Account Name column in the table below. Flow Account Name Username Password API key API location API key name Max -> Tri mxtririga Your TAS Username Your TAS Password N/A N/A N/A Tri -> Max trimaximo N/A N/A Your Maximo apikey header apikey App Connect catalog","title":"App Connect Authentication"},{"location":"mas-tas-home/#importing-app-connect-flows","text":"Locate the .yaml files for the desired flows from the provided zip. Import the Selected Flows into App Connect Enterprise , or App Connect SaaS on IBM Cloud using the instructions below","title":"Importing App Connect Flows"},{"location":"mas-tas-home/#import-steps-app-connect-enterprise-cp4i-app-connect-saas-aws","text":"From the App Connect Dashboard, click New and select Import Flow from the drop down menu. Either drag and drop or select the flow for import. In this example, the MX2TRI Person flow will be used. The flow should now be uploaded onto the App Connect instance. From this screen navigate using the Edit flow button to see the individual nodes of this flow. Be sure to select the HTTP account that was configured for Maximo to TRIRIGA for the connector. Click Done on the top right of the screen then click on the three dots in the top right corner and select Start API . Go to the Test tab once it shows that the flow is Running and select the POST option on the left side of the screen. Click on Try It and grab the URL and security credentials from this screen for the next step. Step 1: App Connect Dashboard Step 2: Import a Flow Step 3: Completed Flow Step 4: Start Flow Step 5: Try It","title":"Import Steps - App Connect Enterprise (CP4I) &amp; App Connect SaaS (AWS)"},{"location":"mas-tas-home/#import-steps-app-connect-saas-ibm-cloud","text":"If your instance of AppConnect is through the cloud, your page will look a bit different. The interface is mostly the same, but instead of Try It you'll see a tab called Manage . This page contains a few important pieces of information that you'll need to complete the configuration. First, at the top of the page under API Info you'll see a field called Route . Piece this together with the correct path of the flow that you're implementing in order to create the proper flow URL. Collect the API key at the bottom of the page from a field called Sharing outside of Cloud Foundry organization . Click on Create API key and documentation link . Provide a name and it will generate an apikey for you to use with this flow along with a documentation link that looks like the Test page from the on-prem configuration. The end of the URL at the top of the page should have the path that will complete the route URL. Copy the end of the URL that begins with tri and add it to the piece gathered earlier. The 6 character string that precedes the path on the documentation should match the last 6 characters of the route piece from before. Step 1: Manage Tab Step 2: Cloud Route Step 3: API key creation Step 4: API Documentation Link","title":"Import Steps - App Connect SaaS (IBM Cloud)"},{"location":"mas-tas-home/#running-in-production","text":"Once the configuration of the below applications is complete and the connectivity between systems is working as expected, you will need to deploy the flow as an integration server via the App Connect Dashboard. From the dashboard view of the Designer application, click the three dots on the flow's tile, select Export , and then Runtime flow asset (BAR) . In the App Connect Dashboard instance, select Create Server and then the type of integration you would like to run. The flows work with all types of integration sizes, so pick the one that is right for your deployment. Import the bar file you just downloaded into the section on the Integrations tab. On the Configuration screen, refer to securing a REST-based integration to properly secure the server. Give the intended details of the server such as the name, version, and correct license to use. Click Create when you're ready to deploy. It may take upwards of 5-10 minutes for the server to be up and running. Step 1: Export Bar file Step 2: Create Integration Server Step 3: Final details","title":"Running in Production"},{"location":"mas-tas-home/#part-3-configure-tas","text":"","title":"Part 3. Configure TAS"},{"location":"mas-tas-home/#import-object-migration-package","text":"Import the OM Package labeled APIConnector into the TAS instance. Go to Tools -> Administration -> Object Migration and select New Import Package to begin the import process. Refer to the Object Migration Overview for more information on Object Migration","title":"Import Object Migration Package"},{"location":"mas-tas-home/#tas-api-user-access","text":"In order for App Connect to be able to use TAS APIs, it will need a user with certain permissions. These user's credentials will be configured in App Connect. Create an integration user by following the steps given in Chapter 2 . This user should be a non-admin user and not part of the Admin security group. Assign that user to a new or existing group for the integration. If you need to create a new security group, follow the steps given in Chapter 1 . Add the \"TAS Base License\" to the License Details section on the user Profile. Select the newly created group or desired existing group and switch to the Access tab and add the appropriate access for the integration.","title":"TAS API User Access"},{"location":"mas-tas-home/#outbound-traffic-from-tas","text":"For outbound traffic from TAS, grant at least READ access on the Business Objects that will be used. The table below shows the various supported business Objects the API can pull from: Module Business Object Label Asset Building Equipment Classification Request Class Classification Space Class Current Classification Asset Spec Class People People Location Property Location Building Location Floor Location Space Organization Organization Request Service Request Task Work Task In the example below, the API user is able to pull data from the Building Business Object:","title":"Outbound Traffic from TAS"},{"location":"mas-tas-home/#inbound-traffic-to-tas","text":"For inbound traffic, Data Access needs to be enabled as well as Application Access permissions to the triAPIConnect Module or the individual Objects. To enable an API user to create a building, grant access to the triAPICBuilding Business object as shown below:","title":"Inbound traffic to TAS"},{"location":"mas-tas-home/#date-time-format","text":"The Date Time Format field in the user profile must be in UTC. Navigate to Portfolio -> People -> My Profile and select the user profile that will be triggering the action. The Date Time Format should be in UTC as shown below. Date Time Format Field in My Profile","title":"Date Time Format"},{"location":"mas-tas-home/#configure-integration-object","text":"Configure the Integration Object - From the main page of TAS, click on Tools -> System Setup -> Integration -> Integration Object . Under the Name column, type in apic , and select the integration object that pertains to the record that is getting sent. Click on the object and fill in the credentials in the pop-up box. TAS End Point","title":"Configure Integration Object"},{"location":"mas-tas-home/#part-4-maximo-configuration","text":"Note 1 If you have not already done so, please import AppConnect Cert to Maximo to enable encrypted communication. Note 2 : The following steps and prerequisites are done against a Maximo demo database. The naming conventions may slightly differ from this, but these are the necessary components. Within Maximo, configure your instance to be ready to receive records from TAS. If these pre-requisites are not completed, the action will not be recorded.","title":"Part 4. Maximo Configuration"},{"location":"mas-tas-home/#1-create-an-organization-named-tririga","text":"a. Navigate to the Organizations page and click the blue + button on the top row. b. Fill in the Organization name with TRIRIGA and the description as \"TAS Organization\". c. Fill in the remaining required fields as such: Field Name Value Base Currency 1 USD Item Set SET1 Company Set COMPSET1 Default Item Status PENDING Default Stock Category STK d. Click Save Organization on the left side of the screen under Common Actions. This will be set to Active later once there is a clearing account.","title":"1. Create an Organization named TRIRIGA"},{"location":"mas-tas-home/#2-create-a-testing-clearing-account-in-chart-of-accounts","text":"Un-require the GL Account fields a. Navigate to Financial -> Chart of Accounts and click on the previously created TAS org in the Organizations table. Currently, there should be no GL Accounts for TAS present. b. Click GL Component Maintenance on the left side under More Actions and add a New Row with the following values: Field Name Value GL Component Value 1001 Description Testing Active? Yes c. Click OK . Click New Row under GL Accounts for TRIRIGA and click the magnifying glass to search for that GL Component. Select it and it should populate in the GL Account and Description fields. The Active Date field should auto populate to the current date. d. Now that this account is present, head back to Organizations and update the TRIRIGA organization to show the just created Clearing Account, tick the Active box, and click Save Organization .","title":"2. Create a Testing clearing account in Chart of Accounts"},{"location":"mas-tas-home/#3-create-a-site-trimain-and-set-it-to-active","text":"a. On the Organization page, click on the Sites tab at the top of the page. b. Click New Row under Sites and enter TRIMAIN for Site and MAIN Site for Description. Set the site to Active. c. Click Save Organization .","title":"3. Create a site TRIMAIN and set it to active"},{"location":"mas-tas-home/#4-api-key","text":"","title":"4. API Key"},{"location":"mas-tas-home/#mas","text":"Navigate to Integration -> API Keys . Click on the button that reads Add API key Select user MXINTADM and click the Add button to generate an API key for this user. Securely store this API key for later use.","title":"MAS"},{"location":"mas-tas-home/#maximo-7612_1","text":"Navigate to Administration -> Administration and a new tab/window should open with the Maximo-X application. You should be on a page titled 'Integration'. Click on the tab at the top of the page that says API Keys and click on the button with the blue plus sign that reads Add API key Select user MXINTADM and click the Add button to generate an API key for this user. Securely store this API key for later use.","title":"Maximo 7.6.1.2"},{"location":"mas-tas-home/#5-integration-controls","text":"a. Head to Enterprise Services and click on Create Integration Controls . These integration controls help translate specific external values into values Maximo understands. The following tables contain values to set up this demo, but can be customized to fit your TAS and Maximo naming conventions. There should be 5 X-Ref Integration Controls created with the following associations: Integration Control MAXIMO Value External Value Description Domain PLUSILOCSTATUS ACTIVE ACTIVE TAS Location Status mapping for inbound flows LOCASSETSTATUS PLUSILOCSTATUS INACTIVE REVIEW IN PROGRESS N/A N/A PLUSILOCSTATUS OPERATING OPERATING N/A N/A PLUSIORG TRIMAIN IBM Organization mapping for TAS N/A PLUSIORG TAS TAS N/A N/A PLUSIORGEN TAS EAGLENA TAS Organization mapping for Inbound flow N/A PLUSIORGEN TAS IBM N/A N/A PLUSIORGEN TAS MAXIMO ORG N/A N/A PLUSIORGEN TAS TAS N/A N/A PLUSIPRIORITY 1 High Priority mapping for TAS N/A PLUSIPRIORITY 2 Medium N/A N/A PLUSIPRIORITY 3 Low N/A N/A PLUSISITEEN TRIMAIN BEDFORD TAS Location mapping for inbound flows N/A PLUSISITEEN TRIMAIN SPACE 01 N/A N/A PLUSISITEEN TRIMAIN TRIMAIN N/A N/A b. Once these Integration Controls are created, associate them in both the created Enterprise Services and Publish Channels by using the following two tables Enterprise Service Control PLUSIASSET PLUSIORGEN PLUSIASSET PLUSIPRIORITY PLUSIASSET PLUSISITEEN PLUSILOCATION PLUSILOCSTATUS PLUSILOCATION PLUSISITEEN PLUSILOCATION PLUSISTATUS PLUSIPERSON PLUSIORGEN PLUSIPERSON PLUSISITEEN PLUSIWO PLUSIWOPRIORITY Publish Channel Control PLUSIASSET PLUSIPRIORITY PLUSILOCATION N/A PLUSIPERSON PLUSIORG PLUSISR N/A PLUSIWO PLUSIWOPRIORITY PLUSIWO PLUSIWOSTART c. Return to the PLUSITRIRIGA External System. On the left side of the External Systems page, select Setup Integration Controls under More Actions and make sure that all 5 Integration Controls are showing as present.","title":"5. Integration Controls"},{"location":"mas-tas-home/#6-enable-object-structure-security","text":"The user needs to be able to transact against the specific object structureS in Manage. Navigate to Object Structures and search for MXPERSON. On the left side of the MXPERSON screen select Configure Object Structure Storage and turn on the button underneath Use Object Structure for Authorization Name? Configure Object Security Screen The structure should save. Complete this process for the following objects: MXASSET MXOPERLOC MXSR MXWO MXDOMAIN Next, navigate to Security -> Security Groups and select the security group the current user is apart of. Click the Object Structures tab and filter search for MXPERSON. Once selected, click Grant Listed Options for This Object Structure and Save the Group. Security Group Page Note Repeat this process for the other changed structures.","title":"6. Enable Object Structure Security"},{"location":"mas-tas-home/#7-application-designer-changes","text":"Go to System Configuration -> Platform Configuration -> Application Designer","title":"7. Application Designer Changes"},{"location":"mas-tas-home/#person","text":"Search for PERSON , switch to the Person Tab and select a section to add three new boxes. At the top, click the icon labeled Control Palette and add a Multipart Textbox at the bottom of the section. Add the values from the below table within the properties of the Multipart Textbox and click Save Definition Note Be sure that the Attributes are taken from the PERSON Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TAS Location Path PLUSIPRIMARYLOCPATH PLUSILOCATIONPATH.DESCRIPTION VALUELIST Readonly Multipart Textbox TAS Primary Organization PLUSIORGPATH PLUSIORGANIZATIONPATH.DESCRIPTION VALUELIST Readonly Textbox TAS Record ID EXTERNALREFID ( From the Person Object ) N/A N/A N/A Follow the same directions using the following information for the other objects that will be used in the integration","title":"Person"},{"location":"mas-tas-home/#asset","text":"Search for ASSET in Application Designer Note Be sure that the Attributes are taken from the ASSET Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TAS Building Equipment Spec PLUSIASSETSPECNAME PLUSIASSETSPECCLASS.DESCRIPTION VALUELIST Readonly Multipart Textbox TAS Primary Organization PLUSIORGPATH PLUSIORGANIZATIONPATH.DESCRIPTION VALUELIST Readonly Multipart Textbox TAS Location Path PLUSILOCPATH PLUSILOCATIONPATH.DESCRIPTION VALUELIST Readonly Textbox TAS Record ID EXTERNALREFID ( From the Asset Object ) N/A N/A N/A","title":"Asset"},{"location":"mas-tas-home/#location","text":"Search for LOCATION in Application Designer Note Be sure that the Attributes are taken from the LOCATION Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TAS Space Classification PLUSISPACECLASSIFICATION PLUSISPCCLASSIFICATION.DESCRIPTION VALUELIST Readonly Multipart Textbox TAS Parent Location PLUSIPARENTLOCATION PLUSIPARENTPATH.DESCRIPTION VALUELIST Readonly Textbox TAS Record ID EXTERNALREFID ( From the Location Object ) N/A N/A N/A","title":"Location"},{"location":"mas-tas-home/#service-request","text":"Search for SR in Application Designer Note Be sure that the PLUSIREQCLASSID Attribute is taken from the TICKET Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TAS Request Classification PLUSIREQCLASSID PLUSIREQCLASS.DESCRIPTION VALUELIST Readonly Multipart Textbox TAS Primary Organization PLUSIORGPATH PLUSIORGANIZATIONPATH.DESCRIPTION VALUELIST Readonly Multipart Textbox TAS Parent Location PLUSIPARENTLOCATION PLUSIPARENTPATH.DESCRIPTION VALUELIST Readonly Textbox TAS Record ID EXTERNALREFID ( From the Ticket Object ) N/A N/A N/A e. Click Save Definition after the changes are added.","title":"Service Request"},{"location":"mas-tas-home/#work-order","text":"Search for WOTRACK in Application Designer Note Be sure that the PLUSIREQCLASSID Attribute is taken from the WORKORDER Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TAS Location Path PLUSILOCPATH PLUSILOCATIONPATH.DESCRIPTION VALUELIST Readonly Multipart Textbox TAS Primary Organization PLUSIORGPATH PLUSIORGANIZATIONPATH.DESCRIPTION VALUELIST Readonly Textbox External Ref ID EXTERNALREFID ( From the WorkOrder Object ) N/A N/A N/A Click Save Definition after the changes are added.","title":"Work Order"},{"location":"mas-tas-home/#8-tas-to-maximo-service-request-comment-attachment-support","text":"The PLUSITRIServiceRequest2MX_v_1_1_0 integration supports both Comments and Attachments from TAS. This includes pictures or images that are attached as either Comments or Attachments in TAS. Some configuration will need to be done in order to allow for this.","title":"8. TAS to Maximo Service Request Comment &amp; Attachment Support"},{"location":"mas-tas-home/#db-config","text":"Go to Database Configuration and pull up the WORKLOG object. Add 'EXTERNALREFID' as an Attribute with a type ALN and 50 length. Turn on Admin Mode and Run Database Configuration for this change as it will need to be referenced in the Index. Once EXTERNALREFID has been added, navigate to Indexes and add the index 'PLUSIEXTREF'. Under the Columns table make sure to reference the EXTERNALREFID that was just created as well as to leave 'Enforce Uniqueness' unchecked. Turn on Admin Mode and run Database Configuration again to have these changes take effect.","title":"DB Config"},{"location":"mas-tas-home/#object-structure","text":"Navigate to Object Structure and find MXSR. Duplicate this Object Structure and rename it PLUSISRDOCS. Add two new Source Objects to the Structure underneath SR: \"DOCLINKS\" and \"WORKLOG\". Make sure the Relationships match with the following image. - Select the WORKLOG Source Object and in the 'Details' section add the 'PLUSIEXTREF' Index to the Alternate Key field. Then navigate to Exclude/Include Fields. Click on Non-Persistent Fields in the window and make sure 'Description_LongDescription' is selected as Included.","title":"Object Structure"},{"location":"mas-tas-home/#enterprise-services","text":"Now that the changes are made in database and object structure level, they need to be referenced by the Enterprise Service. To do this, duplicate the Default PLUSISR Enterprise Service and name this new one 'PLUSISRDOCS' that references the newly created Object Structure. Remove the default PLUSISR Enterprise Service from the PLUSITRIRIGA External System in order to Delete the Enterprise Service. Once deleted, duplicate PLUSISRDOCS and name it PLUSISR to replace the deleted Enterprise Service. Add the new PLUSISR back into the External System. Now when a Service Request comes in with a comment or attachment, Maximo will keep track of the comments in the Worklog and images/documents in the Attachments.","title":"Enterprise Services"},{"location":"mas-tas-home/#part-5-testing","text":"To test that the configuration is complete, send a test payload in order to test connectivity.","title":"Part 5: Testing"},{"location":"mas-tas-home/#maximo","text":"Go to the End Points application and click Test at the bottom of the integration you would like to test. Send a test payload that is a valid object. {\"Hello\":\"World\"} would work. If the response is anything other than Bad Request , see what might be causing the error","title":"Maximo"},{"location":"mas-tas-home/#tas","text":"Use a tool like POSTMan to test the connectivity of the App Connect flow. You can use a Sample JSON Payload from this open source repository .","title":"TAS"},{"location":"mas-tas-home/#part-6-data-pre-requisites-for-the-integrations","text":"Because of internal references within the TRIRIGA and Maximo data models some flows depend on other flows to function. The following tables describe these relationships. You will need to run the pre-requisite flows first before running the main flows.","title":"Part 6: Data Pre-requisites for the Integrations"},{"location":"mas-tas-home/#maximo-to-tas","text":"Flow Flows to run first MXPerson2TRI TRISpace2MX, TRILocPath2MX, TRIOrg2MX MXAsset2TRI TRISpace2MX, TRILocPath2MX, TRIOrg2MX, TRIAssetSpec2MX MXLocation2TRI TRILocPath2MX, TRISpaceClass2MX MXServiceReq2TRI TRILocPath2MX, TRIReqClass2MX, TRIPerson2MX MXWorkOrder2TRI TRILocPath2MX, TRIOrg2MX","title":"Maximo to TAS"},{"location":"mas-tas-home/#tas-to-maximo","text":"Flow Flows to run first TRIPerson2MX TRISpace2MX, TRILocPath2MX, TRIOrg2MX TRIAsset2MX TRISpace2MX, TRILocPath2MX, TRIOrg2MX, TRIAssetSpec2MX TRISpace2MX TRILocPath2MX, TRISpaceClass2MX TRIServiceReq2MX TRILocPath2MX, TRIReqClass2MX, TRIPerson2MX TRIWorkOrder2MX TRILocPath2MX, TRIOrg2MX PeopleBatch TRISpace2MX, TRILocPath2MX, TRIOrg2MX AssetBatch TRISpace2MX, TRILocPath2MX, TRIOrg2MX, TRIAssetSpec2MX SpaceBatch TRILocPath2MX, TRISpaceClass2MX","title":"TAS to Maximo"},{"location":"mas-tas-home/#troubleshooting","text":"Depending on the direction of the flow, cross-referencing errors from two systems can help identify the root cause of an issue with the integration. For example: If there is a Not Found error in Maximo Message Reprocessing when running the flow, double check the logs in App Connect to see if there is a corresponding error. If there is, the root cause might be related to what is being sent out of Maximo. If there isn't, then the message never left Maximo and the flow should be checked to make sure it is running.","title":"Troubleshooting"},{"location":"mas-tas-home/#where-to-find-logs-in-app-connect","text":"","title":"Where to find logs in App Connect"},{"location":"mas-tas-home/#app-connect-enterprise","text":"The logs in App Connect Enterprise are found in the Integration Server pod. If running from App Connect Designer, this integration server is found in the App Connect namespace in OpenShift. Typically the Designer integration server has 4 pods and the logs can be found in the pod with a similar naming convention to des-01-quickstart-ma-designer-designer-flows . App Connect Enterprise","title":"App Connect Enterprise"},{"location":"mas-tas-home/#app-connect-saas-aws","text":"The logs in App Connect SaaS and AWS can be found within the application itself by clicking on the clipboard icon on the left side task bar. App Connect SaaS or AWS","title":"App Connect SaaS &amp; AWS"},{"location":"mas-tas-home/#common-errors-that-arise-from-maximo","text":"","title":"Common errors that arise from Maximo"},{"location":"mas-tas-home/#testing-the-end-point","text":"When testing that the end point is entered correctly on the End Points application, there are two common errors: Error Cause Resolution Response code received from the HTTP request from the endpoint is not successful Invalid URL in the Integration Object Double check the URL that all of the components are entered correctly. Make sure there are no accidental spaces at the beginning or the end in event of a copy/paste. 404: Not Found Flow is not Running Make sure the flow is Running before starting the cron tasks PKSync error Certificate error Confirm the certificate is configured correctly","title":"Testing the End Point"},{"location":"mas-tas-home/#message-reprocessing","text":"Errors in Maximo can be found in Message Reprocessing Error Cause Resolution Bad Request error Either the payload being sent out is incorrect or the account configurations are wrong Double check to make sure the Accounts from the App Connect pre-requisite section are correct and that the item in Maximo has all of the mapped fields","title":"Message Reprocessing"},{"location":"mas-tas-home/#common-errors-that-arise-from-tas","text":"Error Cause Resolution 404 - Not Found: Cannot POST Invalid URL in the Integration Object Double check the URL in the specific Integration Object that all of the components are entered correctly 404 - API doesn't exist Flow is not running Double check that the flow is Active in App Connect 404 - The HTTP request returned with an error 404 \"Not Found\" Incorrect App Connect connector config Double check that the credentials being used in the HTTP post node in App Connect are correct 401 - Authorization error Too many user sessions open in TAS Open the Admin dashboard on the TAS environment and check the Users logged in. This issue can arise after a number of requests are made to TAS and then gives a 401 error even with the proper credentials. Clear the users logged in and the issue should clear. Clear OSLC Cache in TAS Admin Console in case the integrations do not work in intended manner.","title":"Common errors that arise from TAS"},{"location":"mas-tas-home/#reference-for-pre-requisite","text":"","title":"Reference for Pre-requisite"},{"location":"mas-tas-home/#pre-requisite-add-an-app-connect-certificate-in-mas-89","text":"Extract the App Connect certificate from an imported flow URL. Navigate to the flow's page and click on Test and then Try It to get the proper URL. Navigate to the Admin dashboard for MAS and go to the workspace where Manage is deployed. Update the configuration and scroll down to Imported Certificates . Untick the System managed button and fill in the extracted certificate in the fields. Click Apply Changes at the top of the page and MAS will update the truststore with the new certificate. Step 2: Manage Workspace Step 3: Imported Certificates","title":"Pre-requisite: add an App Connect Certificate in MAS 8.9+"},{"location":"mas-tas-home/#pre-requisite-add-an-app-connect-certificate-in-maximo-7612","text":"Configure WebSphere Certificates. This makes a test connection to a Secure Sockets Layer (SSL) port and retrieves the signer from the server during the handshake. Log into the WebSphere console that is hosting the Maximo server. Click on Security -> SSL certificate & key management . Under Related Items click on Key stores and certificates . Click on CellDefaultTrustStore and on the next page under Additional Properties click on Signer certificates . From this page, click on the button that says Retrieve from Port and fill in the required fields using the table below: Field Value Host The host from the imported flow URL Port 443 Alias appconnect Once all three have been entered in, click Retrieve signer information and the information from the URL will populate on screen. Click Save in the box at the top and then repeat the process for NodeDefaultTrustStore . Step 2: WebSphere Homepage Step 3: Websphere Keystores Step 4: Websphere Signer Certs","title":"Pre-requisite: add an App Connect Certificate in Maximo 7.6.1.2+:"},{"location":"mas-tas-home/#pre-requisite-update-tas-custom-resource-with-app-connect-certificates","text":"In OpenShift, import the following yaml with the App Connect certificates added under the spec.alias.crt section: cat <<EOF | oc create -f - apiVersion: truststore-mgr.ibm.com/v1 kind: Truststore metadata: name: my-tas-truststore spec: license: accept: true includeDefaultCAs: true servers: - \"example.com:443\" certificates: - alias: alias_1 crt: | -----BEGIN CERTIFICATE----- ... Certificate 1 ... -----END CERTIFICATE----- ... - alias: alias_n crt: | -----BEGIN CERTIFICATE----- ... Certificate n ... -----END CERTIFICATE----- EOF If there is already a truststore for TAS, update the truststore with the App Connect certificate.","title":"Pre-Requisite: Update TAS Custom Resource with App Connect Certificates:"},{"location":"mas-tas/","text":"Portfolio Data Integration - To be deleted Summary Enable automated synchronization of portfolio data between TRIRIGA and Maximo Asset Management systems to integrate asset and facility management operations. Description In this code pattern, learn how to synchronize bi-directional portfolio data of IBM TRIRIGA and IBM Maximo Asset Management using App Connect Designer flows for synchronizing People, Locations, and Assets. Using the provided flows, enable a variety of use cases: from a simple trigger-action integration that updates or populates the records in the other application, to more complex scripted cron jobs that orchestrate these objects into their respective primary system of record. When a record is updated in Maximo Asset Management, it triggers the flow to synchronize with TRIRIGA. (There is also a flow that works in the reverse direction, that works in a similar way.) App Connect sends a request with the updated information through the flow towards the target system (TRIRIGA). A JSON Parser sifts through the request and converts it to an object. Each object that is returned from the JSON Parser goes through Steps 5-8. The data from the object is mapped to the corresponding fields in the target application (TRIRIGA). The newly mapped data is sent to the target application (TRIRIGA) where the record is created or updated. This record is then validated with the original application (Maximo Asset Management) via another Post request. An ID is either created or updated within the original application (Maximo Asset Management). Steps 4-8 are repeated for each object that is present in the original request (Maximo Asset Management). At the end of this process, a Person, Asset, or Location can be sent from Maximo to TRIRIGA and TRIRIGA to Maximo utilizing App Connect. A detailed breakdown of the specific fields being mapped within each flow can be found in this Mapping Document Prerequisites 1. Application Designer Changes SR Application Designer Go to System Configuration -> Platform Configuration -> Application Designer Person Search for PERSON , switch to the Person Tab and select a section to add three new boxes. At the top, click the icon labeled Control Palette and add a Multipart Textbox at the bottom of the section. Add the values from the below table within the properties of the Multipart Textbox and click Save Definition Note Be sure that the Attributes are taken from the PERSON Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TRIRIGA Location Path PLUSIPRIMARYLOCPATH PLUSILOCATIONPATH.DESCRIPTION VALUELIST Readonly Multipart Textbox TRIRIGA Primary Organization PLUSIORGPATH PLUSIORGANIZATIONPATH.DESCRIPTION VALUELIST Readonly Textbox TRIRIGA Record ID EXTERNALREFID ( From the Person Object ) N/A N/A N/A Follow the same directions using the following information for Asset and Location Asset Search for ASSET in Application Designer Note Be sure that the Attributes are taken from the ASSET Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TRIRIGA Building Equipment Spec PLUSIASSETSPECNAME PLUSIASSETSPECCLASS.DESCRIPTION VALUELIST Readonly Multipart Textbox TRIRIGA Primary Organization PLUSIORGPATH PLUSIORGANIZATIONPATH.DESCRIPTION VALUELIST Readonly Multipart Textbox TRIRIGA Location Path PLUSILOCPATH PLUSILOCATIONPATH.DESCRIPTION VALUELIST Readonly Textbox TRIRIGA Record ID EXTERNALREFID ( From the Asset Object ) N/A N/A N/A Location Search for LOCATION in Application Designer Note Be sure that the Attributes are taken from the LOCATION Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TRIRIGA Space Classification PLUSISPACECLASSIFICATION PLUSISPCCLASSIFICATION.DESCRIPTION VALUELIST Readonly Multipart Textbox TRIRIGA Parent Location PLUSIPARENTLOCATION PLUSIPARENTPATH.DESCRIPTION VALUELIST Readonly Textbox TRIRIGA Record ID EXTERNALREFID ( From the Location Object ) N/A N/A N/A Step 1 - Select an App Connect Flow for deployment Locate the .yaml file for the deployment direction (MX2TRI or TRI2MX) based on the system of record and download to the local machine. From the previous example, use the MX2TRI flow in the Person row. Asset Maximo TRIRIGA Person MX2TRI TRI2MX Asset MX2TRI TRI2MX Location MX2TRI TRI2MX Make sure to check the Mapping Document that the correct fields are being mapped in the selected flow. Step 2 - Import the Selected Flow in App Connect Follow the below steps to import the flow. Import Steps (ACE) From the App Connect Dashboard, click New and select Import Flow from the drop down menu. Either drag and drop or select the flow for import. In this example, the MX2TRI Person flow will be used. The flow should now be uploaded onto the App Connect instance. From this screen navigate using the Edit flow button to see the individual nodes of this flow. Be sure to select the HTTP account that was configured for Maximo to TRIRIGA for the connector. Click Done on the top right of the screen then click on the three dots in the top right corner and select Start API . Go to the Test tab once it shows that the flow is Running and select the POST option on the left side of the screen. Click on Try It and grab the url and security credentials from this screen for the next step. Step 1: App Connect Dashboard Step 2: Import a Flow Step 3: Completed Flow Import Steps (App Connect Cloud) If your instance of AppConnect is through the cloud, your page will look a bit different The interface is mostly the same, but instead of Try It you'll see a tab called Manage . This page contains a few important pieces of information that you'll need to complete the configuration. First, at the top of the page under API Info you'll see a field called Route . Piece this together with the correct path of the flow that you're implementing in order to create the proper flow url. Collect the API key at the bottom of the page from a field called Sharing outside of Cloud Foundry organization . Click on Create API key and documentation link . Provide a name and it will generate an apikey for you to use with this flow along with a documentation link that looks like the Test page from the on-prem configuration. The end of the url at the top of the page should have the path that will complete the route url. Copy the end of the url that begins with tri and add it to the piece gathered earlier. The 6 character string that precedes the path on the documentation should match the last 6 characters of the route piece from before. Step 1: Manage Tab Step 2: Cloud Route Step 3: API key creation Step 4: API Documentation Link Step 3 - Configure Maximo and TRIRIGA instances with App Connect urls Using the credentials from the end of Step 2, populate the instances of Maximo and TRIRIGA with the correct End Points of the recently created flow. Maximo From the main page of Maximo, click the menu icon on the top left and navigate to Integration -> End Points . Fill in the properties with the url, username, and password from Step 2. Click the Test button at the bottom right of the screen and send a simple {\"hello\":\"world\"}. With the proper configuration, there will be an expected error that ends with Bad Request . If there is a different error than Bad Request in the Response window, refer to the Troubleshooting section to debug. If the instance is based in the cloud, there is a slight difference in authorization. Since the authorization is with an api key, remove the basic authorization values (USERNAME & PASSWORD) and in the HEADERS field add the API key after Content-Type. The Headers field should look something like this: Content-Type: application/json, X-IBM-Client-Id: [apikey from step 2] Step 1: Maximo End Point app On-prem End Point properties Cloud End Point properties TRIRIGA From the main page of TRIRIGA, click on Tools -> System Setup -> Integration -> Integration Object . Under the Name column, type in apic , and select the integration object that pertains to the record that is getting sent. Click on the object and fill in the credentials in the pop-up box. TRIRIGA End Point Step 4 - Test the Flow With these 3 steps completed, test the flow with a payload. Head back to the Try It page in the deployed flow (or the documentation page if cloud-based) and scroll down to the bottom of the page under Parameters. In here, fill in mxUrl and triUrl with the urls for the respective applications, generate a test payload to send through the flow and monitor if the information populates in the desired application. If there is a response other than 200 from the Test, refer to Troubleshooting. App Connect Test References Mapping Document Troubleshooting Common errors that arise from App Connect #### Testing the whole flow If there is a 404 Not Found error when trying to test the flow, this could mean that the flow is not running. Double check to make sure the flow shows a green dot and says Running after edits have been made. If there is a 400 Bad Request error when testing the flow, this could mean the wrong account configurations. Double check to make sure the Accounts from the App Connect pre-requisite section are correct. Common errors that arise from Maximo #### Testing the End Point If an error reads \"The response code received from the HTTP request from the endpoint is not successful.\", this is related to the configured End Point. Double check and make sure the values in End Points are correct. Make sure there are no accidental spaces at the beginning or end in the event of the values being copy/pasted. If an error comes back as a PKSync error , this is related to the certificates in WebSphere. Double check and confirm that the certificates from Step 4 are correctly configured. Common errors that arise from TRIRIGA Clear OSLC Cache in TRIRIGA Admin Console in case the integrations do not work in intended manner.","title":"Portfolio Data Integration - To be deleted"},{"location":"mas-tas/#portfolio-data-integration-to-be-deleted","text":"","title":"Portfolio Data Integration - To be deleted"},{"location":"mas-tas/#summary","text":"Enable automated synchronization of portfolio data between TRIRIGA and Maximo Asset Management systems to integrate asset and facility management operations.","title":"Summary"},{"location":"mas-tas/#description","text":"In this code pattern, learn how to synchronize bi-directional portfolio data of IBM TRIRIGA and IBM Maximo Asset Management using App Connect Designer flows for synchronizing People, Locations, and Assets. Using the provided flows, enable a variety of use cases: from a simple trigger-action integration that updates or populates the records in the other application, to more complex scripted cron jobs that orchestrate these objects into their respective primary system of record. When a record is updated in Maximo Asset Management, it triggers the flow to synchronize with TRIRIGA. (There is also a flow that works in the reverse direction, that works in a similar way.) App Connect sends a request with the updated information through the flow towards the target system (TRIRIGA). A JSON Parser sifts through the request and converts it to an object. Each object that is returned from the JSON Parser goes through Steps 5-8. The data from the object is mapped to the corresponding fields in the target application (TRIRIGA). The newly mapped data is sent to the target application (TRIRIGA) where the record is created or updated. This record is then validated with the original application (Maximo Asset Management) via another Post request. An ID is either created or updated within the original application (Maximo Asset Management). Steps 4-8 are repeated for each object that is present in the original request (Maximo Asset Management). At the end of this process, a Person, Asset, or Location can be sent from Maximo to TRIRIGA and TRIRIGA to Maximo utilizing App Connect. A detailed breakdown of the specific fields being mapped within each flow can be found in this Mapping Document","title":"Description"},{"location":"mas-tas/#prerequisites","text":"","title":"Prerequisites"},{"location":"mas-tas/#1-application-designer-changes","text":"SR Application Designer Go to System Configuration -> Platform Configuration -> Application Designer","title":"1. Application Designer Changes"},{"location":"mas-tas/#person","text":"Search for PERSON , switch to the Person Tab and select a section to add three new boxes. At the top, click the icon labeled Control Palette and add a Multipart Textbox at the bottom of the section. Add the values from the below table within the properties of the Multipart Textbox and click Save Definition Note Be sure that the Attributes are taken from the PERSON Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TRIRIGA Location Path PLUSIPRIMARYLOCPATH PLUSILOCATIONPATH.DESCRIPTION VALUELIST Readonly Multipart Textbox TRIRIGA Primary Organization PLUSIORGPATH PLUSIORGANIZATIONPATH.DESCRIPTION VALUELIST Readonly Textbox TRIRIGA Record ID EXTERNALREFID ( From the Person Object ) N/A N/A N/A Follow the same directions using the following information for Asset and Location","title":"Person"},{"location":"mas-tas/#asset","text":"Search for ASSET in Application Designer Note Be sure that the Attributes are taken from the ASSET Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TRIRIGA Building Equipment Spec PLUSIASSETSPECNAME PLUSIASSETSPECCLASS.DESCRIPTION VALUELIST Readonly Multipart Textbox TRIRIGA Primary Organization PLUSIORGPATH PLUSIORGANIZATIONPATH.DESCRIPTION VALUELIST Readonly Multipart Textbox TRIRIGA Location Path PLUSILOCPATH PLUSILOCATIONPATH.DESCRIPTION VALUELIST Readonly Textbox TRIRIGA Record ID EXTERNALREFID ( From the Asset Object ) N/A N/A N/A","title":"Asset"},{"location":"mas-tas/#location","text":"Search for LOCATION in Application Designer Note Be sure that the Attributes are taken from the LOCATION Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox TRIRIGA Space Classification PLUSISPACECLASSIFICATION PLUSISPCCLASSIFICATION.DESCRIPTION VALUELIST Readonly Multipart Textbox TRIRIGA Parent Location PLUSIPARENTLOCATION PLUSIPARENTPATH.DESCRIPTION VALUELIST Readonly Textbox TRIRIGA Record ID EXTERNALREFID ( From the Location Object ) N/A N/A N/A","title":"Location"},{"location":"mas-tas/#step-1-select-an-app-connect-flow-for-deployment","text":"Locate the .yaml file for the deployment direction (MX2TRI or TRI2MX) based on the system of record and download to the local machine. From the previous example, use the MX2TRI flow in the Person row. Asset Maximo TRIRIGA Person MX2TRI TRI2MX Asset MX2TRI TRI2MX Location MX2TRI TRI2MX Make sure to check the Mapping Document that the correct fields are being mapped in the selected flow.","title":"Step 1 - Select an App Connect Flow for deployment"},{"location":"mas-tas/#step-2-import-the-selected-flow-in-app-connect","text":"Follow the below steps to import the flow.","title":"Step 2 - Import the Selected Flow in App Connect"},{"location":"mas-tas/#import-steps-ace","text":"From the App Connect Dashboard, click New and select Import Flow from the drop down menu. Either drag and drop or select the flow for import. In this example, the MX2TRI Person flow will be used. The flow should now be uploaded onto the App Connect instance. From this screen navigate using the Edit flow button to see the individual nodes of this flow. Be sure to select the HTTP account that was configured for Maximo to TRIRIGA for the connector. Click Done on the top right of the screen then click on the three dots in the top right corner and select Start API . Go to the Test tab once it shows that the flow is Running and select the POST option on the left side of the screen. Click on Try It and grab the url and security credentials from this screen for the next step. Step 1: App Connect Dashboard Step 2: Import a Flow Step 3: Completed Flow","title":"Import Steps (ACE)"},{"location":"mas-tas/#import-steps-app-connect-cloud","text":"If your instance of AppConnect is through the cloud, your page will look a bit different The interface is mostly the same, but instead of Try It you'll see a tab called Manage . This page contains a few important pieces of information that you'll need to complete the configuration. First, at the top of the page under API Info you'll see a field called Route . Piece this together with the correct path of the flow that you're implementing in order to create the proper flow url. Collect the API key at the bottom of the page from a field called Sharing outside of Cloud Foundry organization . Click on Create API key and documentation link . Provide a name and it will generate an apikey for you to use with this flow along with a documentation link that looks like the Test page from the on-prem configuration. The end of the url at the top of the page should have the path that will complete the route url. Copy the end of the url that begins with tri and add it to the piece gathered earlier. The 6 character string that precedes the path on the documentation should match the last 6 characters of the route piece from before. Step 1: Manage Tab Step 2: Cloud Route Step 3: API key creation Step 4: API Documentation Link","title":"Import Steps (App Connect Cloud)"},{"location":"mas-tas/#step-3-configure-maximo-and-tririga-instances-with-app-connect-urls","text":"Using the credentials from the end of Step 2, populate the instances of Maximo and TRIRIGA with the correct End Points of the recently created flow.","title":"Step 3 - Configure Maximo and TRIRIGA instances with App Connect urls"},{"location":"mas-tas/#maximo","text":"From the main page of Maximo, click the menu icon on the top left and navigate to Integration -> End Points . Fill in the properties with the url, username, and password from Step 2. Click the Test button at the bottom right of the screen and send a simple {\"hello\":\"world\"}. With the proper configuration, there will be an expected error that ends with Bad Request . If there is a different error than Bad Request in the Response window, refer to the Troubleshooting section to debug. If the instance is based in the cloud, there is a slight difference in authorization. Since the authorization is with an api key, remove the basic authorization values (USERNAME & PASSWORD) and in the HEADERS field add the API key after Content-Type. The Headers field should look something like this: Content-Type: application/json, X-IBM-Client-Id: [apikey from step 2] Step 1: Maximo End Point app On-prem End Point properties Cloud End Point properties","title":"Maximo"},{"location":"mas-tas/#tririga","text":"From the main page of TRIRIGA, click on Tools -> System Setup -> Integration -> Integration Object . Under the Name column, type in apic , and select the integration object that pertains to the record that is getting sent. Click on the object and fill in the credentials in the pop-up box. TRIRIGA End Point","title":"TRIRIGA"},{"location":"mas-tas/#step-4-test-the-flow","text":"With these 3 steps completed, test the flow with a payload. Head back to the Try It page in the deployed flow (or the documentation page if cloud-based) and scroll down to the bottom of the page under Parameters. In here, fill in mxUrl and triUrl with the urls for the respective applications, generate a test payload to send through the flow and monitor if the information populates in the desired application. If there is a response other than 200 from the Test, refer to Troubleshooting. App Connect Test","title":"Step 4 - Test the Flow"},{"location":"mas-tas/#references","text":"Mapping Document","title":"References"},{"location":"mas-tas/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"mas-tas/#common-errors-that-arise-from-app-connect","text":"#### Testing the whole flow If there is a 404 Not Found error when trying to test the flow, this could mean that the flow is not running. Double check to make sure the flow shows a green dot and says Running after edits have been made. If there is a 400 Bad Request error when testing the flow, this could mean the wrong account configurations. Double check to make sure the Accounts from the App Connect pre-requisite section are correct.","title":"Common errors that arise from App Connect"},{"location":"mas-tas/#common-errors-that-arise-from-maximo","text":"#### Testing the End Point If an error reads \"The response code received from the HTTP request from the endpoint is not successful.\", this is related to the configured End Point. Double check and make sure the values in End Points are correct. Make sure there are no accidental spaces at the beginning or end in the event of the values being copy/pasted. If an error comes back as a PKSync error , this is related to the certificates in WebSphere. Double check and confirm that the certificates from Step 4 are correctly configured.","title":"Common errors that arise from Maximo"},{"location":"mas-tas/#common-errors-that-arise-from-tririga","text":"Clear OSLC Cache in TRIRIGA Admin Console in case the integrations do not work in intended manner.","title":"Common errors that arise from TRIRIGA"},{"location":"max-envizi-index/","text":"IBM MAS Connector for Envizi The IBM MAS Connector for Envizi is released under the name \"MAS Connector for Envizi\". This connector supports the following capabilities through an App Connect flow: -Automatically synchronize Asset location and meter readings from MAS to Envizi to automate tracking of energy usage and calculating its related scope 1 and 2 emissions of Electricity, Natural Gas, and Water. Use Case Examples Corporate sustainability managers can gather and maintain accurate sustainability data for their portfolio of assets directly from Maximo where those assets are managed. Location data from Maximo serves as a baseline for all other Sustainability reports in Envizi; continuously updated meter readings data captured in Maximo (Electricity, Gas, Water) enables accurate GHG accounting and performance reporting in Envizi. Connector Architecture IBM App Connect provides a flexible environment for integration solutions to transform, enrich, route, and process business messages and data. App Connect Flows enable specific integration use cases by connecting to predefined APIs to route and map data. Mapping has been pre-defined, but it can be customized. Native API framework is used for Maximo and enabled thorugh provided packages that can be imported. Maximo to Envizi Integration Diagram Data Mapping The image below illustrates the type of data that is being sent by the API and App Connect Flows. Maximo to Envizi Data Map App Connect Flows Included with this connector is a flow that exports locations and meter readings, along with all the required fields they contain. The table below shows the naming convention for this flow and the current integration use case. File Flow Destination Operation PLUSZMXTOS3_v1_1_0.yaml Locations and Meter Reading Max to Envizi Bulk Load or Load Changes Installation & Configuration Guide Before you begin you will need: An instance of App Connect Enterprise or App Connect Pro with the Designer component. Admin access to your Maximo instance with an api key generated for this integration. Envizi instance with a AWS S3 Bucket Import AppConnect Cert to Maximo to enable encrypted communication Check that meter data uses supported units of measure Downloadable Resources Download the zip file that has all of the flows and configuration files. Installation Steps Overview Install Connector Configure App Connect a. Import Flows into App Connect b. Configure Flows Configure Maximo a. Upload Configuration File b. Categorize existing meters into groups c. Set up End Points d. Configure Cron Task Test a. MAS outbound connectivity Part 1. Install Connector MAS 8.9 The MAS Connector to Envizi in Manage can be installed by using the proper customization .zip file provided from Passport Advantage. This .zip file will have all of the configurations necessary to complete step 4 in the Installation process. To apply this to the instance of Manage: Head to the admin dashboard and click on Catalog and then Manage . Click on View deployment details and then Go to workspace details page at the bottom of the Manage status page. At the top right corner of the page under Actions , select Update configuration . From the right side of the pop up under Configurations , select Customization . Un-select the System managed button and add the File address for the file. Be sure to leave the Alias as the default value or else this process will not work properly. Once added, scroll to the top and click Apply Changes and Confirm on the next window. Step 1: Manage dashboard Step 2: Navigate to Configuration page Step 3: Add Customization to Manage instance The process can take anywhere between 20 minutes to 1-2 hours depending on the cluster resources. Monitor the logs in the created build pods in the Manage namespace in the OpenShift cluster to see the progress. MAS 8.10 The MAS Connector to Envizi in Manage can be installed on the MAS administration dashboard through the tile. Search for Envizi from the Catalog page and select the Envizi Connector tile. On the tile page, click Configure and it will take you to the update configuration page. Select the latest version of the connector under the Components section and then click Apply Changes at the top of the page and Confirm on the next window. Step 1: Select Connector Tile Step 2: Apply Changes for Connector The process can take anywhere between 20 minutes to 1-2 hours depending on the cluster resources. Monitor the logs in the created build pods in the Manage namespace in the OpenShift cluster to see the progress. Part 2: App Connect Configuration Note: IBM App Connect Professional or Enterprise is needed to run this flow. The flows have been tested on IBM Cloud App Connect, AWS App Connect, as well as the containerized version of App Connect. Note: The names in the screenshots are generic, other instances will not have the same names during setup. Adding Accounts Before importing the flow to App Connect, add Accounts for S3 and HTTP connectors. Navigate to Catalog section of the AppConnect instance In the Search application , type name of the connector to add the account for If the AppConnect instance does not have an account for the connector, click on Connect to create a new account. Else, open the account selection drop down, and click on Add a new account ... Enter the necessary details for the connector a. For S3, it will be the S3 server and user account. b. For HTTP, it will be the Authentication Key or username and password needed for Maximo. Click on Connect From the account selection drop down, select the newly created account. e.g., The default name will be Account 2 if you already have Account 1 Click on the kebab menu (three dots) and select Rename Account Enter an account name and click on Rename Account . This name can now be used by the connector in the flow. Importing the flow Open the Dashboard of the AppConnect instance Click on the New button and select Import Flow Browse to the flow's YAML file and click on Import The flow will now be imported and opened. Configuring the flow to use the right accounts When importing a flow, it is important to check if the flow is using the right accounts for the different connectors. Click on Edit Flow See if the connectors are using the right accounts. To change the account for any connector, select the connector and click on the dropdown icon next to the Account's name Select the account name that you want to use from the list Authentication On-prem based App Connect If the instance of App Connect is on-prem, basic authentication will be used. Click on Test . Click on POST from the left hand side and then the Try It tab to get the username and password for the flow. Use this authentication in the End Point section of the Maximo Configuration. Cloud based App Connect If the instance of App Connect is on cloud Click on Manage . Scroll to the bottom of the page. Click on Create API key and documentation link Enter name for the API key and click on Create Make a note of this key. Maximo End Point will be configured to send this API key as value for the X-IBM-Client-Id header while calling the AppConnect flow API. Open the API Documentation Link in another tab. Click on the Route to see the URL Copy the link show in details tab next to the HTTP Method It is helpful to use a tool like Postman to test this API prior to testing it for the first time from Maximo. Part 3: Maximo Configuration Deploying Maximo Build Within Maximo, the integration provides integration components (via .dbc script files) and Java Classes as part of the solution. These components need to be installed in the customer's Maximo environment. Components and other content created for this integration solution will be identified by names that begin with PLUSZ. Maximo 7.6.1.2+ On the Maximo Admin workstation, overlay the Maximo SMP directory ( /opt/IBM/SMP/maximo ) with the contents from the solution zip file that is provided. This will lay down the Java Classes and .dbc files provided with the solution. Shutdown the MXServer Run UpdateDB command to install the solution components a. Navigate to /opt/IBM/SMP/maximo/tools/maximo b. Run ./updatedb.sh Build the Maximo EAR file a. Navigate to /opt/IBM/SMP/maximo/deployment b. Run ./buildmaximoear.sh Deploy the new Maximo EAR File on all Maximo servers a. In WebSphere console, navigate to Applications->Application types->Websphere enterprise applications b. Select MAXIMO and then hit Update button c. Select Browse and select the EAR file from Step 4 d. Hit Next and accept defaults from all pages e. After deployment, click on Save Start the applications and MXServer MAS 8.8+ Navigate to the Admin dashboard of the instance of Manage within MAS. Select the workspace the instance is deployed on and update the configuration. Scroll down to Customization and link to the location of the .zip file in the field ( Additional details on setting customizations) Click Apply Changes and Manage will update the instance with the customization. Configuring Artifacts Meter Groups Make sure all the Meters that need to sync with Envizi are in the right meter groups: Meter Group Meters PLUSZ_ELEC Electric Meters PLUSZ_GAS Natural Gas Meters PLUSZ_WTR Water Meters End Points In the Maximo menu, click on Integration -> End Points End Point Navigation Under the End Point column, type PLUSZ and hit Enter to search. End Point Search Click on the PLUSZEXPORT End Point and configure the parameters required to execute the App Connect flow. Click on Save End Point on the left side once finished. Field Value URL Full URL to the flow's API as show in App Connect API Documentation Link HEADERS Replace the YourAPIKeyHere with the App Connect flow's API Key. Do not remove ,Content-Type:application/json HTTPMETHOD Do not change the default value POST End Point Configuration Page Cron Tasks The integration is equipped to export Locations and Location Meter Readings out of Maximo. As part of the PLUSZEXPORT cron task, there are four individual cron tasks that are configurable- two for Locations and two for Meter Readings. These are the Always-on Cron Task and the On-Demand Cron Task. Always-on Cron Task This is for pulling the live or recent data from Locations and Meter Readings, and is supposed to be run frequently. This will pull the data that has come into the system after the last time it ran. Based on its running frequency, the system will automatically decide the window of dates/timestamps to pull the data from. These Cron Task Instances will use the suffix _ALWAYS_ON On-demand Cron Task Even though this is a Cron Task, this is supposed to run just once, on-demand, when the user wants to pull data between fixed dates/timestamps. Once the first cron event executes, it should be stopped in order to prevent the same data getting pulled again and again. This is a stop-gap for now as we dont have another way from Maximo to call a preconfigured API on-demand. This might be needed during initial setup of the system, as the Always-on Cron will start pulling data that has arrived after it has been started. This can also be used anytime in the future if there is any need to pull any historical data on-demand. The start date ( OVERRIDESTARTDATE ) and end date ( OVERRIDEENDDATE ) have to be configured before the cron is started. These Cron Task Instances will use the suffix ON_DEMAND Basic Cron Task Parameters MXURL : The base URL of the Maximo instance. This would include the protocol, domain name and the port number. Do not put a trailing / at the end. In the absense of port number, the default port for the protocol will be used (80 for http, 443 for https) . e.g. http://example.maximo.com:9080 For MAS deployments, this route should be to the Manage instance domain. You can find this route in OpenShift under Networking -> Routes within the Manage namespace. The route should have the format \"workspace\".manage.\"instanceid\" CUSTOMER : Name of the customer. This will be used to name the CSV files. To be supplied by Envizi. OVERRIDESTARTDATE : (Only for On-demand Cron) The start timestamp of the window between which the data will be pulled from. The date must be specified in ISO 8601 format. e.g. 2022-06-26T23:09:30-07:00 where -07:00 represents timezone offset of UTC-07:00 hours OVERRIDEENDDATE : (Only for On-demand Cron) The end timestamp of the window between which the data will be pulled from. The date must be specified in ISO 8601 format. e.g. 2022-06-26T23:09:30-07:00 where -07:00 represents timezone offset of UTC-07:00 hours Advanced Parameters These will be set to the right values by default. DO NOT change these parameters without consulting with Envizi. SELECT : Names of the fields from the Object Structure FROM : Name of the Object Structure WHERE : (Optional). OSLC Condition for data selection ORDERBY : (Optional). Sequence by which the data should be pulled TARGETDATECOLUMN : Name of the DateTime Column in the Object structure that will be used to filter records between a date-time window. SAVEDQUERY ENDPOINTNAME - Name of the Maximo End Point configured to connect to the AppConnect flow. PAGESIZE - Maximum number of records pulled from Maximo in one API call. This will also be the maximum number of records in the CSV file. Configuring Cron Task In the Maximo menu, click on System Configuration > Platform Configuration > Cron Task Setup Under the Cron Task column, type PLUSZ and hit enter to search Click on the PLUSZEXPORT Cron Task In the Cron Task Instances, click on the Schedule/Calendar icon to set the schedule for the Cron Task Instance Once it is configured, click on OK Select the Cron Task Instance to configure Parameters Scroll down to the Cron Task Parameters section and configure the Value column as needed. Ideally, only the CUSTOMER and MXURL parameters will need to be edited for all instances. For the On-demand Crons, the OVERRIDESTARTDATE and OVERRIDEENDDATE parameters will need to be edited. Click on Save Operating the connector Start/Stop the Cron Task Instance Navigate inside the desired Cron Task. Enable/Disable the checkbox in the column Active? in front of the desired Cron Task Instance Click on Save Starting and Stopping the flow If using AppConnect Dashboard, Click on the kebab menu (three dots) on the flow's tile. If inside the flow, Click on the kebab menu (three dots) on top right of the screen. Click on Start API or Stop API depending on what action you want to perform. Part 3: Testing To test that the configuration is complete, send a test payload in order to test connectivity. Go to the End Points application and click Test at the bottom of the PLUSZExport end point. Send a test payload that is a valid object. {\"Hello\":\"World\"} would work. If the response is anything other than Bad Request , see what might be causing the error Troubleshooting When testing that the end point is entered correctly on the End Points application, there are a few common errors: Error Cause Resolution Response code received from the HTTP request from the endpoint is not successful Invalid URL in the Integration Object Double check the URL that all of the components are entered correctly. Make sure there are no accidental spaces at the beginning or the end in event of a copy/paste. 404: Not Found Flow is not Running Make sure the flow is Running before starting the cron tasks PKSync error Certificate error Confirm the certificate is configured correctly Unable to Verify Leaf Signature Certificate Error Confirm the HTTP account is set with \"Allow Self-Signed Certificates\" as True. Status Code 400: BMXAA8744E - The OSLC query was not parsed. Incorrect character in cron task parameter field, such as a decimal point in Page Size Check the parameters in the cron task. \u201cCannot GET /maximo/api/os/PLUSZLOCATIONS\u201d MXUrl is not valid Verify the MXURL parameter in Cron task that it is pointing to the correct url. Reference for Pre-requisite Pre-requisite: Pre-check that Data is correct Data for Location Meter will not be exported to Envizi if the Unit of Measurement is not configured. The current integration with Envizi only supports the following Units of Measurement for the Location Meter data: Meter Type Supported Unit of Measure Electric GJ, kWh, MWh Natural Gas GJ, MJ, kWh, m3, mmbtu, therms Water litres, kliter, m3, gallons If these Units of Measure are not present on the instance of Maximo, they can be added in the Item Master application under Inventory on the drop down menu. Select Unit of Measure and Conversion -> Add/Modify Units of Measure and add the missing units. A Service Address must be configured in Location with the following fields for proper functioning of Envizi's features: Street Address City State Post Code Country Latitude Longitude If the Location does not have a Service Address, navigate to Address Information under the Location and either select or create an associated Service Address. If one needs to be created, navigate to Administration -> Service Address and click New Service Address and enter the required fields. Then return to the desired location and associate the Service Address in Address Information . Pre-requisite: add an App Connect Certificate in MAS 8.9+ Extract the App Connect certificate from an imported flow URL. Navigate to the flow's page and click on Test and then Try It to get the proper URL. Navigate to the Admin dashboard for MAS and go to the workspace where Manage is deployed. Update the configuration and scroll down to Imported Certificates . Untick the System managed button and fill in the extracted certificate in the fields. Click Apply Changes at the top of the page and MAS will update the truststore with the new certificate. Step 2: Manage Workspace Step 3: Imported Certificates Pre-requisite: add an App Connect Certificate in Maximo 7.6.1.2+: Configure WebSphere Certificates. This makes a test connection to a Secure Sockets Layer (SSL) port and retrieves the signer from the server during the handshake. Log into the WebSphere console that is hosting the Maximo server. Click on Security -> SSL certificate & key management . Under Related Items click on Key stores and certificates . Click on CellDefaultTrustStore and on the next page under Additional Properties click on Signer certificates . From this page, click on the button that says Retrieve from Port and fill in the required fields using the table below: Field Value Host The host from the imported flow URL Port 443 Alias appconnect Once all three have been entered in, click Retrieve signer information and the information from the URL will populate on screen. Click Save in the box at the top and then repeat the process for NodeDefaultTrustStore . Step 2: WebSphere Homepage Step 3: Websphere Keystores Step 4: Websphere Signer Certs","title":"IBM MAS Connector for Envizi"},{"location":"max-envizi-index/#ibm-mas-connector-for-envizi","text":"The IBM MAS Connector for Envizi is released under the name \"MAS Connector for Envizi\". This connector supports the following capabilities through an App Connect flow: -Automatically synchronize Asset location and meter readings from MAS to Envizi to automate tracking of energy usage and calculating its related scope 1 and 2 emissions of Electricity, Natural Gas, and Water.","title":"IBM MAS Connector for Envizi"},{"location":"max-envizi-index/#use-case-examples","text":"Corporate sustainability managers can gather and maintain accurate sustainability data for their portfolio of assets directly from Maximo where those assets are managed. Location data from Maximo serves as a baseline for all other Sustainability reports in Envizi; continuously updated meter readings data captured in Maximo (Electricity, Gas, Water) enables accurate GHG accounting and performance reporting in Envizi.","title":"Use Case Examples"},{"location":"max-envizi-index/#connector-architecture","text":"IBM App Connect provides a flexible environment for integration solutions to transform, enrich, route, and process business messages and data. App Connect Flows enable specific integration use cases by connecting to predefined APIs to route and map data. Mapping has been pre-defined, but it can be customized. Native API framework is used for Maximo and enabled thorugh provided packages that can be imported. Maximo to Envizi Integration Diagram","title":"Connector Architecture"},{"location":"max-envizi-index/#data-mapping","text":"The image below illustrates the type of data that is being sent by the API and App Connect Flows. Maximo to Envizi Data Map","title":"Data Mapping"},{"location":"max-envizi-index/#app-connect-flows","text":"Included with this connector is a flow that exports locations and meter readings, along with all the required fields they contain. The table below shows the naming convention for this flow and the current integration use case. File Flow Destination Operation PLUSZMXTOS3_v1_1_0.yaml Locations and Meter Reading Max to Envizi Bulk Load or Load Changes","title":"App Connect Flows"},{"location":"max-envizi-index/#installation-configuration-guide","text":"","title":"Installation &amp; Configuration Guide"},{"location":"max-envizi-index/#before-you-begin-you-will-need","text":"An instance of App Connect Enterprise or App Connect Pro with the Designer component. Admin access to your Maximo instance with an api key generated for this integration. Envizi instance with a AWS S3 Bucket Import AppConnect Cert to Maximo to enable encrypted communication Check that meter data uses supported units of measure","title":"Before you begin you will need:"},{"location":"max-envizi-index/#downloadable-resources","text":"Download the zip file that has all of the flows and configuration files.","title":"Downloadable Resources"},{"location":"max-envizi-index/#installation-steps-overview","text":"Install Connector Configure App Connect a. Import Flows into App Connect b. Configure Flows Configure Maximo a. Upload Configuration File b. Categorize existing meters into groups c. Set up End Points d. Configure Cron Task Test a. MAS outbound connectivity","title":"Installation Steps Overview"},{"location":"max-envizi-index/#part-1-install-connector","text":"","title":"Part 1. Install Connector"},{"location":"max-envizi-index/#mas-89","text":"The MAS Connector to Envizi in Manage can be installed by using the proper customization .zip file provided from Passport Advantage. This .zip file will have all of the configurations necessary to complete step 4 in the Installation process. To apply this to the instance of Manage: Head to the admin dashboard and click on Catalog and then Manage . Click on View deployment details and then Go to workspace details page at the bottom of the Manage status page. At the top right corner of the page under Actions , select Update configuration . From the right side of the pop up under Configurations , select Customization . Un-select the System managed button and add the File address for the file. Be sure to leave the Alias as the default value or else this process will not work properly. Once added, scroll to the top and click Apply Changes and Confirm on the next window. Step 1: Manage dashboard Step 2: Navigate to Configuration page Step 3: Add Customization to Manage instance The process can take anywhere between 20 minutes to 1-2 hours depending on the cluster resources. Monitor the logs in the created build pods in the Manage namespace in the OpenShift cluster to see the progress.","title":"MAS 8.9"},{"location":"max-envizi-index/#mas-810","text":"The MAS Connector to Envizi in Manage can be installed on the MAS administration dashboard through the tile. Search for Envizi from the Catalog page and select the Envizi Connector tile. On the tile page, click Configure and it will take you to the update configuration page. Select the latest version of the connector under the Components section and then click Apply Changes at the top of the page and Confirm on the next window. Step 1: Select Connector Tile Step 2: Apply Changes for Connector The process can take anywhere between 20 minutes to 1-2 hours depending on the cluster resources. Monitor the logs in the created build pods in the Manage namespace in the OpenShift cluster to see the progress.","title":"MAS 8.10"},{"location":"max-envizi-index/#part-2-app-connect-configuration","text":"Note: IBM App Connect Professional or Enterprise is needed to run this flow. The flows have been tested on IBM Cloud App Connect, AWS App Connect, as well as the containerized version of App Connect. Note: The names in the screenshots are generic, other instances will not have the same names during setup.","title":"Part 2: App Connect Configuration"},{"location":"max-envizi-index/#adding-accounts","text":"Before importing the flow to App Connect, add Accounts for S3 and HTTP connectors. Navigate to Catalog section of the AppConnect instance In the Search application , type name of the connector to add the account for If the AppConnect instance does not have an account for the connector, click on Connect to create a new account. Else, open the account selection drop down, and click on Add a new account ... Enter the necessary details for the connector a. For S3, it will be the S3 server and user account. b. For HTTP, it will be the Authentication Key or username and password needed for Maximo. Click on Connect From the account selection drop down, select the newly created account. e.g., The default name will be Account 2 if you already have Account 1 Click on the kebab menu (three dots) and select Rename Account Enter an account name and click on Rename Account . This name can now be used by the connector in the flow.","title":"Adding Accounts"},{"location":"max-envizi-index/#importing-the-flow","text":"Open the Dashboard of the AppConnect instance Click on the New button and select Import Flow Browse to the flow's YAML file and click on Import The flow will now be imported and opened.","title":"Importing the flow"},{"location":"max-envizi-index/#configuring-the-flow-to-use-the-right-accounts","text":"When importing a flow, it is important to check if the flow is using the right accounts for the different connectors. Click on Edit Flow See if the connectors are using the right accounts. To change the account for any connector, select the connector and click on the dropdown icon next to the Account's name Select the account name that you want to use from the list","title":"Configuring the flow to use the right accounts"},{"location":"max-envizi-index/#authentication","text":"","title":"Authentication"},{"location":"max-envizi-index/#on-prem-based-app-connect","text":"If the instance of App Connect is on-prem, basic authentication will be used. Click on Test . Click on POST from the left hand side and then the Try It tab to get the username and password for the flow. Use this authentication in the End Point section of the Maximo Configuration.","title":"On-prem based App Connect"},{"location":"max-envizi-index/#cloud-based-app-connect","text":"If the instance of App Connect is on cloud Click on Manage . Scroll to the bottom of the page. Click on Create API key and documentation link Enter name for the API key and click on Create Make a note of this key. Maximo End Point will be configured to send this API key as value for the X-IBM-Client-Id header while calling the AppConnect flow API. Open the API Documentation Link in another tab. Click on the Route to see the URL Copy the link show in details tab next to the HTTP Method It is helpful to use a tool like Postman to test this API prior to testing it for the first time from Maximo.","title":"Cloud based App Connect"},{"location":"max-envizi-index/#part-3-maximo-configuration","text":"","title":"Part 3: Maximo Configuration"},{"location":"max-envizi-index/#deploying-maximo-build","text":"Within Maximo, the integration provides integration components (via .dbc script files) and Java Classes as part of the solution. These components need to be installed in the customer's Maximo environment. Components and other content created for this integration solution will be identified by names that begin with PLUSZ.","title":"Deploying Maximo Build"},{"location":"max-envizi-index/#maximo-7612","text":"On the Maximo Admin workstation, overlay the Maximo SMP directory ( /opt/IBM/SMP/maximo ) with the contents from the solution zip file that is provided. This will lay down the Java Classes and .dbc files provided with the solution. Shutdown the MXServer Run UpdateDB command to install the solution components a. Navigate to /opt/IBM/SMP/maximo/tools/maximo b. Run ./updatedb.sh Build the Maximo EAR file a. Navigate to /opt/IBM/SMP/maximo/deployment b. Run ./buildmaximoear.sh Deploy the new Maximo EAR File on all Maximo servers a. In WebSphere console, navigate to Applications->Application types->Websphere enterprise applications b. Select MAXIMO and then hit Update button c. Select Browse and select the EAR file from Step 4 d. Hit Next and accept defaults from all pages e. After deployment, click on Save Start the applications and MXServer","title":"Maximo 7.6.1.2+"},{"location":"max-envizi-index/#mas-88","text":"Navigate to the Admin dashboard of the instance of Manage within MAS. Select the workspace the instance is deployed on and update the configuration. Scroll down to Customization and link to the location of the .zip file in the field ( Additional details on setting customizations) Click Apply Changes and Manage will update the instance with the customization.","title":"MAS 8.8+"},{"location":"max-envizi-index/#configuring-artifacts","text":"","title":"Configuring Artifacts"},{"location":"max-envizi-index/#meter-groups","text":"Make sure all the Meters that need to sync with Envizi are in the right meter groups: Meter Group Meters PLUSZ_ELEC Electric Meters PLUSZ_GAS Natural Gas Meters PLUSZ_WTR Water Meters","title":"Meter Groups"},{"location":"max-envizi-index/#end-points","text":"In the Maximo menu, click on Integration -> End Points End Point Navigation Under the End Point column, type PLUSZ and hit Enter to search. End Point Search Click on the PLUSZEXPORT End Point and configure the parameters required to execute the App Connect flow. Click on Save End Point on the left side once finished. Field Value URL Full URL to the flow's API as show in App Connect API Documentation Link HEADERS Replace the YourAPIKeyHere with the App Connect flow's API Key. Do not remove ,Content-Type:application/json HTTPMETHOD Do not change the default value POST End Point Configuration Page","title":"End Points"},{"location":"max-envizi-index/#cron-tasks","text":"The integration is equipped to export Locations and Location Meter Readings out of Maximo. As part of the PLUSZEXPORT cron task, there are four individual cron tasks that are configurable- two for Locations and two for Meter Readings. These are the Always-on Cron Task and the On-Demand Cron Task.","title":"Cron Tasks"},{"location":"max-envizi-index/#always-on-cron-task","text":"This is for pulling the live or recent data from Locations and Meter Readings, and is supposed to be run frequently. This will pull the data that has come into the system after the last time it ran. Based on its running frequency, the system will automatically decide the window of dates/timestamps to pull the data from. These Cron Task Instances will use the suffix _ALWAYS_ON","title":"Always-on Cron Task"},{"location":"max-envizi-index/#on-demand-cron-task","text":"Even though this is a Cron Task, this is supposed to run just once, on-demand, when the user wants to pull data between fixed dates/timestamps. Once the first cron event executes, it should be stopped in order to prevent the same data getting pulled again and again. This is a stop-gap for now as we dont have another way from Maximo to call a preconfigured API on-demand. This might be needed during initial setup of the system, as the Always-on Cron will start pulling data that has arrived after it has been started. This can also be used anytime in the future if there is any need to pull any historical data on-demand. The start date ( OVERRIDESTARTDATE ) and end date ( OVERRIDEENDDATE ) have to be configured before the cron is started. These Cron Task Instances will use the suffix ON_DEMAND","title":"On-demand Cron Task"},{"location":"max-envizi-index/#basic-cron-task-parameters","text":"MXURL : The base URL of the Maximo instance. This would include the protocol, domain name and the port number. Do not put a trailing / at the end. In the absense of port number, the default port for the protocol will be used (80 for http, 443 for https) . e.g. http://example.maximo.com:9080 For MAS deployments, this route should be to the Manage instance domain. You can find this route in OpenShift under Networking -> Routes within the Manage namespace. The route should have the format \"workspace\".manage.\"instanceid\" CUSTOMER : Name of the customer. This will be used to name the CSV files. To be supplied by Envizi. OVERRIDESTARTDATE : (Only for On-demand Cron) The start timestamp of the window between which the data will be pulled from. The date must be specified in ISO 8601 format. e.g. 2022-06-26T23:09:30-07:00 where -07:00 represents timezone offset of UTC-07:00 hours OVERRIDEENDDATE : (Only for On-demand Cron) The end timestamp of the window between which the data will be pulled from. The date must be specified in ISO 8601 format. e.g. 2022-06-26T23:09:30-07:00 where -07:00 represents timezone offset of UTC-07:00 hours","title":"Basic Cron Task Parameters"},{"location":"max-envizi-index/#advanced-parameters","text":"These will be set to the right values by default. DO NOT change these parameters without consulting with Envizi. SELECT : Names of the fields from the Object Structure FROM : Name of the Object Structure WHERE : (Optional). OSLC Condition for data selection ORDERBY : (Optional). Sequence by which the data should be pulled TARGETDATECOLUMN : Name of the DateTime Column in the Object structure that will be used to filter records between a date-time window. SAVEDQUERY ENDPOINTNAME - Name of the Maximo End Point configured to connect to the AppConnect flow. PAGESIZE - Maximum number of records pulled from Maximo in one API call. This will also be the maximum number of records in the CSV file.","title":"Advanced Parameters"},{"location":"max-envizi-index/#configuring-cron-task","text":"In the Maximo menu, click on System Configuration > Platform Configuration > Cron Task Setup Under the Cron Task column, type PLUSZ and hit enter to search Click on the PLUSZEXPORT Cron Task In the Cron Task Instances, click on the Schedule/Calendar icon to set the schedule for the Cron Task Instance Once it is configured, click on OK Select the Cron Task Instance to configure Parameters Scroll down to the Cron Task Parameters section and configure the Value column as needed. Ideally, only the CUSTOMER and MXURL parameters will need to be edited for all instances. For the On-demand Crons, the OVERRIDESTARTDATE and OVERRIDEENDDATE parameters will need to be edited. Click on Save","title":"Configuring Cron Task"},{"location":"max-envizi-index/#operating-the-connector","text":"","title":"Operating the connector"},{"location":"max-envizi-index/#startstop-the-cron-task-instance","text":"Navigate inside the desired Cron Task. Enable/Disable the checkbox in the column Active? in front of the desired Cron Task Instance Click on Save","title":"Start/Stop the Cron Task Instance"},{"location":"max-envizi-index/#starting-and-stopping-the-flow","text":"If using AppConnect Dashboard, Click on the kebab menu (three dots) on the flow's tile. If inside the flow, Click on the kebab menu (three dots) on top right of the screen. Click on Start API or Stop API depending on what action you want to perform.","title":"Starting and Stopping the flow"},{"location":"max-envizi-index/#part-3-testing","text":"To test that the configuration is complete, send a test payload in order to test connectivity. Go to the End Points application and click Test at the bottom of the PLUSZExport end point. Send a test payload that is a valid object. {\"Hello\":\"World\"} would work. If the response is anything other than Bad Request , see what might be causing the error","title":"Part 3: Testing"},{"location":"max-envizi-index/#troubleshooting","text":"When testing that the end point is entered correctly on the End Points application, there are a few common errors: Error Cause Resolution Response code received from the HTTP request from the endpoint is not successful Invalid URL in the Integration Object Double check the URL that all of the components are entered correctly. Make sure there are no accidental spaces at the beginning or the end in event of a copy/paste. 404: Not Found Flow is not Running Make sure the flow is Running before starting the cron tasks PKSync error Certificate error Confirm the certificate is configured correctly Unable to Verify Leaf Signature Certificate Error Confirm the HTTP account is set with \"Allow Self-Signed Certificates\" as True. Status Code 400: BMXAA8744E - The OSLC query was not parsed. Incorrect character in cron task parameter field, such as a decimal point in Page Size Check the parameters in the cron task. \u201cCannot GET /maximo/api/os/PLUSZLOCATIONS\u201d MXUrl is not valid Verify the MXURL parameter in Cron task that it is pointing to the correct url.","title":"Troubleshooting"},{"location":"max-envizi-index/#reference-for-pre-requisite","text":"","title":"Reference for Pre-requisite"},{"location":"max-envizi-index/#pre-requisite-pre-check-that-data-is-correct","text":"Data for Location Meter will not be exported to Envizi if the Unit of Measurement is not configured. The current integration with Envizi only supports the following Units of Measurement for the Location Meter data: Meter Type Supported Unit of Measure Electric GJ, kWh, MWh Natural Gas GJ, MJ, kWh, m3, mmbtu, therms Water litres, kliter, m3, gallons If these Units of Measure are not present on the instance of Maximo, they can be added in the Item Master application under Inventory on the drop down menu. Select Unit of Measure and Conversion -> Add/Modify Units of Measure and add the missing units. A Service Address must be configured in Location with the following fields for proper functioning of Envizi's features: Street Address City State Post Code Country Latitude Longitude If the Location does not have a Service Address, navigate to Address Information under the Location and either select or create an associated Service Address. If one needs to be created, navigate to Administration -> Service Address and click New Service Address and enter the required fields. Then return to the desired location and associate the Service Address in Address Information .","title":"Pre-requisite: Pre-check that Data is correct"},{"location":"max-envizi-index/#pre-requisite-add-an-app-connect-certificate-in-mas-89","text":"Extract the App Connect certificate from an imported flow URL. Navigate to the flow's page and click on Test and then Try It to get the proper URL. Navigate to the Admin dashboard for MAS and go to the workspace where Manage is deployed. Update the configuration and scroll down to Imported Certificates . Untick the System managed button and fill in the extracted certificate in the fields. Click Apply Changes at the top of the page and MAS will update the truststore with the new certificate. Step 2: Manage Workspace Step 3: Imported Certificates","title":"Pre-requisite: add an App Connect Certificate in MAS 8.9+"},{"location":"max-envizi-index/#pre-requisite-add-an-app-connect-certificate-in-maximo-7612","text":"Configure WebSphere Certificates. This makes a test connection to a Secure Sockets Layer (SSL) port and retrieves the signer from the server during the handshake. Log into the WebSphere console that is hosting the Maximo server. Click on Security -> SSL certificate & key management . Under Related Items click on Key stores and certificates . Click on CellDefaultTrustStore and on the next page under Additional Properties click on Signer certificates . From this page, click on the button that says Retrieve from Port and fill in the required fields using the table below: Field Value Host The host from the imported flow URL Port 443 Alias appconnect Once all three have been entered in, click Retrieve signer information and the information from the URL will populate on screen. Click Save in the box at the top and then repeat the process for NodeDefaultTrustStore . Step 2: WebSphere Homepage Step 3: Websphere Keystores Step 4: Websphere Signer Certs","title":"Pre-requisite: add an App Connect Certificate in Maximo 7.6.1.2+:"},{"location":"maximo-envizi/","text":"Maximo - Envizi integration Using the flows Flow included in this integration: PLUSZMXTOSFTP This integration uses the built in Maximo cron tasks. Cron Tasks For every Object Structure that gets pulled out of Maximo and pushed to Envizi, there are two types of Cron jobs: Always-on Cron (Always-Cron) This is for pulling the live or recent data, which is supposed to run frequently. This will pull the data that has come into the system after the last time it ran. Based on its running frequency, the system will automatically decide the window of dates/timestamps to pull the data from. These Cron Task Instances will use the suffix _ALWAYS_ON On-demand Cron (Cron-demand) Even though this is a Cron, this is supposed to run just once, on-demand, when the customer or the user wants to pull data between fixed dates/timestamps. Once the first cron event executes, it should be stopped in order to prevent the same data getting pulled again and again. This is a stop-gap for now as we dont have another way from Maximo to call a preconfigured API on-demand. This might be needed during initial setup of the system, as the Always-on Cron will start pulling data that has arrived after it has been started. This can also be used anytime in the future if there is any need to pull any historical data on-demand. The start date ( OVERRIDESTARTDATE ) and end date ( OVERRIDEENDDATE ) have to be configured before the cron is started. These Cron Task Instances will use the suffix ON_DEMAND Basic Parameters MXURL : The base URL of the Maximo instance. This would include the protocol, domain name and the port number. Do not put a trailing / at the end. In the absense of port number, the default port for the protocol will be used (80 for http, 443 for https) . e.g. http://example.maximo.com:9080 CUSTOMER : Name of the customer. This will be used to name the CSV files. To be supplied by Envizi. OVERRIDESTARTDATE : (Only for On-demand Cron) The start timestamp of the window between which the data will be pulled from. The date must be specified in ISO 8601 format. e.g. 2022-06-26T23:09:30-07:00 where -07:00 represents timezone offset of UTC-07:00 hours OVERRIDEENDDATE : (Only for On-demand Cron) The end timestamp of the window between which the data will be pulled from. The date must be specified in ISO 8601 format. e.g. 2022-06-26T23:09:30-07:00 where -07:00 represents timezone offset of UTC-07:00 hours Advanced Parameters These will be set to the right values by default. DO NOT change these parameters without consulting with Envizi. SELECT : Names of the fields from the Object Structure FROM : Name of the Object Structure WHERE : (Optional). OSLC Condition for data selection ORDERBY : (Optional). Sequence by which the data should be pulled TARGETDATECOLUMN : Name of the DateTime Column in the Object structure that will be used to filter records between a date-time window. SAVEDQUERY ENDPOINTNAME - Name of the Maximo End Point configured to connect to the AppConnect flow. PAGESIZE - Maximum number of records pulled from Maximo in one API call. This will also be the maximum number of records in the CSV file. Configuring Cron Task In the Maximo menu, click on System Configuration > Platform Configuration > Cron Task Setup Under the \"Cron Task\" column, type \"PLUSZ\" and hit enter to search Click on the \"PLUSZEXPORT\" Cron Task In the Cron Task Instances, click on the Schedule/Calendar icon to set the schedule for the Cron Task Instance Once it is configured, click on \"OK\" Select the Cron Task Instance to configure Parameters for Scroll down to the \"Cron Task Parameters\" section and configure the \"Value\" column as needed. Ideally, only the CUSTOMER and MXURL parameters will need to be edited for all instances. For the \"On-demand\" Crons, the OVERRIDESTARTDATE and OVERRIDEENDDATE parameters will need to be edited. Click on Save Start/Stop the Cron Task Instance Navigate inside the Cron Task as mentioned above Enable/Disable the checkbox in the column \"Active?\" in front of the desired Cron Task Instance Click on Save Starting and Stopping the flow If using AppConnect Dashboard, Click on the kebab menu (three dots) on the flow's tile. If inside the flow, Click on the kebab menu (three dots) on top right of the screen. Click on \"Start API\" or \"Stop API\" depending on what action you want to perform. Note: If the flow is not running, AppConnect will give Error 404 on the API call. If Checks for the mandatory fields that are required for the flow to function. If any of the fields are missing, the flow returns Error 400. If 3 Checks the presence of OVERRIDESTARTDATE and OVERRIDEENDDATE in the Request. It also checks if they are valid dates. If all checks are passed, dateQuery with these override dates is generated. If the override date checks fail, it checks if LASTSTARTDATE is present. If it is, dateQuery with it is generated. If all checks fail, the flow returns Error 400 Set variable 5 where : The final oslc.where query created using WHERE from Request and dateQuery from the output of \"If 3\" batchTimestamp : Current Epoc timestamp in milliseconds. This will be included in every filename to indicate that they are from the same batch of exported data. Set variable query : The query to be sent to OSLC API to fetch the required data pagesize : The total number of records that will be fetched in one page. If it is not specified in the request, the flow will use default value of 100 . HTTP Invoke Method Does an HTTP GET to the Maximo's OSLC API to fetch the total number of records that match the query. countonlyparams from \"Set variable\" will be used here. Set variable 2 totalCount : Total Count received in the response of the \"HTTP Invoke Method\" Set variable 3 totalPages : Total number of pages to be fetched from Maximo OSLC API that match the query. This is calculated using the totalCount from \"Set variable 2\" and pagesize from \"Set variable\". For each: page Summary: Fetches data from Maximo OSLC API page by page and writes CSV file to the SFTP server. Input: An array of numbers from 1 to totalPages from \"Set variable 3\" Output: fileWritten - Array of string containing names of files written to the SFTP server. HTTP Invoke method 2 Does an HTTP GET call to Maximo OSLC API to fetch the data that matches queryparams from \"Set variable\" for the current page from \"For each: page\" Set variable 4 filename : File name as per format specified by Envizi team using CUSTOMER and FROM from the Request, batchTimestamp from \"Set variable 5\" and current page from \"For each: page\". eg. DemoCorporation_PLUSZLOCATIONS_1654856600431_1.csv SFTP Create file Writes Response body from \"HTTP Invoke method 2\" to the configured SFTP Server with the filename from \"Set variable 4\" at the preconfigured file path. Response Returns HTTP Status 200 with JSON body containing: - filesWritten : Array of file names written to the SFTP server, obtained from the output of \"For each: page\" - queryparams : query from \"Set variable\" The JSON in response body is not used or stored by Maximo.","title":"Maximo - Envizi integration"},{"location":"maximo-envizi/#maximo-envizi-integration","text":"","title":"Maximo - Envizi integration"},{"location":"maximo-envizi/#using-the-flows","text":"Flow included in this integration: PLUSZMXTOSFTP This integration uses the built in Maximo cron tasks.","title":"Using the flows"},{"location":"maximo-envizi/#cron-tasks","text":"For every Object Structure that gets pulled out of Maximo and pushed to Envizi, there are two types of Cron jobs:","title":"Cron Tasks"},{"location":"maximo-envizi/#always-on-cron-always-cron","text":"This is for pulling the live or recent data, which is supposed to run frequently. This will pull the data that has come into the system after the last time it ran. Based on its running frequency, the system will automatically decide the window of dates/timestamps to pull the data from. These Cron Task Instances will use the suffix _ALWAYS_ON","title":"Always-on Cron (Always-Cron)"},{"location":"maximo-envizi/#on-demand-cron-cron-demand","text":"Even though this is a Cron, this is supposed to run just once, on-demand, when the customer or the user wants to pull data between fixed dates/timestamps. Once the first cron event executes, it should be stopped in order to prevent the same data getting pulled again and again. This is a stop-gap for now as we dont have another way from Maximo to call a preconfigured API on-demand. This might be needed during initial setup of the system, as the Always-on Cron will start pulling data that has arrived after it has been started. This can also be used anytime in the future if there is any need to pull any historical data on-demand. The start date ( OVERRIDESTARTDATE ) and end date ( OVERRIDEENDDATE ) have to be configured before the cron is started. These Cron Task Instances will use the suffix ON_DEMAND","title":"On-demand Cron (Cron-demand)"},{"location":"maximo-envizi/#basic-parameters","text":"MXURL : The base URL of the Maximo instance. This would include the protocol, domain name and the port number. Do not put a trailing / at the end. In the absense of port number, the default port for the protocol will be used (80 for http, 443 for https) . e.g. http://example.maximo.com:9080 CUSTOMER : Name of the customer. This will be used to name the CSV files. To be supplied by Envizi. OVERRIDESTARTDATE : (Only for On-demand Cron) The start timestamp of the window between which the data will be pulled from. The date must be specified in ISO 8601 format. e.g. 2022-06-26T23:09:30-07:00 where -07:00 represents timezone offset of UTC-07:00 hours OVERRIDEENDDATE : (Only for On-demand Cron) The end timestamp of the window between which the data will be pulled from. The date must be specified in ISO 8601 format. e.g. 2022-06-26T23:09:30-07:00 where -07:00 represents timezone offset of UTC-07:00 hours","title":"Basic Parameters"},{"location":"maximo-envizi/#advanced-parameters","text":"These will be set to the right values by default. DO NOT change these parameters without consulting with Envizi. SELECT : Names of the fields from the Object Structure FROM : Name of the Object Structure WHERE : (Optional). OSLC Condition for data selection ORDERBY : (Optional). Sequence by which the data should be pulled TARGETDATECOLUMN : Name of the DateTime Column in the Object structure that will be used to filter records between a date-time window. SAVEDQUERY ENDPOINTNAME - Name of the Maximo End Point configured to connect to the AppConnect flow. PAGESIZE - Maximum number of records pulled from Maximo in one API call. This will also be the maximum number of records in the CSV file.","title":"Advanced Parameters"},{"location":"maximo-envizi/#configuring-cron-task","text":"In the Maximo menu, click on System Configuration > Platform Configuration > Cron Task Setup Under the \"Cron Task\" column, type \"PLUSZ\" and hit enter to search Click on the \"PLUSZEXPORT\" Cron Task In the Cron Task Instances, click on the Schedule/Calendar icon to set the schedule for the Cron Task Instance Once it is configured, click on \"OK\" Select the Cron Task Instance to configure Parameters for Scroll down to the \"Cron Task Parameters\" section and configure the \"Value\" column as needed. Ideally, only the CUSTOMER and MXURL parameters will need to be edited for all instances. For the \"On-demand\" Crons, the OVERRIDESTARTDATE and OVERRIDEENDDATE parameters will need to be edited. Click on Save","title":"Configuring Cron Task"},{"location":"maximo-envizi/#startstop-the-cron-task-instance","text":"Navigate inside the Cron Task as mentioned above Enable/Disable the checkbox in the column \"Active?\" in front of the desired Cron Task Instance Click on Save","title":"Start/Stop the Cron Task Instance"},{"location":"maximo-envizi/#starting-and-stopping-the-flow","text":"If using AppConnect Dashboard, Click on the kebab menu (three dots) on the flow's tile. If inside the flow, Click on the kebab menu (three dots) on top right of the screen. Click on \"Start API\" or \"Stop API\" depending on what action you want to perform. Note: If the flow is not running, AppConnect will give Error 404 on the API call.","title":"Starting and Stopping the flow"},{"location":"maximo-envizi/#if","text":"Checks for the mandatory fields that are required for the flow to function. If any of the fields are missing, the flow returns Error 400.","title":"If"},{"location":"maximo-envizi/#if-3","text":"Checks the presence of OVERRIDESTARTDATE and OVERRIDEENDDATE in the Request. It also checks if they are valid dates. If all checks are passed, dateQuery with these override dates is generated. If the override date checks fail, it checks if LASTSTARTDATE is present. If it is, dateQuery with it is generated. If all checks fail, the flow returns Error 400","title":"If 3"},{"location":"maximo-envizi/#set-variable-5","text":"where : The final oslc.where query created using WHERE from Request and dateQuery from the output of \"If 3\" batchTimestamp : Current Epoc timestamp in milliseconds. This will be included in every filename to indicate that they are from the same batch of exported data.","title":"Set variable 5"},{"location":"maximo-envizi/#set-variable","text":"query : The query to be sent to OSLC API to fetch the required data pagesize : The total number of records that will be fetched in one page. If it is not specified in the request, the flow will use default value of 100 .","title":"Set variable"},{"location":"maximo-envizi/#http-invoke-method","text":"Does an HTTP GET to the Maximo's OSLC API to fetch the total number of records that match the query. countonlyparams from \"Set variable\" will be used here.","title":"HTTP Invoke Method"},{"location":"maximo-envizi/#set-variable-2","text":"totalCount : Total Count received in the response of the \"HTTP Invoke Method\"","title":"Set variable 2"},{"location":"maximo-envizi/#set-variable-3","text":"totalPages : Total number of pages to be fetched from Maximo OSLC API that match the query. This is calculated using the totalCount from \"Set variable 2\" and pagesize from \"Set variable\".","title":"Set variable 3"},{"location":"maximo-envizi/#for-each-page","text":"Summary: Fetches data from Maximo OSLC API page by page and writes CSV file to the SFTP server. Input: An array of numbers from 1 to totalPages from \"Set variable 3\" Output: fileWritten - Array of string containing names of files written to the SFTP server.","title":"For each: page"},{"location":"maximo-envizi/#http-invoke-method-2","text":"Does an HTTP GET call to Maximo OSLC API to fetch the data that matches queryparams from \"Set variable\" for the current page from \"For each: page\"","title":"HTTP Invoke method 2"},{"location":"maximo-envizi/#set-variable-4","text":"filename : File name as per format specified by Envizi team using CUSTOMER and FROM from the Request, batchTimestamp from \"Set variable 5\" and current page from \"For each: page\". eg. DemoCorporation_PLUSZLOCATIONS_1654856600431_1.csv","title":"Set variable 4"},{"location":"maximo-envizi/#sftp-create-file","text":"Writes Response body from \"HTTP Invoke method 2\" to the configured SFTP Server with the filename from \"Set variable 4\" at the preconfigured file path.","title":"SFTP Create file"},{"location":"maximo-envizi/#response","text":"Returns HTTP Status 200 with JSON body containing: - filesWritten : Array of file names written to the SFTP server, obtained from the output of \"For each: page\" - queryparams : query from \"Set variable\" The JSON in response body is not used or stored by Maximo.","title":"Response"},{"location":"service-request/","text":"Maximo Within Maximo, a change to the Service Request application is needed to see additional fields. 1. Service Request SR Application Designer a. Go to System Configuration -> Platform Configuration -> Application Designer b. Search for SR c. Switch to the Service Request Tab and scroll down to the Service Request Details section d. At the top, click the icon labeled Control Palette and add a Multipart Textbox at the top of the right section. Add these values within the properties of the Multipart Textbox. Be sure that the PLUSIREQCLASSID Attribute is taken from the TICKET Object. Field Name Value Attribute PLUSIREQCLASSID Attribute for Part 2 PLUSIREQCLASS.DESCRIPTION Lookup VALUELIST Input Mode for Part 2 Readonly e. Click Save Definition after the changes are added. AppConnect The configuration of AppConnect from the previous code pattern should provide the mxtririga and trimaximo accounts within AppConnect needed for the flows to work properly. Download and import the .yaml files for PLUSIMXServiceReq2TRI and PLUSITRIReqClass2MX and keep the urls handy for a later step. Use the following table for the parameters: Parameter Name Value mxUrl http://[host]:[port]/meaweb/esqueue/PLUSITRIRIGA/PLUSIMXSR triUrl http://[host]:[port]/oslc/so/triAPICServiceRequestCF mxDomain PLUSIREQCLASS TRIRIGA Populate the domain created in the Maximo pre-requisites Go to Tools -> System Setup -> Integration -> Integration Object Select triRequestClass - APIC - HTTP Post from the table and fill in the URL with the correct parameters from the App Connect section. Click Execute at the top of the window. The process will take a few minutes since there is a large amount of files, but once it is completed you can check that the batch processed correctly under the specified domain. Filter To filter the request from TRIRIGA to Maximo, update the Filter tab in the triAPICServiceRequest - OSLC - Outbound query to trigger for selected Request Classes. Select System for triRequestClassCL and then Run Report Step 1. Create a Service Request Prerequisite A service request needs a member of the TRIRIGA organization on both Maximo and TRIRIGA. If you are starting from scratch, run the PLUSITRIPERSONBATCH flow to sync the person records in both systems. If data is already aligned and present, you are ready to run the integration. Maximo to TRIRIGA Go to Service Desk -> Service Request and click on the blue plus sign to create a new service request. Assign a user to Reported By and Affected Person. From here, a request classification needs to be selected and the value and description will populate in. Click Save Service Request and the flow should fire. Create Service Request in Maximo TRIRIGA to Maximo Go to Requests -> Manage Requests -> Electrical & Lighting and click the Add button on the top right. Fill in the required fields and click Submit at the top right of the newly opened window. The flow should fire upon submission. TRIRIGA Service Request Troubleshooting Common Errors and their resolutions: Maximo Common errors found in the Maximo system Error Cause 401: Bad Request This usually means an aspect of the request was not sent correctly- double check what is being sent as well as the flow in AppConnect to make sure everything is correct and running. AppConnect Troubleshoot App Connect with the logging function. While the flow is stopped, add a Log node into the flow from the Toolbox tab. This will allow mapping of any field to the Logging section of the application. Select Info for the Log level and then map the field that needs debugging. In this example the Request Object has been mapped to see what is being sent through the flow. Click the icon to the right of the Message Detail to map the desired field. The Log node will compile the message and read it out in the Logging section. Diagnose the response that shows up in this section to learn what might be causing the issue. Step 1: Log Node Step 2: Log Content Logging section Step 3: Logging Dashboard TRIRIGA Common errors found in the TRIRIGA system Error Cause ERROR: Requested For Does not Exist No People record exists with the triIdTX value mentioned in triRequestedForTX field of the payload ERROR: Requested By Does not Exist No People record exists with the triIdTX value mentioned in triRequestedByTX field of the payload ERROR: Building Does not Exist No Building record exists with the triNameTX value mentioned in triBuildingTX field of the payload ERROR: Request Class Does not Exist No Request Class record exists with the triNameTX value mentioned in triRequestClassCL field of the payload ERROR: Organization Does not Exist No Organization record exists with the triPathTX value mentioned in triCustomerOrgTX field of the payload ERROR: Service Request Does not Exist No Service Request record exists with the triIdTX value mentioned in triExternalReferenceTX field of the payload","title":"Service request"},{"location":"service-request/#maximo","text":"Within Maximo, a change to the Service Request application is needed to see additional fields.","title":"Maximo"},{"location":"service-request/#1-service-request","text":"SR Application Designer a. Go to System Configuration -> Platform Configuration -> Application Designer b. Search for SR c. Switch to the Service Request Tab and scroll down to the Service Request Details section d. At the top, click the icon labeled Control Palette and add a Multipart Textbox at the top of the right section. Add these values within the properties of the Multipart Textbox. Be sure that the PLUSIREQCLASSID Attribute is taken from the TICKET Object. Field Name Value Attribute PLUSIREQCLASSID Attribute for Part 2 PLUSIREQCLASS.DESCRIPTION Lookup VALUELIST Input Mode for Part 2 Readonly e. Click Save Definition after the changes are added.","title":"1. Service Request"},{"location":"service-request/#appconnect","text":"The configuration of AppConnect from the previous code pattern should provide the mxtririga and trimaximo accounts within AppConnect needed for the flows to work properly. Download and import the .yaml files for PLUSIMXServiceReq2TRI and PLUSITRIReqClass2MX and keep the urls handy for a later step. Use the following table for the parameters: Parameter Name Value mxUrl http://[host]:[port]/meaweb/esqueue/PLUSITRIRIGA/PLUSIMXSR triUrl http://[host]:[port]/oslc/so/triAPICServiceRequestCF mxDomain PLUSIREQCLASS","title":"AppConnect"},{"location":"service-request/#tririga","text":"","title":"TRIRIGA"},{"location":"service-request/#populate-the-domain-created-in-the-maximo-pre-requisites","text":"Go to Tools -> System Setup -> Integration -> Integration Object Select triRequestClass - APIC - HTTP Post from the table and fill in the URL with the correct parameters from the App Connect section. Click Execute at the top of the window. The process will take a few minutes since there is a large amount of files, but once it is completed you can check that the batch processed correctly under the specified domain.","title":"Populate the domain created in the Maximo pre-requisites"},{"location":"service-request/#filter","text":"To filter the request from TRIRIGA to Maximo, update the Filter tab in the triAPICServiceRequest - OSLC - Outbound query to trigger for selected Request Classes. Select System for triRequestClassCL and then Run Report","title":"Filter"},{"location":"service-request/#step-1-create-a-service-request","text":"","title":"Step 1. Create a Service Request"},{"location":"service-request/#prerequisite","text":"A service request needs a member of the TRIRIGA organization on both Maximo and TRIRIGA. If you are starting from scratch, run the PLUSITRIPERSONBATCH flow to sync the person records in both systems. If data is already aligned and present, you are ready to run the integration.","title":"Prerequisite"},{"location":"service-request/#maximo-to-tririga","text":"Go to Service Desk -> Service Request and click on the blue plus sign to create a new service request. Assign a user to Reported By and Affected Person. From here, a request classification needs to be selected and the value and description will populate in. Click Save Service Request and the flow should fire. Create Service Request in Maximo","title":"Maximo to TRIRIGA"},{"location":"service-request/#tririga-to-maximo","text":"Go to Requests -> Manage Requests -> Electrical & Lighting and click the Add button on the top right. Fill in the required fields and click Submit at the top right of the newly opened window. The flow should fire upon submission. TRIRIGA Service Request","title":"TRIRIGA to Maximo"},{"location":"service-request/#troubleshooting","text":"Common Errors and their resolutions:","title":"Troubleshooting"},{"location":"service-request/#maximo_1","text":"Common errors found in the Maximo system Error Cause 401: Bad Request This usually means an aspect of the request was not sent correctly- double check what is being sent as well as the flow in AppConnect to make sure everything is correct and running.","title":"Maximo"},{"location":"service-request/#appconnect_1","text":"Troubleshoot App Connect with the logging function. While the flow is stopped, add a Log node into the flow from the Toolbox tab. This will allow mapping of any field to the Logging section of the application. Select Info for the Log level and then map the field that needs debugging. In this example the Request Object has been mapped to see what is being sent through the flow. Click the icon to the right of the Message Detail to map the desired field. The Log node will compile the message and read it out in the Logging section. Diagnose the response that shows up in this section to learn what might be causing the issue. Step 1: Log Node Step 2: Log Content Logging section Step 3: Logging Dashboard","title":"AppConnect"},{"location":"service-request/#tririga_1","text":"Common errors found in the TRIRIGA system Error Cause ERROR: Requested For Does not Exist No People record exists with the triIdTX value mentioned in triRequestedForTX field of the payload ERROR: Requested By Does not Exist No People record exists with the triIdTX value mentioned in triRequestedByTX field of the payload ERROR: Building Does not Exist No Building record exists with the triNameTX value mentioned in triBuildingTX field of the payload ERROR: Request Class Does not Exist No Request Class record exists with the triNameTX value mentioned in triRequestClassCL field of the payload ERROR: Organization Does not Exist No Organization record exists with the triPathTX value mentioned in triCustomerOrgTX field of the payload ERROR: Service Request Does not Exist No Service Request record exists with the triIdTX value mentioned in triExternalReferenceTX field of the payload","title":"TRIRIGA"},{"location":"tas-envizi-index/","text":"IBM TAS Connector for Envizi The IBM TAS Connector for Envizi is released under the name \"TAS Connector for Envizi\". This connector supports the following capabilities through an App Connect flow: -Automatically sync space and occupancy data from TAS with Envizi to enable energy usage calculations across entire facility portfolio with advanced analytics by location, by SQF, and by occupant. Use Case Examples Corporate sustainability managers can gather and maintain accurate sustainability data for their entire global real estate portfolio directly from TAS where those facilities are managed. Location data from TAS serves as a baseline for all other Sustainability reports in Envizi; space classification, floor space, and headcount data allows Sustainability managers to normalize data (by square meter, or by employee) to enable meaningful comparisons between buildings across the entire portfolio to identify opportunities to reduce environmental impact. Connector Architecture IBM App Connect provides a flexible environment for integration solutions to transform, enrich, route, and process business messages and data. App Connect Flows enable specific integration use cases by connecting to predefined APIs to route and map data. Mapping has been pre-defined, but it can be customized. Native API framework is used for TAS and enabled thorugh provided packages that can be imported. TAS to Envizi Integration Diagram Data Mapping The image below illustrates the type of data that is being sent by the API and App Connect Flows. TAS to Envizi Data Map App Connect Flows Included with this connector are two flows that export locations and accounts, along with all the required fields they contain. The table below shows the naming convention for these flows and the current integration use case. File Flow Destination Operation TririgaBuildings_Always_On_v1_1_1.yaml Space Data TAS to Envizi Changes Only TririgaBuildings_On_Demand_v1_1_1.yaml Space Data TAS to Envizi Bulk Initial Load Installation & Configuration Guide Before you begin you will need: An instance of App Connect Enterprise or App Connect Pro with the Designer component. Admin access to TAS with user/pw for integration Envizi instance with a AWS S3 Bucket Import AppConnect Cert to TAS to enable encrypted communication Downloadable Resources Download the zip file that has all of the flows and configuration files. Installation Steps Overview App Connect Configuration a. Import Flows into App Connect b. Configure Flows TAS a. Security Role Configuration b. Group Name Configuration Test a. TAS outbound connectivity Part 1: App Connect Configuration Note: IBM App Connect Professional or Enterprise is needed to run this flow. The flows have been tested on IBM Cloud App Connect, AWS App Connect, as well as the containerized version of App Connect. Note: The names in the screenshots are generic, the elements in this integration will not have the same names during setup. Adding Accounts Before importing the flow to App Connect, add Accounts for Amazon S3 and HTTP connectors. While adding the HTTP connector account, include credentials for the TAS user which can consume the OSLC API. Navigate to Catalog section of the App Connect instance Create Account 1 In the Search application field, type name of the connector. If the App Connect instance does not have an account for the connector, click on Connect to create a new account. Else, open the account selection drop down, and click on Add a new account ... Add a New Account Enter the necessary details for the connector a. For Amazon S3, it will be the Secret Access Key and Access Key ID provided by Envizi. b. For HTTP, it will be the Authentication Key or username and password needed for TAS. Connect to S3/HTTP Click on Connect Connect the Account From the account selection drop down, select the newly created account. e.g., The default name will be Account 2 if Account 1 is already present Click on the kebab menu (three dots) and select Rename Account Rename the Account step 1 Enter an account name and click on Rename Account . This name can now be used by the connector in the flow. Rename the Account step 2 Importing the flow Open the Dashboard of the App Connect instance. Click on the New button and select Import Flow . Import the flow Browse to the flow's YAML file and click on Import . Select the desired flow The flow will now be imported and opened. The main page for the flow Configuring the flow to use the right accounts When importing a flow, it is important to check if the flow is using the right accounts for the different connectors. Click on Edit Flow See if the connectors are using the right accounts. To change the account for any connector, select the connector and click on the dropdown icon next to the Account's name Change associated Account Select the account name to use from the list Select desired Account Configuring the Scheduler Click on the Scheduler node The Scheduler node Configure the schedule as needed Options for configuration- Hourly Options for configuration- Daily Part 2: TAS Configuration Step 1: Initial TAS Configuration Import OM Package Go to Tools -> Administration -> Object Migration and select New Import Package . Select the OM Package labeled APIConnector into the TAS instance. In the pop-up window, select Validate to validate that the package can be imported properly and then Import to start the import process Please refer to the IBM\u00ae TAS documentation for more information on Object Migration. TAS API User Access In order for App Connect to be able to use TAS APIs, it will need a user with certain permissions. These user's credentials will be configured in App Connect. Create an integration user by following the steps given in Chapter 2 . This user should be a non-admin user and not part of the Admin security group. Assign that user to a new or existing group for the integration. If you need to create a new security group, follow the steps given in Chapter 1 . Add the \"TAS Base License\" to the License Details section on the user Profile. Select the newly created group or desired existing group and switch to the Access tab and add the appropriate access for the integration. Add the permissions below for the new user's group: Module Business Object Permissions Location triBuilding Read triAPIConnect triAPICTimestamp Read and Update The user will now be able to interact with the proper TAS Modules. Minimum requirements For users to pull from these URLs, the minimum requirements are: URL Requirement GET /oslc/spq/triAPICOutboundBuildingQC READ access to triBuilding Business object GET /oslc/spq/triAPICTimeStampQC READ access to triAPICTimestamp Business Object POST /oslc/so/triAPICTimeStampRS/ Write access to triAPICTimestamp Business Object Step 2: Group Name Configuration The following steps outline the necessary configurations for the Envizi Group Name Configuration page. Data Modeler Go to Tools -> Builder Tools -> Data Modeler and using the Object Browser navigate to Location->triBuilding . Revise the BO and add four fields: cstEnviziParentOneTX , cstEnviziParentTwoTX , cstEnviziParentThreeTX and cstEnviziGroupNamePathTX Name and Label should be the following: Name Label cstEnviziParentOneTX Envizi Group 1 cstEnviziParentTwoTX Envizi Group 2 cstEnviziParentThreeTX Envizi Group 3 cstEnviziGroupNamePathTX Envizi Path After entering these values, click Publish to publish the BO Form Builder Under Tools -> Builder Tools -> Form Builder , click on the Location module on the left side of the screen and then click on triBuilding . Revise the triBuilding form by clicking Revise in the top right corner of the pop-up In the Navigation pane on the left side of the screen, click on triBuilding and then Add Tab . Enter cstEnvizi as the Name and Envizi as the Label. Click Apply Select this new tab and click on Add Section Enter cstEnviziDetails as the Name and Envizi Details as the Label . Click Apply . Now click on the newly created Section and select Add Field . Select each of the four created business objects from the previous step as fields under Envizi Details . Select cstEnviziGroupNamePathTX and modify Start Row to 3 and Col Span to 2 on the properties window. Mark this field as ReadOnly and click Apply . The form should look like this: 8. Click on triBuilding on the left panel and then click on Sort Tab . Move the cstEnvizi tab to the second position and click Apply 9. Publish the form Object Migration Go to Import Migration and import package EnviziConfig.zip To do that, click on New Import Package, and select the zip file and click Ok. A new window will be displayed. If it is not displayed, just select the package from the list. On this package, click on Validate and wait for the validation to complete. If no errors are displayed, import the package. Security Manager Go to Tools -> Administration -> Security Manager This application sets who can and cannot access this newly created tab. Click on ( Insert new created group name here ) group, and navigate to the Access tab On this tab select Location -> triBuilding -> cstEnvizi Choose the access level for the group and Save Workflow Builder Go to Tools -> Builder Tools -> Workflow Builder . Select Location -> triBuilding . Within the Location object, search for the existing Workflow triBuilding - Synchronous - Permanent Save Validation . Revise this workflow and after Call Module Level Validation add a new WF call to triBuilding - Update Envizi fields with defined options like displayed below: It will be defined as the image below: Click on the Start task at the top and publish the workflow My Reports and OSLC Go to My Reports and navigate to System Reports . Add those four new fields to the existing query triAPICBuilding - OSLC -- Outbound by clicking the Columns tab and adding the four fields like below: Save the report. Open Tools -> System Setup -> Integration -> OSLC Resource manager and search for triAPICOutboundBuildingRS . On this resource, add the four new fields either individually or using Import all Fields Navigation Builder Go to Tools->Builder Tools-> Navigation Builder and find TAS Global Menu (or the menu associated to the user that will need access to the app). Select and click Edit On navigation Items section, expand Landing Page \u2013Tools -> Menu Group \u2013>System Setup . Select Menu group \u2013>Integration and expand Navigations Item Library Search for Envizi, select the item and click on Add to Collection Click Save . Logout and Login again to the system Using the Integration This tool will allow user to make changes on this new Envizi group name fields. But we must consider the existing records too. If you want those records to be populated, there is a patch helper workflow that can handle that. The first thing that must be defined is which fields will be used to populate groups 1, 2 and 3 to be used on Envizi. To access the Envizi tool, go to Tools -> System Setup -> Integration -> Envizi Integation . When you open the page, the fields will be displayed as Group1/Group2/Group3. By default the values are World Region/Country/City. The field Envizi Hierarchy Path shows how the Envizi groupnames will be configured according to the selected option. Enable Envizi checkbox is available too. The Envizi tab will be displayed only when this checkbox is marked One more item that must be configured is the Number of levels to be used on the Envizi configuration. Envizi hierarchy path will match this selection. Also, notice that there is a section named Active/Retire with missing data and Draft/Revision with Missing Data . This section will list the buildings that don\u2019t have data defined for Envizi group 3, so it means that no Envizi group will be populated on those buildings. You can filter to change only the desired records by changing query cst -triBuilding -Query -Get All Buildings for envizi . The list of buildings displayed on this query will be the list of buildings that the patch helper will modify. To use the tool, just select the desired Envizi group names and click Save . On the moment Save is triggered, all buildings will be populated with the desired options. This process may take a few minutes depending on how many buildings you have on your system. After that Envizi groups and path will be updated according to the selections made on Envizi Integration page. Also, every time a building is saved and there are changes on the defined fields, or a new building is created, the Envizi groups and path will be modified according to the selected options. You can find the groups on tab Envizi on the building record Operating the Connector On-demand Flow This flow is for the initial sync or to sync data that was added/updated between specific dates. This flow is meant to be executed just once whenever needed and then stopped. The following parameters in the initial Set variable node need to be configured in order to use this flow: Override Dates in Set Variable Parameter Value OverrideFromDate The start timestamp of the window between which the data will be pulled from. e.g., 2022-06-26T23:09:30-07:00 OverrideToDate The end timestamp of the window between which the data will be pulled from. e.g., 2023-06-26T23:09:30-07:00 These dates must be specified in ISO 8601 format Always-On Flow This flow is to keep syncing the data after the initial sync. This flow is meant to be kept running and will only sync the data that has been added or updated after its previous execution event. For example, if the flow executes at 2 PM and it's previous execution was at 1 PM, the flow will pull data that has been added or updated after 1 PM. Configuring the Flow Parameters Click on the initial Set variable node Fields in the initial Set variable node In Variable -> config -> customer , enter the value provided by Envizi In Variable -> config -> triURL , enter URL for the TAS instance. (e.g., https://example.com:9080) Starting and Stopping the flow Click on the kebab menu (three dots) on either the flow's tile or the specific flow page. Start the flow from the dashboard Start the flow from the page Click on Start API or Stop API depending on which action is desired. Part 3: Testing A good way to test the TAS Outbound connectivity is to use the Always_On flow. Start the Always_On flow and add a test building in the system. If configured correctly, the integration should pick up this change and deliver a .csv file with just that test building. See below for any errors that arise in App Connect. Troubleshooting Common errors that arise from TAS The below errors are found in the App Connect logs. Error Cause Resolution 404 - The HTTP request returned with an error 404 \"Not Found\" Incorrect App Connect connector config Double check that the credentials being used in the HTTP post node in App Connect are correct 401 - Authorization error Too many user sessions open in TAS Open the Admin dashboard on the TAS environment and check the Users logged in. This issue can arise after a number of requests are made to TAS and then gives a 401 error even with the proper credentials. Clear the users logged in and the issue should clear. If these do not resolve the issue, try clearing the OSLC Cache in TAS Admin Console in case the integrations do not work in intended manner. Reference for Pre-requisite TAS Certificates Note If you have not already done so, please import App Connect Cert to TAS to enable encrypted communication. Provide the Cert from App Connect as a secret to the instance of TAS as such: cat <<EOF | oc create -f - apiVersion: truststore-mgr.ibm.com/v1 kind: Truststore metadata: name: my-tas-truststore spec: license: accept: true includeDefaultCAs: true servers: - \"example.com:443\" certificates: - alias: alias_1 crt: | -----BEGIN CERTIFICATE----- ... Certificate 1 ... -----END CERTIFICATE----- ... - alias: alias_n crt: | -----BEGIN CERTIFICATE----- ... Certificate n ... -----END CERTIFICATE----- EOF This must then be added as the truststore to the TAS instance. In the Custom Resource Definition for TAS, update the spec.integration.truststore field to reference the name of the created truststore. If there already is a truststore for TAS, update the Truststore resource to include the certificate with an additional alias. Field Mapping Buildings CSV Headers TAS Fields Comments CITY spi:triCityTX COUNTRY spi:triCountryTX DESCRIPTION spi:triDescriptionTX GROUPNAME1 spi:cstEnviziParentOneTX Value for this field will be available only after Location Hierarchy mapping for Envizi groups is completed on TAS GROUPNAME2 spi:cstEnviziParentTwoTX Value for this field will be available only after Location Hierarchy mapping for Envizi groups is completed on TAS GROUPNAME3 spi:cstEnviziParentThreeTX Value for this field will be available only after Location Hierarchy mapping for Envizi groups is completed on TAS LATITUDEY spi:triGisLatitudeNU LOCATION spi:triNameTX LOCATIONCLOSEDATE spi:triActiveEndDA LOCATIONID spi:triIdTX LONGITUDEX spi:triGisLongitudeNU POSTALCODE spi:triZipPostalTX STATEPROVINCE spi:triStateProvTX STREETADDRESS spi:triAddressTX Accounts CSV Header TAS Field Comment ACCOUNT spi:triIdTX + (\"_HEADCOUNT\" or \"_FLOORAREA\") DATATYPE \"HEADCOUNT\" or \"FLOORAREA\" LOCATION spi:triNameTX LOCATIONID spi:triIdTX MEASUREUNITID spi:triAreaUO METERNAME \"HEADCOUNT\" or \"FLOORAREA\" READING spi:triHeadcountNU or spi:triTotalAreaOccupiedCalcNU READINGDATE spi:triModifiedSY","title":"IBM TAS Connector for Envizi"},{"location":"tas-envizi-index/#ibm-tas-connector-for-envizi","text":"The IBM TAS Connector for Envizi is released under the name \"TAS Connector for Envizi\". This connector supports the following capabilities through an App Connect flow: -Automatically sync space and occupancy data from TAS with Envizi to enable energy usage calculations across entire facility portfolio with advanced analytics by location, by SQF, and by occupant.","title":"IBM TAS Connector for Envizi"},{"location":"tas-envizi-index/#use-case-examples","text":"Corporate sustainability managers can gather and maintain accurate sustainability data for their entire global real estate portfolio directly from TAS where those facilities are managed. Location data from TAS serves as a baseline for all other Sustainability reports in Envizi; space classification, floor space, and headcount data allows Sustainability managers to normalize data (by square meter, or by employee) to enable meaningful comparisons between buildings across the entire portfolio to identify opportunities to reduce environmental impact.","title":"Use Case Examples"},{"location":"tas-envizi-index/#connector-architecture","text":"IBM App Connect provides a flexible environment for integration solutions to transform, enrich, route, and process business messages and data. App Connect Flows enable specific integration use cases by connecting to predefined APIs to route and map data. Mapping has been pre-defined, but it can be customized. Native API framework is used for TAS and enabled thorugh provided packages that can be imported. TAS to Envizi Integration Diagram","title":"Connector Architecture"},{"location":"tas-envizi-index/#data-mapping","text":"The image below illustrates the type of data that is being sent by the API and App Connect Flows. TAS to Envizi Data Map","title":"Data Mapping"},{"location":"tas-envizi-index/#app-connect-flows","text":"Included with this connector are two flows that export locations and accounts, along with all the required fields they contain. The table below shows the naming convention for these flows and the current integration use case. File Flow Destination Operation TririgaBuildings_Always_On_v1_1_1.yaml Space Data TAS to Envizi Changes Only TririgaBuildings_On_Demand_v1_1_1.yaml Space Data TAS to Envizi Bulk Initial Load","title":"App Connect Flows"},{"location":"tas-envizi-index/#installation-configuration-guide","text":"","title":"Installation &amp; Configuration Guide"},{"location":"tas-envizi-index/#before-you-begin-you-will-need","text":"An instance of App Connect Enterprise or App Connect Pro with the Designer component. Admin access to TAS with user/pw for integration Envizi instance with a AWS S3 Bucket Import AppConnect Cert to TAS to enable encrypted communication","title":"Before you begin you will need:"},{"location":"tas-envizi-index/#downloadable-resources","text":"Download the zip file that has all of the flows and configuration files.","title":"Downloadable Resources"},{"location":"tas-envizi-index/#installation-steps-overview","text":"App Connect Configuration a. Import Flows into App Connect b. Configure Flows TAS a. Security Role Configuration b. Group Name Configuration Test a. TAS outbound connectivity","title":"Installation Steps Overview"},{"location":"tas-envizi-index/#part-1-app-connect-configuration","text":"Note: IBM App Connect Professional or Enterprise is needed to run this flow. The flows have been tested on IBM Cloud App Connect, AWS App Connect, as well as the containerized version of App Connect. Note: The names in the screenshots are generic, the elements in this integration will not have the same names during setup.","title":"Part 1: App Connect Configuration"},{"location":"tas-envizi-index/#adding-accounts","text":"Before importing the flow to App Connect, add Accounts for Amazon S3 and HTTP connectors. While adding the HTTP connector account, include credentials for the TAS user which can consume the OSLC API. Navigate to Catalog section of the App Connect instance Create Account 1 In the Search application field, type name of the connector. If the App Connect instance does not have an account for the connector, click on Connect to create a new account. Else, open the account selection drop down, and click on Add a new account ... Add a New Account Enter the necessary details for the connector a. For Amazon S3, it will be the Secret Access Key and Access Key ID provided by Envizi. b. For HTTP, it will be the Authentication Key or username and password needed for TAS. Connect to S3/HTTP Click on Connect Connect the Account From the account selection drop down, select the newly created account. e.g., The default name will be Account 2 if Account 1 is already present Click on the kebab menu (three dots) and select Rename Account Rename the Account step 1 Enter an account name and click on Rename Account . This name can now be used by the connector in the flow. Rename the Account step 2","title":"Adding Accounts"},{"location":"tas-envizi-index/#importing-the-flow","text":"Open the Dashboard of the App Connect instance. Click on the New button and select Import Flow . Import the flow Browse to the flow's YAML file and click on Import . Select the desired flow The flow will now be imported and opened. The main page for the flow","title":"Importing the flow"},{"location":"tas-envizi-index/#configuring-the-flow-to-use-the-right-accounts","text":"When importing a flow, it is important to check if the flow is using the right accounts for the different connectors. Click on Edit Flow See if the connectors are using the right accounts. To change the account for any connector, select the connector and click on the dropdown icon next to the Account's name Change associated Account Select the account name to use from the list Select desired Account","title":"Configuring the flow to use the right accounts"},{"location":"tas-envizi-index/#configuring-the-scheduler","text":"Click on the Scheduler node The Scheduler node Configure the schedule as needed Options for configuration- Hourly Options for configuration- Daily","title":"Configuring the Scheduler"},{"location":"tas-envizi-index/#part-2-tas-configuration","text":"","title":"Part 2: TAS Configuration"},{"location":"tas-envizi-index/#step-1-initial-tas-configuration","text":"","title":"Step 1: Initial TAS Configuration"},{"location":"tas-envizi-index/#import-om-package","text":"Go to Tools -> Administration -> Object Migration and select New Import Package . Select the OM Package labeled APIConnector into the TAS instance. In the pop-up window, select Validate to validate that the package can be imported properly and then Import to start the import process Please refer to the IBM\u00ae TAS documentation for more information on Object Migration.","title":"Import OM Package"},{"location":"tas-envizi-index/#tas-api-user-access","text":"In order for App Connect to be able to use TAS APIs, it will need a user with certain permissions. These user's credentials will be configured in App Connect. Create an integration user by following the steps given in Chapter 2 . This user should be a non-admin user and not part of the Admin security group. Assign that user to a new or existing group for the integration. If you need to create a new security group, follow the steps given in Chapter 1 . Add the \"TAS Base License\" to the License Details section on the user Profile. Select the newly created group or desired existing group and switch to the Access tab and add the appropriate access for the integration. Add the permissions below for the new user's group: Module Business Object Permissions Location triBuilding Read triAPIConnect triAPICTimestamp Read and Update The user will now be able to interact with the proper TAS Modules.","title":"TAS API User Access"},{"location":"tas-envizi-index/#minimum-requirements","text":"For users to pull from these URLs, the minimum requirements are: URL Requirement GET /oslc/spq/triAPICOutboundBuildingQC READ access to triBuilding Business object GET /oslc/spq/triAPICTimeStampQC READ access to triAPICTimestamp Business Object POST /oslc/so/triAPICTimeStampRS/ Write access to triAPICTimestamp Business Object","title":"Minimum requirements"},{"location":"tas-envizi-index/#step-2-group-name-configuration","text":"The following steps outline the necessary configurations for the Envizi Group Name Configuration page.","title":"Step 2: Group Name Configuration"},{"location":"tas-envizi-index/#data-modeler","text":"Go to Tools -> Builder Tools -> Data Modeler and using the Object Browser navigate to Location->triBuilding . Revise the BO and add four fields: cstEnviziParentOneTX , cstEnviziParentTwoTX , cstEnviziParentThreeTX and cstEnviziGroupNamePathTX Name and Label should be the following: Name Label cstEnviziParentOneTX Envizi Group 1 cstEnviziParentTwoTX Envizi Group 2 cstEnviziParentThreeTX Envizi Group 3 cstEnviziGroupNamePathTX Envizi Path After entering these values, click Publish to publish the BO","title":"Data Modeler"},{"location":"tas-envizi-index/#form-builder","text":"Under Tools -> Builder Tools -> Form Builder , click on the Location module on the left side of the screen and then click on triBuilding . Revise the triBuilding form by clicking Revise in the top right corner of the pop-up In the Navigation pane on the left side of the screen, click on triBuilding and then Add Tab . Enter cstEnvizi as the Name and Envizi as the Label. Click Apply Select this new tab and click on Add Section Enter cstEnviziDetails as the Name and Envizi Details as the Label . Click Apply . Now click on the newly created Section and select Add Field . Select each of the four created business objects from the previous step as fields under Envizi Details . Select cstEnviziGroupNamePathTX and modify Start Row to 3 and Col Span to 2 on the properties window. Mark this field as ReadOnly and click Apply . The form should look like this: 8. Click on triBuilding on the left panel and then click on Sort Tab . Move the cstEnvizi tab to the second position and click Apply 9. Publish the form","title":"Form Builder"},{"location":"tas-envizi-index/#object-migration","text":"Go to Import Migration and import package EnviziConfig.zip To do that, click on New Import Package, and select the zip file and click Ok. A new window will be displayed. If it is not displayed, just select the package from the list. On this package, click on Validate and wait for the validation to complete. If no errors are displayed, import the package.","title":"Object Migration"},{"location":"tas-envizi-index/#security-manager","text":"Go to Tools -> Administration -> Security Manager This application sets who can and cannot access this newly created tab. Click on ( Insert new created group name here ) group, and navigate to the Access tab On this tab select Location -> triBuilding -> cstEnvizi Choose the access level for the group and Save","title":"Security Manager"},{"location":"tas-envizi-index/#workflow-builder","text":"Go to Tools -> Builder Tools -> Workflow Builder . Select Location -> triBuilding . Within the Location object, search for the existing Workflow triBuilding - Synchronous - Permanent Save Validation . Revise this workflow and after Call Module Level Validation add a new WF call to triBuilding - Update Envizi fields with defined options like displayed below: It will be defined as the image below: Click on the Start task at the top and publish the workflow","title":"Workflow Builder"},{"location":"tas-envizi-index/#my-reports-and-oslc","text":"Go to My Reports and navigate to System Reports . Add those four new fields to the existing query triAPICBuilding - OSLC -- Outbound by clicking the Columns tab and adding the four fields like below: Save the report. Open Tools -> System Setup -> Integration -> OSLC Resource manager and search for triAPICOutboundBuildingRS . On this resource, add the four new fields either individually or using Import all Fields","title":"My Reports and OSLC"},{"location":"tas-envizi-index/#navigation-builder","text":"Go to Tools->Builder Tools-> Navigation Builder and find TAS Global Menu (or the menu associated to the user that will need access to the app). Select and click Edit On navigation Items section, expand Landing Page \u2013Tools -> Menu Group \u2013>System Setup . Select Menu group \u2013>Integration and expand Navigations Item Library Search for Envizi, select the item and click on Add to Collection Click Save . Logout and Login again to the system","title":"Navigation Builder"},{"location":"tas-envizi-index/#using-the-integration","text":"This tool will allow user to make changes on this new Envizi group name fields. But we must consider the existing records too. If you want those records to be populated, there is a patch helper workflow that can handle that. The first thing that must be defined is which fields will be used to populate groups 1, 2 and 3 to be used on Envizi. To access the Envizi tool, go to Tools -> System Setup -> Integration -> Envizi Integation . When you open the page, the fields will be displayed as Group1/Group2/Group3. By default the values are World Region/Country/City. The field Envizi Hierarchy Path shows how the Envizi groupnames will be configured according to the selected option. Enable Envizi checkbox is available too. The Envizi tab will be displayed only when this checkbox is marked One more item that must be configured is the Number of levels to be used on the Envizi configuration. Envizi hierarchy path will match this selection. Also, notice that there is a section named Active/Retire with missing data and Draft/Revision with Missing Data . This section will list the buildings that don\u2019t have data defined for Envizi group 3, so it means that no Envizi group will be populated on those buildings. You can filter to change only the desired records by changing query cst -triBuilding -Query -Get All Buildings for envizi . The list of buildings displayed on this query will be the list of buildings that the patch helper will modify. To use the tool, just select the desired Envizi group names and click Save . On the moment Save is triggered, all buildings will be populated with the desired options. This process may take a few minutes depending on how many buildings you have on your system. After that Envizi groups and path will be updated according to the selections made on Envizi Integration page. Also, every time a building is saved and there are changes on the defined fields, or a new building is created, the Envizi groups and path will be modified according to the selected options. You can find the groups on tab Envizi on the building record","title":"Using the Integration"},{"location":"tas-envizi-index/#operating-the-connector","text":"","title":"Operating the Connector"},{"location":"tas-envizi-index/#on-demand-flow","text":"This flow is for the initial sync or to sync data that was added/updated between specific dates. This flow is meant to be executed just once whenever needed and then stopped. The following parameters in the initial Set variable node need to be configured in order to use this flow: Override Dates in Set Variable Parameter Value OverrideFromDate The start timestamp of the window between which the data will be pulled from. e.g., 2022-06-26T23:09:30-07:00 OverrideToDate The end timestamp of the window between which the data will be pulled from. e.g., 2023-06-26T23:09:30-07:00 These dates must be specified in ISO 8601 format","title":"On-demand Flow"},{"location":"tas-envizi-index/#always-on-flow","text":"This flow is to keep syncing the data after the initial sync. This flow is meant to be kept running and will only sync the data that has been added or updated after its previous execution event. For example, if the flow executes at 2 PM and it's previous execution was at 1 PM, the flow will pull data that has been added or updated after 1 PM.","title":"Always-On Flow"},{"location":"tas-envizi-index/#configuring-the-flow-parameters","text":"Click on the initial Set variable node Fields in the initial Set variable node In Variable -> config -> customer , enter the value provided by Envizi In Variable -> config -> triURL , enter URL for the TAS instance. (e.g., https://example.com:9080)","title":"Configuring the Flow Parameters"},{"location":"tas-envizi-index/#starting-and-stopping-the-flow","text":"Click on the kebab menu (three dots) on either the flow's tile or the specific flow page. Start the flow from the dashboard Start the flow from the page Click on Start API or Stop API depending on which action is desired.","title":"Starting and Stopping the flow"},{"location":"tas-envizi-index/#part-3-testing","text":"A good way to test the TAS Outbound connectivity is to use the Always_On flow. Start the Always_On flow and add a test building in the system. If configured correctly, the integration should pick up this change and deliver a .csv file with just that test building. See below for any errors that arise in App Connect.","title":"Part 3: Testing"},{"location":"tas-envizi-index/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"tas-envizi-index/#common-errors-that-arise-from-tas","text":"The below errors are found in the App Connect logs. Error Cause Resolution 404 - The HTTP request returned with an error 404 \"Not Found\" Incorrect App Connect connector config Double check that the credentials being used in the HTTP post node in App Connect are correct 401 - Authorization error Too many user sessions open in TAS Open the Admin dashboard on the TAS environment and check the Users logged in. This issue can arise after a number of requests are made to TAS and then gives a 401 error even with the proper credentials. Clear the users logged in and the issue should clear. If these do not resolve the issue, try clearing the OSLC Cache in TAS Admin Console in case the integrations do not work in intended manner.","title":"Common errors that arise from TAS"},{"location":"tas-envizi-index/#reference-for-pre-requisite","text":"","title":"Reference for Pre-requisite"},{"location":"tas-envizi-index/#tas-certificates","text":"Note If you have not already done so, please import App Connect Cert to TAS to enable encrypted communication. Provide the Cert from App Connect as a secret to the instance of TAS as such: cat <<EOF | oc create -f - apiVersion: truststore-mgr.ibm.com/v1 kind: Truststore metadata: name: my-tas-truststore spec: license: accept: true includeDefaultCAs: true servers: - \"example.com:443\" certificates: - alias: alias_1 crt: | -----BEGIN CERTIFICATE----- ... Certificate 1 ... -----END CERTIFICATE----- ... - alias: alias_n crt: | -----BEGIN CERTIFICATE----- ... Certificate n ... -----END CERTIFICATE----- EOF This must then be added as the truststore to the TAS instance. In the Custom Resource Definition for TAS, update the spec.integration.truststore field to reference the name of the created truststore. If there already is a truststore for TAS, update the Truststore resource to include the certificate with an additional alias.","title":"TAS Certificates"},{"location":"tas-envizi-index/#field-mapping","text":"","title":"Field Mapping"},{"location":"tas-envizi-index/#buildings","text":"CSV Headers TAS Fields Comments CITY spi:triCityTX COUNTRY spi:triCountryTX DESCRIPTION spi:triDescriptionTX GROUPNAME1 spi:cstEnviziParentOneTX Value for this field will be available only after Location Hierarchy mapping for Envizi groups is completed on TAS GROUPNAME2 spi:cstEnviziParentTwoTX Value for this field will be available only after Location Hierarchy mapping for Envizi groups is completed on TAS GROUPNAME3 spi:cstEnviziParentThreeTX Value for this field will be available only after Location Hierarchy mapping for Envizi groups is completed on TAS LATITUDEY spi:triGisLatitudeNU LOCATION spi:triNameTX LOCATIONCLOSEDATE spi:triActiveEndDA LOCATIONID spi:triIdTX LONGITUDEX spi:triGisLongitudeNU POSTALCODE spi:triZipPostalTX STATEPROVINCE spi:triStateProvTX STREETADDRESS spi:triAddressTX","title":"Buildings"},{"location":"tas-envizi-index/#accounts","text":"CSV Header TAS Field Comment ACCOUNT spi:triIdTX + (\"_HEADCOUNT\" or \"_FLOORAREA\") DATATYPE \"HEADCOUNT\" or \"FLOORAREA\" LOCATION spi:triNameTX LOCATIONID spi:triIdTX MEASUREUNITID spi:triAreaUO METERNAME \"HEADCOUNT\" or \"FLOORAREA\" READING spi:triHeadcountNU or spi:triTotalAreaOccupiedCalcNU READINGDATE spi:triModifiedSY","title":"Accounts"},{"location":"tririga-configuration/","text":"Tririga Configuration for Envizi Integration Table of Contents Overview Security Role Configuration Object Migration Access Import OM Package Tririga API User Access Outbound Traffic Inbound Traffic Group Name Configuration Data Modeler Form Builder Security Manager Workflow Builder My Reports/OSLC How to Use TimeStamp Pre-Requisite Using the Integration Overview This solution adds four fields that hold the group name values and the path for the integration. It also makes sure that the payload is sent when there is an update. Security Role Configuration In order to properly configure Tririga, a user needs to be configured with the proper security access for Object Migration and Tririga APIs Object Migration Access Object Migration is a task managed by administrators. If a Tririga User needs to have full access to Object Migration in Tririga, access is granted at the group level. Follow the steps given in Chapter 1 to create a new security group. Select the newly created group or desired existing group and switch to the Access tab and add the appropriate access for importation of Object Migration Packages. There are 2 panes in Access : Object and Permissions . Scroll and find the Object Migration Object on the left pane and click Full Access on the right pane. The users in this group, granted through the Members tab, will be able to import the Tririga API Object Migration package. Access Permissions Additional information on Tririga Security groups Import OM Package Import the most recent OM Package into the Tririga instance. Go to Tools -> Administration -> Object Migration and select New Import Package to begin the import process. Please refer to official IBM\u00ae Tririga documentation for more information on Object Migration Tririga API User Access In order for AppConnect to be able to use Tririga APIs, it will need a user with certain permissions. These user's credentials will be configured in App Connect. Create a new user by following the steps given in Chapter 2 . Choose a security group or create a new group (refer to the above OM Migration steps if a new security group needs to be created) and add the newly created user to it. Add the permissions below for the new user's group: Module Business Object Permissions Location triBuilding Read triAPIConnect triAPICTimestamp Read and Update Follow the steps given in Chapter 1 and add any one of the licenses below for the new user's group: a. IBM TRIRIGA Portfolio Data Manager b. IBM Facilities and Real Estate Management on Cloud Self Service c. Any other license that grants access to the modules Please refer to the TRIRIGA Documentation on Security and Licenses for additional information. The user will now be able to interact with the proper Tririga Modules. Outbound Traffic from Tririga For outbound traffic from Tririga, grant at least READ access on the Business Objects that will be used. The table below shows the various supported business Objects the API can pull from: Module Business Object Label Asset Building Equipment Classification Request Class Classification Space Class Current Classification Asset Spec Class People People Location Property Location Building Location Floor Location Space Organization Organization Request Service Request Task Work Task In the example below, the API user is able to pull data from the Building Business Object: Inbound traffic to Tririga For inbound traffic, Data Access needs to be enabled as well as Application Access permissions to the triAPIConnect Module or the individual Objects. To enable an API user to create a building, grant access to the triAPICBuilding Business object as shown below: Minimum requirements For users to pull from these URLs, the minimum requirements are: URL Requirement GET /oslc/spq/triAPICOutboundBuildingQC READ access to triBuilding Business object GET /oslc/spq/triAPICTimeStampQC READ access to triAPICTimestamp Business Object POST /oslc/so/triAPICTimeStampRS/ Write access to triAPICTimestamp Business Object Group Name Configuration Data Modeler Go to Tools -> Builder Tools -> Data Modeler and using the Object Browser navigate to Location->triBuilding. Revise the BO and add four fields: cstEnviziParentOneTX, cstEnviziParentTwoTX, cstEnviziParentThreeTX and cstEnviziGroupNamePathTX Name and Label should be the following: Name Label cstEnviziParentOneTX Envizi Group 1 cstEnviziParentTwoTX Envizi Group 2 cstEnviziParentThreeTX Envizi Group 3 cstEnviziGroupNamePathTX Envizi Path After entering these values, click 'Publish' to publish the BO Form Builder Under Tools -> Builder Tools -> Form Builder, click on the Location module on the left side of the screen and then click on triBuilding. Revise the triBuilding form by clicking 'Revise' in the top right corner of the pop-up In the Navigation pane on the left side of the screen, click on triBuilding and then 'Add Tab'. Enter \"cstEnvizi\" as the 'Name' and 'Envizi' as the Label. Click Apply Select this new tab and click on 'Add Section' Enter \"cstEnviziDetails\" as the 'Name' and \"Envizi Details\" as the 'Label'. Click 'Apply'. Now click on the newly created Section and select 'Add Field'. Select each of the four created business objects from the previous step as fields under \"Envizi Details\". Select 'cstEnviziGroupNamePathTX' and modify 'Start Row' to 3 and 'Col Span' to 2 on the properties window. Mark this field as \"ReadOnly\" and click 'Apply'. The form should look like this: Click on triBuilding on the left panel and then click on 'Sort Tab'. Move the 'cstEnvizi' tab to the second position and click 'Apply' Publish the form Object Migration Go to Import Migration and import package EnviziConfig.zip To do that, click on New Import Package, and select the zip file and click Ok. A new window will be displayed. If it is not displayed, just select the package from the list. On this package, click on Validate and wait for the validation to complete. If no errors are displayed, import the package. Security Manager Go to Tools -> Administration -> Security Manager This application sets who can and cannot access this newly created tab. Click on the desired group, and navigate to the 'Access' tab On this tab select Location -> triBuilding -> cstEnvizi Choose the access level for the group and 'Save' Workflow Builder Go to Tools -> Builder Tools -> Workflow Builder. Select Location -> triBuilding. Within the Location object, search for the existing Workflow \"triBuilding - Synchronous - Permanent Save Validation\". Revise this workflow and after Call Module Level Validation add a new WF call to \"triBuilding - Update Envizi fields with defined options\" like displayed below: It will be defined as the image below: Click on the 'Start' task at the top and publish the workflow My Reports and OSLC Go to My Reports and navigate to 'System Reports'. Add those four new fields to the existing query \"triAPICBuilding - OSLC -- Outbound\" by clicking the 'Columns' tab and adding the four fields like below: Save the report. Open Tools -> System Setup -> Integration -> OSLC Resource manager and search for \"triAPICOutboundBuildingRS\". On this resource, add the four new fields either individually or using 'Import all Fields' Navigation Builder Go to Tools->Builder Tools-> Navigation Builder and find TRIRIGA Global Menu(or the menu associated to the user that will need access to the app). Select and click Edit On navigation Items section, expand Landing Page \u2013Tools -> Menu Group \u2013System Setup. Select Menu group \u2013Integration and expand Navigations Item Library Search for Envizi, select the item and click on Add to Collection Click Save. Logout and Login again to the system How to use Time Stamp Pre-requisite The triAPICTimestamp is a tririga record needed to set the baseline for when API connect runs for the first time. To enable this functionality go to My Reports -> System Reports and search for Timestamp in the 'Name' section. Run the system report \"triAPICTimestamp \u2013 Display \u2013 Manager Query\" as shown below: Click Add, and create a new record without details, as shown below, and close it The default date and time the record gets automatically applied to the record, and consequent opening of the record shows the default date and time as shown below: Using the Integration This tool will allow user to make changes on this new Envizi group name fields. But we must consider the existing records too. If you want those records to be populated, there is a patch helper workflow that can handle that. The first this that must be defined is which fields will be used to populate groups 1, 2 and 3 to be used on envizi. To access the envizi tool, go to Tools -> System Setup -> Integration -> Envizi Integation. When you open the page, the fieldswill be displayed as group1/group2/group3. By default the values are World Region/Country/City. The field Envizi Hierarchy Path shows how the envizi groupnames will be configured according to the selected option. Enable Envizi checkbox is available too. The envizi tab will be displayed only when this checkbox is marked One more item that must be configured is the Number of levels to be used on the envizi configuration. Envizihierarchy path will match this selection. Also, notice that there is a section named \u201cActive/Retire with missing data\u201dand \u201cDraft/Revision with Missing Data\u201d. This section will list the buildings that don\u2019t have data defined for envizi group 3, so it means that no envizi group will be populated on those buildings. You can filter to change only the desired records by changing query \u201ccst -triBuilding -Query -Get All Buildings for envizi\u201d. The list of buildings displayed on this query will be the listof buildings that the patch helper will modify. To use the tool, just select the desired envizi group names and click Save. On the moment Save is triggered, all buildings will be populated with the desired options. This process may take a few minutes depending on how many buildings you have on your system. After that envizi groups and path will be updated according to the selections made on Envizi Integration page. Also, every time a building is saved and there are changes on the defined fields, or a new building is created, the envizi groups and path will be modifiedaccording to the selected options. You can find the groups on tab Envizi on the building record","title":"Tririga Configuration for Envizi Integration"},{"location":"tririga-configuration/#tririga-configuration-for-envizi-integration","text":"","title":"Tririga Configuration for Envizi Integration"},{"location":"tririga-configuration/#table-of-contents","text":"Overview Security Role Configuration Object Migration Access Import OM Package Tririga API User Access Outbound Traffic Inbound Traffic Group Name Configuration Data Modeler Form Builder Security Manager Workflow Builder My Reports/OSLC How to Use TimeStamp Pre-Requisite Using the Integration","title":"Table of Contents"},{"location":"tririga-configuration/#overview","text":"This solution adds four fields that hold the group name values and the path for the integration. It also makes sure that the payload is sent when there is an update.","title":"Overview"},{"location":"tririga-configuration/#security-role-configuration","text":"In order to properly configure Tririga, a user needs to be configured with the proper security access for Object Migration and Tririga APIs","title":"Security Role Configuration"},{"location":"tririga-configuration/#object-migration-access","text":"Object Migration is a task managed by administrators. If a Tririga User needs to have full access to Object Migration in Tririga, access is granted at the group level. Follow the steps given in Chapter 1 to create a new security group. Select the newly created group or desired existing group and switch to the Access tab and add the appropriate access for importation of Object Migration Packages. There are 2 panes in Access : Object and Permissions . Scroll and find the Object Migration Object on the left pane and click Full Access on the right pane. The users in this group, granted through the Members tab, will be able to import the Tririga API Object Migration package. Access Permissions Additional information on Tririga Security groups","title":"Object Migration Access"},{"location":"tririga-configuration/#import-om-package","text":"Import the most recent OM Package into the Tririga instance. Go to Tools -> Administration -> Object Migration and select New Import Package to begin the import process. Please refer to official IBM\u00ae Tririga documentation for more information on Object Migration","title":"Import OM Package"},{"location":"tririga-configuration/#tririga-api-user-access","text":"In order for AppConnect to be able to use Tririga APIs, it will need a user with certain permissions. These user's credentials will be configured in App Connect. Create a new user by following the steps given in Chapter 2 . Choose a security group or create a new group (refer to the above OM Migration steps if a new security group needs to be created) and add the newly created user to it. Add the permissions below for the new user's group: Module Business Object Permissions Location triBuilding Read triAPIConnect triAPICTimestamp Read and Update Follow the steps given in Chapter 1 and add any one of the licenses below for the new user's group: a. IBM TRIRIGA Portfolio Data Manager b. IBM Facilities and Real Estate Management on Cloud Self Service c. Any other license that grants access to the modules Please refer to the TRIRIGA Documentation on Security and Licenses for additional information. The user will now be able to interact with the proper Tririga Modules.","title":"Tririga API User Access"},{"location":"tririga-configuration/#outbound-traffic-from-tririga","text":"For outbound traffic from Tririga, grant at least READ access on the Business Objects that will be used. The table below shows the various supported business Objects the API can pull from: Module Business Object Label Asset Building Equipment Classification Request Class Classification Space Class Current Classification Asset Spec Class People People Location Property Location Building Location Floor Location Space Organization Organization Request Service Request Task Work Task In the example below, the API user is able to pull data from the Building Business Object:","title":"Outbound Traffic from Tririga"},{"location":"tririga-configuration/#inbound-traffic-to-tririga","text":"For inbound traffic, Data Access needs to be enabled as well as Application Access permissions to the triAPIConnect Module or the individual Objects. To enable an API user to create a building, grant access to the triAPICBuilding Business object as shown below:","title":"Inbound traffic to Tririga"},{"location":"tririga-configuration/#minimum-requirements","text":"For users to pull from these URLs, the minimum requirements are: URL Requirement GET /oslc/spq/triAPICOutboundBuildingQC READ access to triBuilding Business object GET /oslc/spq/triAPICTimeStampQC READ access to triAPICTimestamp Business Object POST /oslc/so/triAPICTimeStampRS/ Write access to triAPICTimestamp Business Object","title":"Minimum requirements"},{"location":"tririga-configuration/#group-name-configuration","text":"","title":"Group Name Configuration"},{"location":"tririga-configuration/#data-modeler","text":"Go to Tools -> Builder Tools -> Data Modeler and using the Object Browser navigate to Location->triBuilding. Revise the BO and add four fields: cstEnviziParentOneTX, cstEnviziParentTwoTX, cstEnviziParentThreeTX and cstEnviziGroupNamePathTX Name and Label should be the following: Name Label cstEnviziParentOneTX Envizi Group 1 cstEnviziParentTwoTX Envizi Group 2 cstEnviziParentThreeTX Envizi Group 3 cstEnviziGroupNamePathTX Envizi Path After entering these values, click 'Publish' to publish the BO","title":"Data Modeler"},{"location":"tririga-configuration/#form-builder","text":"Under Tools -> Builder Tools -> Form Builder, click on the Location module on the left side of the screen and then click on triBuilding. Revise the triBuilding form by clicking 'Revise' in the top right corner of the pop-up In the Navigation pane on the left side of the screen, click on triBuilding and then 'Add Tab'. Enter \"cstEnvizi\" as the 'Name' and 'Envizi' as the Label. Click Apply Select this new tab and click on 'Add Section' Enter \"cstEnviziDetails\" as the 'Name' and \"Envizi Details\" as the 'Label'. Click 'Apply'. Now click on the newly created Section and select 'Add Field'. Select each of the four created business objects from the previous step as fields under \"Envizi Details\". Select 'cstEnviziGroupNamePathTX' and modify 'Start Row' to 3 and 'Col Span' to 2 on the properties window. Mark this field as \"ReadOnly\" and click 'Apply'. The form should look like this: Click on triBuilding on the left panel and then click on 'Sort Tab'. Move the 'cstEnvizi' tab to the second position and click 'Apply' Publish the form","title":"Form Builder"},{"location":"tririga-configuration/#object-migration","text":"Go to Import Migration and import package EnviziConfig.zip To do that, click on New Import Package, and select the zip file and click Ok. A new window will be displayed. If it is not displayed, just select the package from the list. On this package, click on Validate and wait for the validation to complete. If no errors are displayed, import the package.","title":"Object Migration"},{"location":"tririga-configuration/#security-manager","text":"Go to Tools -> Administration -> Security Manager This application sets who can and cannot access this newly created tab. Click on the desired group, and navigate to the 'Access' tab On this tab select Location -> triBuilding -> cstEnvizi Choose the access level for the group and 'Save'","title":"Security Manager"},{"location":"tririga-configuration/#workflow-builder","text":"Go to Tools -> Builder Tools -> Workflow Builder. Select Location -> triBuilding. Within the Location object, search for the existing Workflow \"triBuilding - Synchronous - Permanent Save Validation\". Revise this workflow and after Call Module Level Validation add a new WF call to \"triBuilding - Update Envizi fields with defined options\" like displayed below: It will be defined as the image below: Click on the 'Start' task at the top and publish the workflow","title":"Workflow Builder"},{"location":"tririga-configuration/#my-reports-and-oslc","text":"Go to My Reports and navigate to 'System Reports'. Add those four new fields to the existing query \"triAPICBuilding - OSLC -- Outbound\" by clicking the 'Columns' tab and adding the four fields like below: Save the report. Open Tools -> System Setup -> Integration -> OSLC Resource manager and search for \"triAPICOutboundBuildingRS\". On this resource, add the four new fields either individually or using 'Import all Fields'","title":"My Reports and OSLC"},{"location":"tririga-configuration/#navigation-builder","text":"Go to Tools->Builder Tools-> Navigation Builder and find TRIRIGA Global Menu(or the menu associated to the user that will need access to the app). Select and click Edit On navigation Items section, expand Landing Page \u2013Tools -> Menu Group \u2013System Setup. Select Menu group \u2013Integration and expand Navigations Item Library Search for Envizi, select the item and click on Add to Collection Click Save. Logout and Login again to the system","title":"Navigation Builder"},{"location":"tririga-configuration/#how-to-use","text":"","title":"How to use"},{"location":"tririga-configuration/#time-stamp-pre-requisite","text":"The triAPICTimestamp is a tririga record needed to set the baseline for when API connect runs for the first time. To enable this functionality go to My Reports -> System Reports and search for Timestamp in the 'Name' section. Run the system report \"triAPICTimestamp \u2013 Display \u2013 Manager Query\" as shown below: Click Add, and create a new record without details, as shown below, and close it The default date and time the record gets automatically applied to the record, and consequent opening of the record shows the default date and time as shown below:","title":"Time Stamp Pre-requisite"},{"location":"tririga-configuration/#using-the-integration","text":"This tool will allow user to make changes on this new Envizi group name fields. But we must consider the existing records too. If you want those records to be populated, there is a patch helper workflow that can handle that. The first this that must be defined is which fields will be used to populate groups 1, 2 and 3 to be used on envizi. To access the envizi tool, go to Tools -> System Setup -> Integration -> Envizi Integation. When you open the page, the fieldswill be displayed as group1/group2/group3. By default the values are World Region/Country/City. The field Envizi Hierarchy Path shows how the envizi groupnames will be configured according to the selected option. Enable Envizi checkbox is available too. The envizi tab will be displayed only when this checkbox is marked One more item that must be configured is the Number of levels to be used on the envizi configuration. Envizihierarchy path will match this selection. Also, notice that there is a section named \u201cActive/Retire with missing data\u201dand \u201cDraft/Revision with Missing Data\u201d. This section will list the buildings that don\u2019t have data defined for envizi group 3, so it means that no envizi group will be populated on those buildings. You can filter to change only the desired records by changing query \u201ccst -triBuilding -Query -Get All Buildings for envizi\u201d. The list of buildings displayed on this query will be the listof buildings that the patch helper will modify. To use the tool, just select the desired envizi group names and click Save. On the moment Save is triggered, all buildings will be populated with the desired options. This process may take a few minutes depending on how many buildings you have on your system. After that envizi groups and path will be updated according to the selections made on Envizi Integration page. Also, every time a building is saved and there are changes on the defined fields, or a new building is created, the envizi groups and path will be modifiedaccording to the selected options. You can find the groups on tab Envizi on the building record","title":"Using the Integration"},{"location":"tririga-envizi/","text":"Tririga - Envizi integration Using the flows Flows included in this integration: TririgaBuildings_Always_On TririgaBuildings_On_Demand This integration comes with two types of flows: On-demand Flow This flow is for the initial sync or to sync data that was added/updated between specific dates. This flow is meant to be executed just once whenever needed and then stopped. The following parameters in the initial Set variable node need to be configured in order to use this flow: Override Dates in Set Variable Parameter Value OverrideFromDate The start timestamp of the window between which the data will be pulled from. e.g., 2022-06-26T23:09:30-07:00 OverrideToDate The end timestamp of the window between which the data will be pulled from. e.g., 2023-06-26T23:09:30-07:00 These dates must be specified in ISO 8601 format Always-On Flow This flow is to keep syncing the data after the initial sync. This flow is meant to be kept running and will only sync the data that has been added or updated after its previous execution event. For example, if the flow executes at 2 PM and it's previous execution was at 1 PM, the flow will pull data that has been added or updated after 1 PM. Configuring the Flow Parameters Click on the initial Set variable node Fields in the initial Set variable node In Variable -> config -> customer , enter the value provided by Envizi In Variable -> config -> triURL , enter URL for the Tririga instance. (e.g., https://example.com:9080) Starting and Stopping the flow Click on the kebab menu (three dots) on either the flow's tile or the specific flow page. Start the flow from the dashboard Start the flow from the page Click on Start API or Stop API depending on which action is desired. Note: If the flow is not running, App Connect will give Error 404 on the API call. Post-Setup Instructions The Envizi implementation team will support the next steps of the integration by provisioning the file transfer details, enabling Envizi data connectors and validating the Envizi configuration ready to receive the data flow. Reach out to the primary contact at Envizi who can help coordinate next steps.","title":"Tririga - Envizi integration"},{"location":"tririga-envizi/#tririga-envizi-integration","text":"","title":"Tririga - Envizi integration"},{"location":"tririga-envizi/#using-the-flows","text":"Flows included in this integration: TririgaBuildings_Always_On TririgaBuildings_On_Demand This integration comes with two types of flows:","title":"Using the flows"},{"location":"tririga-envizi/#on-demand-flow","text":"This flow is for the initial sync or to sync data that was added/updated between specific dates. This flow is meant to be executed just once whenever needed and then stopped. The following parameters in the initial Set variable node need to be configured in order to use this flow: Override Dates in Set Variable Parameter Value OverrideFromDate The start timestamp of the window between which the data will be pulled from. e.g., 2022-06-26T23:09:30-07:00 OverrideToDate The end timestamp of the window between which the data will be pulled from. e.g., 2023-06-26T23:09:30-07:00 These dates must be specified in ISO 8601 format","title":"On-demand Flow"},{"location":"tririga-envizi/#always-on-flow","text":"This flow is to keep syncing the data after the initial sync. This flow is meant to be kept running and will only sync the data that has been added or updated after its previous execution event. For example, if the flow executes at 2 PM and it's previous execution was at 1 PM, the flow will pull data that has been added or updated after 1 PM.","title":"Always-On Flow"},{"location":"tririga-envizi/#configuring-the-flow-parameters","text":"Click on the initial Set variable node Fields in the initial Set variable node In Variable -> config -> customer , enter the value provided by Envizi In Variable -> config -> triURL , enter URL for the Tririga instance. (e.g., https://example.com:9080)","title":"Configuring the Flow Parameters"},{"location":"tririga-envizi/#starting-and-stopping-the-flow","text":"Click on the kebab menu (three dots) on either the flow's tile or the specific flow page. Start the flow from the dashboard Start the flow from the page Click on Start API or Stop API depending on which action is desired. Note: If the flow is not running, App Connect will give Error 404 on the API call.","title":"Starting and Stopping the flow"},{"location":"tririga-envizi/#post-setup-instructions","text":"The Envizi implementation team will support the next steps of the integration by provisioning the file transfer details, enabling Envizi data connectors and validating the Envizi configuration ready to receive the data flow. Reach out to the primary contact at Envizi who can help coordinate next steps.","title":"Post-Setup Instructions"},{"location":"turbonomic-envizi/","text":"Turbonomic - Envizi Integration Table of contents Use Case Example Connector Architecture Data Mapping App Connect Flows Installation & Configuration Guide Before you begin you will need: Installation Steps Overview Troubleshooting Use Case Example Sustainability Manager can view and compare the energy usage of all data centres within Envizi. They can understand energy efficiency among their data centres by number of active hosts, by number of VMs. They can understand the density of VMs to Hosts in each data centre to identify opportunities to optimize efficiency. Connector Architecture Data Mapping The image below illustrates the type of data that is being sent by the API and App Connect Flows. App Connect Flows Included with this connector are two flows that export locations and accounts, along with all the required fields they contain. The table below shows the naming convention for these flows and the current integration use case. File Flow Destination Operation TurbonomicLocations.yaml Locations Turbonomics to Envizi Depending on configuration Changes only or Bulk initial load TurbonomicAccounts.yaml Accounts Turbonomics to Envizi Depending on configuration Changes only or Bulk initial load Installation & Configuration Guide Before you begin you will need: Turbonomic version 8.6.6 or higher An instance of App Connect with the Designer component. Installation Steps Overview App Connect Configuration a. Add Accounts ( reference document ) b. Importing Flow in App Connect ( reference document ) Turbonomics Configuration a. Create user b. Configure tags Part 1. App Connect Configuration Note : When configuring App Connect you will have the option to set up a secure connection with a self-signed certificates, a private network, or both. Add Account for \"Amazon S3\" connector Credentials will be given by Envizi Add an Account for \"HTTP\" connector This account will be used for Authentication API Key : Enter the Turbonomic credentials in this format <USERNAME>&password=<PASSWORD> e.g., If the username is johndoe and the password is TheCakeIsALie , enter johndoe&password=TheCakeIsALie API Key Location : Select \"body URL encoded\" API Key Name : Enter the text username and not your actual username. Add another Account for \"HTTP\" connector This account will be used for other API calls Leave all authentication related fields empty Import the flows - Navigate to the flows below and use the link to their raw files for importing in App Connect: - TurbonomicLocations - TurbonomicAccounts Configure the flow to use the right accounts - For all Amazon S3 nodes in the flow, select the account created in Step 1 - For the first HTTP node in the flow, select the account created in Step 2 - For all other HTTP nodes in the flow, select the account created in Step 3 Configure the scheduler - Click on the \"Scheduler\" node <img width=\"659\" alt=\"Scheduler node\" src=\"https://media.github.ibm.com/user/375131/files/ea694d80-01ed-11ed-8852-16b625339675\"> - Configure the schedule as needed <img width=\"659\" alt=\"Scheduler configuration 2\" src=\"https://media.github.ibm.com/user/375131/files/00c3d900-01ef-11ed-86aa-29008ff86450\"> The TurbonomicAccounts flow must be executed daily, preferably at UTC midnight between 00:05 and 00:30 The TurbonomicLocations flow can be configured to run once a month or even On-demand as described here Note: By default, the flow will run when it is started. To change this behavior, untick the below checkbox in the \"Scheduler\" node. It is recommended to keep this checked for On-demand data pull Configure the flow parameters - Click on the \"Set variable\" node - In Variable > config > customer, enter the value provided by Envizi - In Variable > config > url, enter URL for the Turbonomic instance - e.g., https://example.com:9080 - Scroll to the Amazon S3 node and click on it to open the configuration ![S3 Configuration](https://media.github.ibm.com/user/375131/files/56c06800-4bb4-11ed-9aa6-c6758d61300a) - From the bucket dropdown, select the bucket name provided by Envizi - Perform these actions on all Amazon S3 nodes in the flow Start the flow Once the flow it started, it will run as configured in scheduler. However, the flow will only collect data from the day it has been started. In order to pull any historical data, additional configurations described in On-demand Data Pull need to be done. Part 2. Turbonomic Configuration Turbonomic v8.6.6 or higher is required for this integration to work Create a user in Turbonomic with \"Observer\" role. The App Connect will be configured to use this user's credentials to fetch the necessary data. For accurate emission calculations from Envizi, add the following Tag Categories in vCenter and add their values as tags to the Data Centers: Country : Name of Country Latitude : Latitude in Decimal Degrees format Longitude : Longitude in Decimal Degrees format - By default, Envizi will use the Data Center name configured in Turbonomic/vCenter. To change this, Tag Category EnviziAlternateName can be added with the desired display name as its value. - Envizi Locations (Data Centers in this case) need unique display names. If there are any Data Centers with same names, they should be changed from vCenter or Tag Category EnviziAlternateName should be added to the Data Center(s) with different name(s) Note: Tags sync from vCenter to Turbonomic might take upto 20 minutes. Operating the connector On-demand Data Pull Use the on-demand data pull when there is historical energy related data that needs to be loaded from Turbonomic to Envizi. This method is for the initial sync or to sync data that was added/updated between specific dates. This method is meant to be executed just once whenever needed and then stopped. App Connect logs will show when On-demand load has been completed successfully or that it can be stopped. For TurbonomicLocations, no additional configuration is needed. To use this method in flows other than TurbonomicLocations, the following parameters in \"Set variable\" node need to be configured: Variable > config > OverrideStartDate: The start timestamp of the window between which the data will be pulled from. The date must be specified in yyyy-mm-dd format. e.g., 2022-06-26 Variable > config > OverrideEndDate: The end timestamp of the window between which the data will be pulled from. The date must be specified in yyyy-mm-dd format. e.g., 2022-06-27 Note: The dates configured here will be considered as UTC Part 3. Testing and Verification Set the OverrideStartDate and OverrideEndDate in the flow to a date that you know the data exists for. Preferably keep it a single date. Refer to the On-demand Data Pull section for more details Make sure \"Also run the flow when it's switched on\" is selected in the \"Scheduler\" node Start the flow Open App Connect logs from the left side bar Keep refreshing the logs until you see a message that says \"Flow completed successfully\". If you get an error message, refer to the Troubleshooting section Troubleshooting The below errors are found in the App Connect logs. Error Cause Resolution The HTTP request returned with an error: 400 \"Bad Request\" No configuration for first HTTP Node Please configure the HTTP node with the exact same configurations as mentioned in the Step 5 of App Connect Configuration The HTTP request returned with an error: 400 \"Bad Request\" Invalid API Key name Enter the text username and not your actual username. The HTTP request returned with an error: 400 \"Bad Request\" Invalid API Key location Select \"body URL encoded\" in API Key Location The HTTP request returned with an error: 401 \"Unauthorized\" Invalid API Key in HTTP Account Enter the correct Turbonomic credentials in this format <USERNAME>&password=<PASSWORD> The HTTP request returned with an error: 500 \"Internal Server Error\" Invalid URL in Set Variable Configure the correct URL of the Turbonomic instance The HTTP request returned with an error: 404 \"Not Found\u201d If / is appended at the end of the valid URL Configure the correct URL of the Turbonomic instance and do not append / at the end of the URL Your Amazon S3 account doesn't have the required permissions to complete the isObjectPresent action on the object object: 403 \"Forbidden\" Invalid customer Configure the customer name in set variable exactly as provided by Envizi","title":"IBM Turbonomic Connector for Envizi"},{"location":"turbonomic-envizi/#turbonomic-envizi-integration","text":"","title":"Turbonomic - Envizi Integration"},{"location":"turbonomic-envizi/#table-of-contents","text":"Use Case Example Connector Architecture Data Mapping App Connect Flows Installation & Configuration Guide Before you begin you will need: Installation Steps Overview Troubleshooting","title":"Table of contents"},{"location":"turbonomic-envizi/#use-case-example","text":"Sustainability Manager can view and compare the energy usage of all data centres within Envizi. They can understand energy efficiency among their data centres by number of active hosts, by number of VMs. They can understand the density of VMs to Hosts in each data centre to identify opportunities to optimize efficiency.","title":"Use Case Example"},{"location":"turbonomic-envizi/#connector-architecture","text":"","title":"Connector Architecture"},{"location":"turbonomic-envizi/#data-mapping","text":"The image below illustrates the type of data that is being sent by the API and App Connect Flows.","title":"Data Mapping"},{"location":"turbonomic-envizi/#app-connect-flows","text":"Included with this connector are two flows that export locations and accounts, along with all the required fields they contain. The table below shows the naming convention for these flows and the current integration use case. File Flow Destination Operation TurbonomicLocations.yaml Locations Turbonomics to Envizi Depending on configuration Changes only or Bulk initial load TurbonomicAccounts.yaml Accounts Turbonomics to Envizi Depending on configuration Changes only or Bulk initial load","title":"App Connect Flows"},{"location":"turbonomic-envizi/#installation-configuration-guide","text":"","title":"Installation &amp; Configuration Guide"},{"location":"turbonomic-envizi/#before-you-begin-you-will-need","text":"Turbonomic version 8.6.6 or higher An instance of App Connect with the Designer component.","title":"&nbsp; Before you begin you will need:"},{"location":"turbonomic-envizi/#installation-steps-overview","text":"App Connect Configuration a. Add Accounts ( reference document ) b. Importing Flow in App Connect ( reference document ) Turbonomics Configuration a. Create user b. Configure tags","title":"Installation Steps Overview"},{"location":"turbonomic-envizi/#part-1-app-connect-configuration","text":"Note : When configuring App Connect you will have the option to set up a secure connection with a self-signed certificates, a private network, or both. Add Account for \"Amazon S3\" connector Credentials will be given by Envizi Add an Account for \"HTTP\" connector This account will be used for Authentication API Key : Enter the Turbonomic credentials in this format <USERNAME>&password=<PASSWORD> e.g., If the username is johndoe and the password is TheCakeIsALie , enter johndoe&password=TheCakeIsALie API Key Location : Select \"body URL encoded\" API Key Name : Enter the text username and not your actual username. Add another Account for \"HTTP\" connector This account will be used for other API calls Leave all authentication related fields empty Import the flows - Navigate to the flows below and use the link to their raw files for importing in App Connect: - TurbonomicLocations - TurbonomicAccounts Configure the flow to use the right accounts - For all Amazon S3 nodes in the flow, select the account created in Step 1 - For the first HTTP node in the flow, select the account created in Step 2 - For all other HTTP nodes in the flow, select the account created in Step 3 Configure the scheduler - Click on the \"Scheduler\" node <img width=\"659\" alt=\"Scheduler node\" src=\"https://media.github.ibm.com/user/375131/files/ea694d80-01ed-11ed-8852-16b625339675\"> - Configure the schedule as needed <img width=\"659\" alt=\"Scheduler configuration 2\" src=\"https://media.github.ibm.com/user/375131/files/00c3d900-01ef-11ed-86aa-29008ff86450\"> The TurbonomicAccounts flow must be executed daily, preferably at UTC midnight between 00:05 and 00:30 The TurbonomicLocations flow can be configured to run once a month or even On-demand as described here Note: By default, the flow will run when it is started. To change this behavior, untick the below checkbox in the \"Scheduler\" node. It is recommended to keep this checked for On-demand data pull Configure the flow parameters - Click on the \"Set variable\" node - In Variable > config > customer, enter the value provided by Envizi - In Variable > config > url, enter URL for the Turbonomic instance - e.g., https://example.com:9080 - Scroll to the Amazon S3 node and click on it to open the configuration ![S3 Configuration](https://media.github.ibm.com/user/375131/files/56c06800-4bb4-11ed-9aa6-c6758d61300a) - From the bucket dropdown, select the bucket name provided by Envizi - Perform these actions on all Amazon S3 nodes in the flow Start the flow Once the flow it started, it will run as configured in scheduler. However, the flow will only collect data from the day it has been started. In order to pull any historical data, additional configurations described in On-demand Data Pull need to be done.","title":"Part 1. App Connect Configuration"},{"location":"turbonomic-envizi/#part-2-turbonomic-configuration","text":"Turbonomic v8.6.6 or higher is required for this integration to work Create a user in Turbonomic with \"Observer\" role. The App Connect will be configured to use this user's credentials to fetch the necessary data. For accurate emission calculations from Envizi, add the following Tag Categories in vCenter and add their values as tags to the Data Centers: Country : Name of Country Latitude : Latitude in Decimal Degrees format Longitude : Longitude in Decimal Degrees format - By default, Envizi will use the Data Center name configured in Turbonomic/vCenter. To change this, Tag Category EnviziAlternateName can be added with the desired display name as its value. - Envizi Locations (Data Centers in this case) need unique display names. If there are any Data Centers with same names, they should be changed from vCenter or Tag Category EnviziAlternateName should be added to the Data Center(s) with different name(s) Note: Tags sync from vCenter to Turbonomic might take upto 20 minutes.","title":"Part 2. Turbonomic Configuration"},{"location":"turbonomic-envizi/#operating-the-connector","text":"","title":"Operating the connector"},{"location":"turbonomic-envizi/#on-demand-data-pull","text":"Use the on-demand data pull when there is historical energy related data that needs to be loaded from Turbonomic to Envizi. This method is for the initial sync or to sync data that was added/updated between specific dates. This method is meant to be executed just once whenever needed and then stopped. App Connect logs will show when On-demand load has been completed successfully or that it can be stopped. For TurbonomicLocations, no additional configuration is needed. To use this method in flows other than TurbonomicLocations, the following parameters in \"Set variable\" node need to be configured: Variable > config > OverrideStartDate: The start timestamp of the window between which the data will be pulled from. The date must be specified in yyyy-mm-dd format. e.g., 2022-06-26 Variable > config > OverrideEndDate: The end timestamp of the window between which the data will be pulled from. The date must be specified in yyyy-mm-dd format. e.g., 2022-06-27 Note: The dates configured here will be considered as UTC","title":"On-demand Data Pull"},{"location":"turbonomic-envizi/#part-3-testing-and-verification","text":"Set the OverrideStartDate and OverrideEndDate in the flow to a date that you know the data exists for. Preferably keep it a single date. Refer to the On-demand Data Pull section for more details Make sure \"Also run the flow when it's switched on\" is selected in the \"Scheduler\" node Start the flow Open App Connect logs from the left side bar Keep refreshing the logs until you see a message that says \"Flow completed successfully\". If you get an error message, refer to the Troubleshooting section","title":"Part 3. Testing and Verification"},{"location":"turbonomic-envizi/#troubleshooting","text":"The below errors are found in the App Connect logs. Error Cause Resolution The HTTP request returned with an error: 400 \"Bad Request\" No configuration for first HTTP Node Please configure the HTTP node with the exact same configurations as mentioned in the Step 5 of App Connect Configuration The HTTP request returned with an error: 400 \"Bad Request\" Invalid API Key name Enter the text username and not your actual username. The HTTP request returned with an error: 400 \"Bad Request\" Invalid API Key location Select \"body URL encoded\" in API Key Location The HTTP request returned with an error: 401 \"Unauthorized\" Invalid API Key in HTTP Account Enter the correct Turbonomic credentials in this format <USERNAME>&password=<PASSWORD> The HTTP request returned with an error: 500 \"Internal Server Error\" Invalid URL in Set Variable Configure the correct URL of the Turbonomic instance The HTTP request returned with an error: 404 \"Not Found\u201d If / is appended at the end of the valid URL Configure the correct URL of the Turbonomic instance and do not append / at the end of the URL Your Amazon S3 account doesn't have the required permissions to complete the isObjectPresent action on the object object: 403 \"Forbidden\" Invalid customer Configure the customer name in set variable exactly as provided by Envizi","title":"Troubleshooting"},{"location":"work-order/","text":"Work Order Integration Summary Enable automated synchronization of Work Order data from IBM Maximo to IBM TRIRIGA or vice versa. Description In this code pattern, learn how to synchronize a Work Order created in Maximo with TRIRIGA using an App Connect Designer flow. A diagram of the Work Order Integration When a Work Order is created in Maximo Asset Management, it triggers the flow to populate the request in TRIRIGA. (There is also a flow that works in the reverse direction, that works in a similar way.) App Connect sends a request with the new information through the flow towards the target system (TRIRIGA). A JSON Parser sifts through the request and converts it to an object. This object from the JSON Parser goes through Steps 5-8. The data from the object is mapped to the corresponding fields in the target application (TRIRIGA). The newly mapped data is sent to the target application (TRIRIGA) where the request is then created. This record is then validated with the original application (Maximo Asset Management) via another Post request. An ID is created within the original application (Maximo Asset Management). At the end of this process, a Work Order can be created within Maximo and sent to TRIRIGA and vice versa. Pre-requisites This configuration assumes the completion of the pre-requisites and steps outlined in the Maximo <-> TRIRIGA code pattern for those steps. Maximo Within Maximo, a change to the Work Order Application is needed to see additional fields. Work Order Work Order Application Designer Go to System Configuration -> Platform Configuration -> Application Designer Search for WOTRACK Switch to the Work Order Tab and scroll down to the Work Order Details section At the top, click the icon labeled Control Palette and drag the respective controls into the first main section on the right of the screen. Add these values within the properties of the control. Be sure that the PLUSIREQCLASSID Attribute is taken from the WORKORDER Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox Tririga Location Path PLUSILOCPATH PLUSILOCATIONPATH.DESCRIPTION VALUELIST Readonly Multipart Textbox Tririga Primary Organization PLUSIORGPATH PLUSIORGANIZATIONPATH.DESCRIPTION VALUELIST Readonly Textbox External Ref ID EXTERNALREFID ( From the WorkOrder Object ) N/A N/A N/A Click Save Definition after the changes are added. App Connect The configuration of App Connect from the previous code pattern should provide the mxtririga and trimaximo accounts within App Connect needed for the flows to work properly. Download and import the .yaml files and keep the urls handy for a later step. Use the following table for the parameters of the flows: Parameter Name Value mxUrl http://[host]:[port]/meaweb/esqueue/PLUSITRIRIGA/PLUSIMXWO triUrl http://[host]:[port]/oslc/so/triAPICWorkTaskCF mxDomain PLUSILOCPATH ( For Loc Path flow ) / PLUSIORGPATH ( For Org Path flow ) TRIRIGA Populate the domains created in the Maximo pre-requisites Go to Tools -> System Setup -> Integration -> Integration Object . Select Organization - APIC - HTTP Post from the table. Fill in the required sections: Click Execute at the top of the window. The process will take a few minutes since there are a large amount of files, but once it is completed you can check that the batch processed correctly under the specified domain. Repeat the process with triBuilding , triProperty , & triFloor using the PLUSITRILocPath2MX url and parameters. Verify the data is in sync by checking the corresponding Domain in Maximo. The populated table should look like this: Populated Domains Step 1. Create a Work Order Maximo to TRIRIGA Go to Work Orders -> Work Order Tracking and click on the blue plus sign to create a new Work Order. Input the desired name/number of the WO along with the description and assign the corresponding Primary Org and Location Path. Click Save Work Order and the flow should fire. The flow also supports the cost calculation of associated actuals within a Work Order. Costs can only be transacted against a Work Order with a valid GL Account. Once the proper GL Account is associated, navigate to the Actuals tab within the desired Work Order to assign costs. The supported actuals are Labor , Materials , Services , and Tools . Work Order Tracking Page A. Labor Select the correct Labor record to associate with the Work Order. Enter in the required Start and End Time fields and Save the Work Order. The flow will update the TRIRIGA application with the correct cost associated with the Labor based on the time entered. Labor Actuals B. Materials Select the correct Material record to associate with the Work Order. Enter in the required Storeroom field and Save the Work Order. The flow will update the TRIRIGA application with the correct cost associated with the Material. Materials C. Services The Service actual will only populate if a PO is received, and the PO has a Service line associated with the Work Order. Create a new Purchasing Order and add a new PO Line. Select Service as the Line Type and enter the Line Cost. Associate the desired WO for this Service to be charged and approve the PO. In the Receiving application, select the approved PO and switch to the Service Receipts tab at the top. Click on Select Ordered Services , select the created Service, and click Save Receipt on the left-hand side of the screen. The Service will then appear as an actual in both Maximo and TRIRIGA via the flow. Step 1: PO Charges Step 3: Service Receipts D. Tools Select the correct Tool record to associate with the Work Order. Enter in the required Bin field and Save the Work Order. The flow will update the TRIRIGA application with the correct cost associated with the Material. Tools Actual A full breakdown of costs can be found by going to View -> Costs from the left side of the screen under More Actions View Cost Breakdown TRIRIGA to Maximo Go to Tasks -> Manage Tasks -> Work Tasks and click the Add button on the top right. Fill in the required fields and click Submit at the top right of the newly opened window. The flow should fire upon submission. TRIRIGA Work Order Troubleshooting Common Errors and their resolutions: Maximo Common errors found in the Maximo system Error Cause 401: Bad Request This usually means an aspect of the request was not sent correctly- double check what is being sent as well as the flow in App Connect to make sure everything is correct and running. App Connect Troubleshoot App Connect with the logging function. While the flow is stopped, add a Log node into the flow from the Toolbox tab. This will allow mapping of any field to the Logging section of the application. Select Info for the Log level and then map the field that needs debugging. In this example the Request Object has been mapped to see what is being sent through the flow. Click the icon to the right of the Message Detail to map the desired field. The Log node will compile the message and read it out in the Logging section. Diagnose the response that shows up in this section to learn what might be causing the issue. Step 1: Log Node Step 2: Log Content Logging section Step 3: Logging Dashboard TRIRIGA Common errors found in the TRIRIGA system Error Cause ERROR: Requested For Does not Exist No People record exists with the triIdTX value mentioned in triRequestedForTX field of the payload ERROR: Building Does not Exist No Building record exists with the triNameTX value mentioned in triBuildingTX field of the payload ERROR: Location Does not Exist No Location record exists with the triNameTX value mentioned in triParentLocationTX field of the payload ERROR: Organization Does not Exist No Organization record exists with the triPathTX value mentioned in triCustomerOrgTX field of the payload 400 - Bad Request :: {\"error\":{\"statusCode\":400,\"message\":\"API creation is failed with not_found\",\"name\":\"Error\"}} To be filled in","title":"Work Order Integration"},{"location":"work-order/#work-order-integration","text":"","title":"Work Order Integration"},{"location":"work-order/#summary","text":"Enable automated synchronization of Work Order data from IBM Maximo to IBM TRIRIGA or vice versa.","title":"Summary"},{"location":"work-order/#description","text":"In this code pattern, learn how to synchronize a Work Order created in Maximo with TRIRIGA using an App Connect Designer flow. A diagram of the Work Order Integration When a Work Order is created in Maximo Asset Management, it triggers the flow to populate the request in TRIRIGA. (There is also a flow that works in the reverse direction, that works in a similar way.) App Connect sends a request with the new information through the flow towards the target system (TRIRIGA). A JSON Parser sifts through the request and converts it to an object. This object from the JSON Parser goes through Steps 5-8. The data from the object is mapped to the corresponding fields in the target application (TRIRIGA). The newly mapped data is sent to the target application (TRIRIGA) where the request is then created. This record is then validated with the original application (Maximo Asset Management) via another Post request. An ID is created within the original application (Maximo Asset Management). At the end of this process, a Work Order can be created within Maximo and sent to TRIRIGA and vice versa.","title":"Description"},{"location":"work-order/#pre-requisites","text":"This configuration assumes the completion of the pre-requisites and steps outlined in the Maximo <-> TRIRIGA code pattern for those steps.","title":"Pre-requisites"},{"location":"work-order/#maximo","text":"Within Maximo, a change to the Work Order Application is needed to see additional fields.","title":"Maximo"},{"location":"work-order/#work-order","text":"Work Order Application Designer Go to System Configuration -> Platform Configuration -> Application Designer Search for WOTRACK Switch to the Work Order Tab and scroll down to the Work Order Details section At the top, click the icon labeled Control Palette and drag the respective controls into the first main section on the right of the screen. Add these values within the properties of the control. Be sure that the PLUSIREQCLASSID Attribute is taken from the WORKORDER Object. Type of Control Label Attribute Attribute for Part 2 ( If Multipart Textbox ) Lookup Input Mode for Part 2 ( If Multipart Textbox ) Multipart Textbox Tririga Location Path PLUSILOCPATH PLUSILOCATIONPATH.DESCRIPTION VALUELIST Readonly Multipart Textbox Tririga Primary Organization PLUSIORGPATH PLUSIORGANIZATIONPATH.DESCRIPTION VALUELIST Readonly Textbox External Ref ID EXTERNALREFID ( From the WorkOrder Object ) N/A N/A N/A Click Save Definition after the changes are added.","title":"Work Order"},{"location":"work-order/#app-connect","text":"The configuration of App Connect from the previous code pattern should provide the mxtririga and trimaximo accounts within App Connect needed for the flows to work properly. Download and import the .yaml files and keep the urls handy for a later step. Use the following table for the parameters of the flows: Parameter Name Value mxUrl http://[host]:[port]/meaweb/esqueue/PLUSITRIRIGA/PLUSIMXWO triUrl http://[host]:[port]/oslc/so/triAPICWorkTaskCF mxDomain PLUSILOCPATH ( For Loc Path flow ) / PLUSIORGPATH ( For Org Path flow )","title":"App Connect"},{"location":"work-order/#tririga","text":"Populate the domains created in the Maximo pre-requisites Go to Tools -> System Setup -> Integration -> Integration Object . Select Organization - APIC - HTTP Post from the table. Fill in the required sections: Click Execute at the top of the window. The process will take a few minutes since there are a large amount of files, but once it is completed you can check that the batch processed correctly under the specified domain. Repeat the process with triBuilding , triProperty , & triFloor using the PLUSITRILocPath2MX url and parameters. Verify the data is in sync by checking the corresponding Domain in Maximo. The populated table should look like this: Populated Domains","title":"TRIRIGA"},{"location":"work-order/#step-1-create-a-work-order","text":"","title":"Step 1. Create a Work Order"},{"location":"work-order/#maximo-to-tririga","text":"Go to Work Orders -> Work Order Tracking and click on the blue plus sign to create a new Work Order. Input the desired name/number of the WO along with the description and assign the corresponding Primary Org and Location Path. Click Save Work Order and the flow should fire. The flow also supports the cost calculation of associated actuals within a Work Order. Costs can only be transacted against a Work Order with a valid GL Account. Once the proper GL Account is associated, navigate to the Actuals tab within the desired Work Order to assign costs. The supported actuals are Labor , Materials , Services , and Tools . Work Order Tracking Page","title":"Maximo to TRIRIGA"},{"location":"work-order/#a-labor","text":"Select the correct Labor record to associate with the Work Order. Enter in the required Start and End Time fields and Save the Work Order. The flow will update the TRIRIGA application with the correct cost associated with the Labor based on the time entered. Labor Actuals","title":"A. Labor"},{"location":"work-order/#b-materials","text":"Select the correct Material record to associate with the Work Order. Enter in the required Storeroom field and Save the Work Order. The flow will update the TRIRIGA application with the correct cost associated with the Material. Materials","title":"B. Materials"},{"location":"work-order/#c-services","text":"The Service actual will only populate if a PO is received, and the PO has a Service line associated with the Work Order. Create a new Purchasing Order and add a new PO Line. Select Service as the Line Type and enter the Line Cost. Associate the desired WO for this Service to be charged and approve the PO. In the Receiving application, select the approved PO and switch to the Service Receipts tab at the top. Click on Select Ordered Services , select the created Service, and click Save Receipt on the left-hand side of the screen. The Service will then appear as an actual in both Maximo and TRIRIGA via the flow. Step 1: PO Charges Step 3: Service Receipts","title":"C. Services"},{"location":"work-order/#d-tools","text":"Select the correct Tool record to associate with the Work Order. Enter in the required Bin field and Save the Work Order. The flow will update the TRIRIGA application with the correct cost associated with the Material. Tools Actual A full breakdown of costs can be found by going to View -> Costs from the left side of the screen under More Actions View Cost Breakdown","title":"D. Tools"},{"location":"work-order/#tririga-to-maximo","text":"Go to Tasks -> Manage Tasks -> Work Tasks and click the Add button on the top right. Fill in the required fields and click Submit at the top right of the newly opened window. The flow should fire upon submission. TRIRIGA Work Order","title":"TRIRIGA to Maximo"},{"location":"work-order/#troubleshooting","text":"Common Errors and their resolutions:","title":"Troubleshooting"},{"location":"work-order/#maximo_1","text":"Common errors found in the Maximo system Error Cause 401: Bad Request This usually means an aspect of the request was not sent correctly- double check what is being sent as well as the flow in App Connect to make sure everything is correct and running.","title":"Maximo"},{"location":"work-order/#app-connect_1","text":"Troubleshoot App Connect with the logging function. While the flow is stopped, add a Log node into the flow from the Toolbox tab. This will allow mapping of any field to the Logging section of the application. Select Info for the Log level and then map the field that needs debugging. In this example the Request Object has been mapped to see what is being sent through the flow. Click the icon to the right of the Message Detail to map the desired field. The Log node will compile the message and read it out in the Logging section. Diagnose the response that shows up in this section to learn what might be causing the issue. Step 1: Log Node Step 2: Log Content Logging section Step 3: Logging Dashboard","title":"App Connect"},{"location":"work-order/#tririga_1","text":"Common errors found in the TRIRIGA system Error Cause ERROR: Requested For Does not Exist No People record exists with the triIdTX value mentioned in triRequestedForTX field of the payload ERROR: Building Does not Exist No Building record exists with the triNameTX value mentioned in triBuildingTX field of the payload ERROR: Location Does not Exist No Location record exists with the triNameTX value mentioned in triParentLocationTX field of the payload ERROR: Organization Does not Exist No Organization record exists with the triPathTX value mentioned in triCustomerOrgTX field of the payload 400 - Bad Request :: {\"error\":{\"statusCode\":400,\"message\":\"API creation is failed with not_found\",\"name\":\"Error\"}} To be filled in","title":"TRIRIGA"}]}